{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259c76c-08e1-4bce-bfef-0f3bf569d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm  #to get python's normal library of colormaps\n",
    "import matplotlib.colors as mplc\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "#from cartopy.util import add_cyclic_point\n",
    "#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import metpy.calc as mpcalc\n",
    "import metpy.plots as mplots\n",
    "from metpy.units import units\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "import scipy.signal as sig\n",
    "\n",
    "from PIL import Image\n",
    "import icartt            #needed to read .ict files\n",
    "\n",
    "import time\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")  #hides \"MatplotlibDeprecationWarning\" with pcolormesh\n",
    "\n",
    "# tstart = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9063b6e2-eb9b-42b0-8a88-f350d9e1a6db",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figure 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7b694-082e-4d87-8d22-d5d2041b3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from CPEXCV_APR.ipynb (see this script for version separated into separate cells)\n",
    "\n",
    "#the only cell with variables you need to change, unless you are not Ben (will need to change cell #3 too)\n",
    "file_date = '20220906'           #which date to create the APR plot for\n",
    "start_time0 = '20220906,161555'  #start date,time for the APR plot\n",
    "end_time0 = '20220906,163100'    #end date,time for the APR plot\n",
    "full = False                     #do the chosen time ranges below encapsulate the entire convective module?\n",
    "\n",
    "#other users will also have to change day_folder, apr_folder, dawn_csv_path, and drop_csv_path filepaths\n",
    "    #dawn_csv_path and drop_csv_path are based on created files from CPEXCV_DAWN.ipynb and CPEXCV_dropsonde(_metrics).ipynb\n",
    "\n",
    "start_time = datetime.strptime(start_time0, '%Y%m%d,%H%M%S')\n",
    "end_time = datetime.strptime(end_time0, '%Y%m%d,%H%M%S')\n",
    "\n",
    "#locations of the APR folder and APR files\n",
    "day_folder = os.path.join(os.getcwd(), file_date)\n",
    "apr_folder = os.path.join(day_folder, 'APR_files')\n",
    "\n",
    "#load the final DAWN and final Dropsonde CSVs\n",
    "dawn_csv_path = os.path.join(day_folder, 'final_dawn_' + file_date + '.csv')\n",
    "dawn_csv = pd.read_csv(dawn_csv_path)\n",
    "\n",
    "drop_csv_path = os.path.join(day_folder, 'final_dropsonde_' + file_date + '.csv')\n",
    "drop_csv = pd.read_csv(drop_csv_path)\n",
    "\n",
    "# #example CPEX-CV APR file\n",
    "# test = xr.open_dataset('/Users/ben/Desktop/CPEX/Coding/20220906/APR_files/cpexcv-APR3_DC8_20220906_R0_S20220906a121242_E20220906a121851_KUsKAsWns.nc')\n",
    "# test\n",
    "\n",
    "#create a list of all the given day's desired range's APR files:\n",
    "for x in os.listdir(apr_folder):\n",
    "    if x[0:3] == '.DS':         #delete hidden .DS_Store files if they come up (will show up if you delete a file)\n",
    "        os.remove(os.path.join(apr_folder, x))\n",
    "\n",
    "#find the starting APR file in apr_folder\n",
    "first_file_index = None \n",
    "\n",
    "#sorted() makes sure the code goes through the files in alphabetical (chronological) order\n",
    "#GOING THROUGH THE FILES IN CHRONOLOGICAL ORDER IS ESSENTIAL FOR THIS CELL TO WORK PROPERLY!!\n",
    "for i, x in enumerate(sorted(os.listdir(apr_folder))):  \n",
    "    file_start_time = datetime.strptime(x[29:37] + x[38:44], '%Y%m%d%H%M%S')\n",
    "    file_end_time = datetime.strptime(x[46:54] + x[55:61], '%Y%m%d%H%M%S')\n",
    "\n",
    "    if start_time <= file_start_time:  #if start_time is before the APR file start time and not within any previous APR file's time ranges\n",
    "        first_file_index = i\n",
    "        break\n",
    "    elif (start_time >= file_start_time) and (start_time < file_end_time):\n",
    "        first_file_index = i\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "if first_file_index == None:\n",
    "    sys.exit('Requested start_time is beyond all available APR files')\n",
    "    \n",
    "#find the ending APR file in apr_folder\n",
    "last_file_index = None    \n",
    "\n",
    "#sorted() makes sure the code goes through the files in alphabetical (chronological) order\n",
    "#GOING THROUGH THE FILES IN CHRONOLOGICAL ORDER IS ESSENTIAL FOR THIS CELL TO WORK PROPERLY!!\n",
    "for i, x in enumerate(sorted(os.listdir(apr_folder))):  \n",
    "    file_start_time = datetime.strptime(x[29:37] + x[38:44], '%Y%m%d%H%M%S')\n",
    "    file_end_time = datetime.strptime(x[46:54] + x[55:61], '%Y%m%d%H%M%S')\n",
    "\n",
    "    if end_time <= file_start_time:  #if end_time is before the APR file start time and not within any previous APR file's time ranges\n",
    "        last_file_index = i - 1\n",
    "        break\n",
    "    elif (end_time > file_start_time) and (end_time <= file_end_time):\n",
    "        last_file_index = i\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "if last_file_index == None:  #the end_time is after all available APR file ranges\n",
    "    last_file_index = len(sorted(os.listdir(apr_folder))) - 1  #the last available APR file's index\n",
    "if last_file_index == -1:\n",
    "    sys.exit('Requested end_time is before all available APR files')\n",
    "\n",
    "apr_file_list = sorted(os.listdir(apr_folder))[first_file_index:last_file_index + 1]\n",
    "#apr_file_list\n",
    "\n",
    "#Calculate time range in minutes, have a tick for each minute\n",
    "num_minutes = (end_time - start_time).total_seconds() // 60\n",
    "fig_length = num_minutes * 2\n",
    "\n",
    "#Calculate height range of the plot, based on the maximum height of all DAWN profiles\n",
    "height_max = dawn_csv['Height [m]'].max()\n",
    "y_max = math.ceil(height_max / 1000) * 1000  #round height_max up to the nearest 1000\n",
    "\n",
    "#matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 45\n",
    "matplotlib.rcParams['axes.titlesize'] = 55\n",
    "matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "matplotlib.rcParams['xtick.labelsize'] = 37\n",
    "matplotlib.rcParams['ytick.labelsize'] = 37\n",
    "matplotlib.rcParams['legend.fontsize'] = 35\n",
    "#matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "#matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "\n",
    "#plotting a time-series plot of just Ku-band reflectivity vs. height\n",
    "ylim=[0,y_max]\n",
    "vlim=[-10,40]\n",
    "vel_lim=[-10,10]\n",
    "\n",
    "#Low resolution ('lores') radar variables in APR hdf files\n",
    "ku_band = 'lores_zhh14' #Ku-band reflectivity\n",
    "ka_band = 'lores_zhh35' #Ka-band reflectivity\n",
    "vel = 'lores_vel14c' #Mean Doppler Velocity from Ku-band (surface Doppler velocity is subtracted and free of aliasing)\n",
    "w_band = 'lores_z95s' #W-band reflectivity\n",
    "w_band_nadir = 'hi2lo_z95n' #W-band reflectivity at nadir, upscaled to same resolution as 'lores'\n",
    "    #^^^for quantitative analysis, will likely want to switch to using 'hires_z95n'\n",
    "\n",
    "if full:\n",
    "    plot_save_name = file_date + '_SinglePanel_full.png'\n",
    "else:\n",
    "    plot_save_name = file_date + '_SinglePanel_' + start_time0[-6:] + '-' + end_time0[-6:] + '.png'\n",
    "\n",
    "#fig,axes = plt.subplots(1,1,figsize=(fig_length,14))\n",
    "fig,axes = plt.subplots(1,1,figsize=(48,14))   #use for AGU Paper Figure 12b (to match figsize for Figure 12a from CPEX_2017_APR.ipynb)\n",
    "\n",
    "for file in apr_file_list:\n",
    "    \n",
    "    #find the APR file of interest (for the desired datetime)\n",
    "    apr_filepath = os.path.join(apr_folder, file)\n",
    "    apr_file = xr.open_dataset(apr_filepath)\n",
    "\n",
    "    #grab the radar variables of interest\n",
    "    ku_good = False\n",
    "    if ku_band in apr_file.keys():\n",
    "        try:   #some APR files, at least in the preliminary data, have corrupted Ku-band data; \n",
    "               #if so, skip plotting the Ku-band for that file\n",
    "               #corrupted: \"OSError: Can't read data (inflate() failed)\"\n",
    "            ku_data = apr_file[ku_band][:]\n",
    "\n",
    "            #mask the missing Ku-band data (values of -99.99)\n",
    "            ku_masked = np.ma.masked_where(ku_data <= -99, ku_data)\n",
    "            ku_masked = np.ma.masked_where(np.isnan(ku_masked), ku_masked)  #masks NaN values (not masked in previous line)\n",
    "            ku_good = True\n",
    "        except:\n",
    "            ku_good = False\n",
    "    \n",
    "    #if Ku-band is available\n",
    "    if ku_good:            \n",
    "\n",
    "        #Convert 'lo-res' APR times to datetimes\n",
    "        time = apr_file['time'][:]  #For 'lores': Time of scan, in seconds since midnight UTC of [YYYY-mm-DD]\n",
    "        alt3d = apr_file['lores_alt3D'][:]\n",
    "\n",
    "        time_dates = np.empty(time.shape, dtype=object)\n",
    "        for i in np.arange(0, time.shape[0]):\n",
    "            #hour, second automatically revert to midnight (hour = 0, seconds = 0) for '%Y%m%d'\n",
    "            time_dates[i] = datetime.strptime(file_date, '%Y%m%d') + timedelta(seconds = float(time[i].values))\n",
    "\n",
    "        #Create a time at each gate (assuming time is the same for each ray of a given scan and down each ray)      \n",
    "        time_gate = np.empty(alt3d.shape, dtype=object)\n",
    "        for i in np.arange(0, alt3d.shape[0]):\n",
    "            time_gate[i,:,:] = time_dates[i]   #assign the same time to all of a given scan's rays and height bins     \n",
    "\n",
    "        time3d = np.copy(time_gate)\n",
    "\n",
    "#         print (time3d[0,0,:])  #times should be the same\n",
    "#         print ('')\n",
    "#         print (time3d[0,:,0])  #times should be the same\n",
    "#         print ('')\n",
    "#         print (time3d[:,0,0])  #times should be different\n",
    "#         sys.exit()\n",
    "\n",
    "        #plot the APR data factoring in the aircraft roll (ray adjustment)\n",
    "        #choose the \"pseudo-nadir\" ray factoring in aircraft roll\n",
    "        roll = apr_file['lores_roll'][:]\n",
    "        ray_angles = np.arange(-25,25.01,25/12)  #in degrees; 25 rays for each scan, with the middle (13th) scan being zero degrees\n",
    "        for scan in range(roll.shape[0]):\n",
    "            if (time_dates[scan] >= start_time) and (time_dates[scan] <= end_time):\n",
    "                ac_roll = np.nanmean(roll[scan,:])  #roll varies slightly w/ray, so take the average roll value for a given scan and use that for ray adjustment\n",
    "                ray_use = np.argmin(np.abs(ray_angles - ac_roll))  #the index of the ray whose angle is closest to that of ac_roll    \n",
    "\n",
    "                #scan + 2 (and not scan + 1) is needed because pcolormesh colors the grid cell from the  \n",
    "                #grid cell's time to the subsequent grid cell's time.  If a subsequent grid cell does not exist,  \n",
    "                #then pcolormesh cannot/doesn't color the grid cell (remember, slicing is right side EXCLUSIVE, \n",
    "                #so scan:scan + 1 is only 1 element and thus doesn't have a subsequent cell!)\n",
    "                #by this same logic, scan:scan + 2 will only color one grid cell, since the 2nd (and last) \n",
    "                #element/grid cell doesn't have a subsequent grid cell\n",
    "\n",
    "                if ku_good:\n",
    "                    pm0 = axes.pcolormesh(time3d[scan:scan+2,ray_use,:], alt3d[scan:scan+2,ray_use,:],\n",
    "                                          ku_masked[scan:scan+2,ray_use,:], cmap=cm.Spectral_r, vmin=vlim[0], vmax=vlim[1])\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    apr_file.close()\n",
    "\n",
    "cbar0 = plt.colorbar(pm0, ax=axes, pad = 0.01, extend = 'both')\n",
    "cbar0.set_label('Z [dBZ]')\n",
    "axes.set_title('b) Ku-band Reflectivity (Case 10, 06 September 2022)')\n",
    "\n",
    "# dawn_skip = slice(None,None,7)   #only plot every 7th value\n",
    "# drop_skip = slice(None,None,20)  #only plot every 20th value\n",
    "dawn_skip = slice(None,None,10)   #only plot every 10th value\n",
    "drop_skip = slice(None,None,50)  #only plot every 50th value\n",
    "\n",
    "#set the plot start time as the beginning time of the first APR file and the\n",
    "    #plot end time as the end time of the last APR file:\n",
    "# range_start = datetime.strptime(apr_file_list[0][32:40] + apr_file_list[0][41:47], '%Y%m%d%H%M%S') \n",
    "# range_end = datetime.strptime(apr_file_list[-1][49:57] + apr_file_list[-1][58:64], '%Y%m%d%H%M%S')\n",
    "# range_start = start_time\n",
    "# range_end = end_time\n",
    "range_start = datetime.strptime(start_time0, '%Y%m%d,%H%M%S') \n",
    "range_end = datetime.strptime(end_time0, '%Y%m%d,%H%M%S')\n",
    "\n",
    "for xx, dawn_profile in enumerate(dawn_csv['Time [UTC]'].unique()):\n",
    "    if xx % 2 == 0:  #skip plotting every other DAWN profile\n",
    "        dawn_csv_prof = dawn_csv[dawn_csv['Time [UTC]'] == dawn_profile]\n",
    "        axes.barbs(pd.to_datetime(dawn_csv_prof['Time [UTC]'])[dawn_skip], dawn_csv_prof['Height [m]'][dawn_skip], \n",
    "                   dawn_csv_prof['U Comp of Wind [m/s]'][dawn_skip], dawn_csv_prof['V Comp of Wind [m/s]'][dawn_skip], \n",
    "                   fill_empty = True, length = 10, pivot='middle', sizes=dict(emptybarb=0.1), barbcolor = 'k')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# axes.barbs(pd.to_datetime(dawn_csv['Time [UTC]'])[dawn_skip], dawn_csv['Height [m]'][dawn_skip], \n",
    "#            dawn_csv['U Comp of Wind [m/s]'][dawn_skip], dawn_csv['V Comp of Wind [m/s]'][dawn_skip], \n",
    "#            fill_empty = True, length = 10, pivot='middle', sizes=dict(emptybarb=0.1), barbcolor = 'k')\n",
    "axes.barbs(pd.to_datetime(drop_csv['Time [UTC]'])[drop_skip], drop_csv['Height [m]'][drop_skip], \n",
    "           drop_csv['U Comp of Wind [m/s]'][drop_skip], drop_csv['V Comp of Wind [m/s]'][drop_skip], \n",
    "           fill_empty = True, length = 10, pivot='middle', sizes=dict(emptybarb=0.1), barbcolor = 'b')\n",
    "\n",
    "##########################\n",
    "\n",
    "axes.set_ylabel('Altitude [m]')\n",
    "axes.set_xlabel('Time [UTC]')\n",
    "axes.set_ylim([ylim[0],ylim[1]])\n",
    "axes.tick_params(axis='x', rotation = 50)\n",
    "axes.tick_params(length = 15, width = 5)\n",
    "axes.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%H:%M:%S\"))\n",
    "axes.xaxis.set_major_locator(ticker.MaxNLocator(num_minutes))      #sets number of ticks\n",
    "axes.set_xlim([np.datetime64(range_start),np.datetime64(range_end)])    \n",
    "    #use the above line to narrow the plot's time range (even within a file!!)\n",
    "        #range_start and range_end must be a datetime object or a string with the \n",
    "            #format: 'YYYY-MM-DD HH:MM:SS' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#save the figure\n",
    "#plt.savefig(os.path.join(day_folder, plot_save_name), bbox_inches = 'tight')\n",
    "plt.savefig('/Users/ben/Desktop/Figure1b.png', bbox_inches = 'tight')\n",
    "#plt.show()  #if you want to also show the image in the output cell, plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "plt.close()\n",
    "\n",
    "\n",
    "##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "from PIL import Image\n",
    "im = Image.open('/Users/ben/Desktop/Figure1b.png')\n",
    "\n",
    "try:\n",
    "    im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "except:\n",
    "    #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "    #though this line will have isolated, extremely minor image effects due to \n",
    "    #only using 256 colors instead of the 3-element RGB scale\n",
    "    im2 = im.convert('P')\n",
    "\n",
    "im2.save('/Users/ben/Desktop/Figure1b.png')\n",
    "im.close()\n",
    "im2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523de7b-4824-4a50-b888-60894d64a244",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figure 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff4d0e-ba3f-46e8-abfc-19ab263fe36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see ERA5_Analysis.ipynb for version separated into separate cells\n",
    "\n",
    "#adapted from CPEXCV_BAMS_Figures.ipynb (Figure 7)\n",
    "#plotting ERA5 reanalysis 850 hPa streamlines (and maybe RH), MIMIC TPW (if not ERA5 RH) and GPM IMERG\n",
    "\n",
    "# case_dict_agu = {0: ['20220909', '2022-09-09', 16, [-60, -10, 0, 30], 'Isolated'],\n",
    "#                 1: ['20220907', '2022-09-07', 15, [-60, -10, 0, 30], 'Organized']}  #flight track lat/lon extent [West,East,South,North]\n",
    "case_dict_agu = {0: ['20220909', '2022-09-09', 16, [-27.5, -12.5, 10, 20], 'Isolated'],\n",
    "                1: ['20220907', '2022-09-07', 15, [-50, -20, 5, 25], 'Organized']}  #flight track lat/lon extent [West,East,South,North]\n",
    "\n",
    "stream_pres = 700  #hPa; pressure level to plot ERA5 streamlines for\n",
    "\n",
    "#set some baseline plot displays\n",
    "\n",
    "#matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 40\n",
    "matplotlib.rcParams['axes.titlesize'] = 41\n",
    "matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "matplotlib.rcParams['xtick.labelsize'] = 40\n",
    "matplotlib.rcParams['ytick.labelsize'] = 40\n",
    "matplotlib.rcParams['legend.fontsize'] = 40\n",
    "matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['axes.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "\n",
    "data_proj = ccrs.PlateCarree()\n",
    "group_fig = plt.figure(figsize = (24,34))\n",
    "\n",
    "for key, case_info in case_dict_agu.items():\n",
    "    \n",
    "    file_date = case_info[0]\n",
    "    file_date_prior = str(int(case_info[0]) - 1)\n",
    "    case_num = case_info[1]   \n",
    "    hr = case_info[2]\n",
    "    campaign_extent = case_info[3]\n",
    "    convective_type = case_info[4]\n",
    "    \n",
    "    ###get locations of the dropsonde/Navigation folder and read the appropriate files in\n",
    "    day_folder = os.path.join(os.getcwd(), file_date)\n",
    "\n",
    "#     #dropsonde data\n",
    "#     drop_csv_path = os.path.join(day_folder, 'final_dropsonde_' + file_date + '.csv')\n",
    "#     drop_csv = pd.read_csv(drop_csv_path)\n",
    "\n",
    "    if file_date[:4] == '2017':\n",
    "        campaign = 'CPEX'\n",
    "        drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations.csv')\n",
    "    elif file_date[:4] == '2021':\n",
    "        campaign = 'CPEXAW'\n",
    "        drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations.csv')\n",
    "    elif file_date[:4] == '2022':\n",
    "        campaign = 'CPEXCV'\n",
    "        drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #ERA5 data\n",
    "    era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "    era5_path = os.path.join(era5_folder, campaign + '_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "    #era5_path = os.path.join(era5_folder, campaign + '_ERA5_Reanalysis_Hourly_Pressure_BAMS.nc')\n",
    "    ds = xr.open_dataset(era5_path)\n",
    "\n",
    "    #Navigation data\n",
    "    nav_folder = os.path.join(day_folder, 'Nav_files')\n",
    "    \n",
    "    for x in os.listdir(nav_folder):\n",
    "        if x[0:3] == '.DS':         #delete hidden .DS_Store files if they come up (will show up if you delete a file)\n",
    "            os.remove(os.path.join(nav_folder, x))\n",
    "\n",
    "    nav_ict_path = os.path.join(nav_folder, os.listdir(nav_folder)[0])  #only one nav file per flight\n",
    "\n",
    "    if campaign != 'CPEX':  #campaign either CPEXAW or CPEXCV\n",
    "\n",
    "        nav_ict = icartt.Dataset(nav_ict_path)    #open the ict file with icartt\n",
    "        flight_lat = nav_ict.data[\"Latitude\"]     #nav latitude, just a normal 1-D array\n",
    "        flight_lon = nav_ict.data[\"Longitude\"]    #nav longitude, just a normal 1-D array\n",
    "\n",
    "    else:  #for CPEX navigation files, open the CSV file with pandas\n",
    "           #(Navigation files for CPEX (2017) are originally .kmz not .ict,\n",
    "           #so I converted them to CSV for free using https://www.gpsvisualizer.com/convert_input\n",
    "\n",
    "        nav_ict = pd.read_csv(nav_ict_path)       #open the ict file with pandas instead\n",
    "        flight_lat = nav_ict[\"latitude\"].values   #nav latitude, just a normal 1-D array\n",
    "        flight_lon = nav_ict[\"longitude\"].values  #nav longitude, just a normal 1-D array    \n",
    "\n",
    "\n",
    "    # ###calculate each dropsonde's mean lat/lon and add the sonde's time and mean lat/lon to a list to be plotted\n",
    "\n",
    "    # drop_coords_and_time = []   #format: longitude, latitude, time (HHSS)\n",
    "\n",
    "    # for sonde_datetime in drop_csv['Time [UTC]'].unique():\n",
    "    #     drop_csv_use = drop_csv[drop_csv['Time [UTC]'] == sonde_datetime].copy()\n",
    "    #     drop_mean_lon = drop_csv_use['Longitude [deg]'].mean()\n",
    "    #     drop_mean_lat = drop_csv_use['Latitude [deg]'].mean()\n",
    "\n",
    "    #     sonde_info = [drop_mean_lon, drop_mean_lat, sonde_datetime[-8:-3]]\n",
    "    #     drop_coords_and_time.append(sonde_info)\n",
    "\n",
    "    # ###calculate each NEAR-STORM dropsonde's mean lat/lon and add the sonde's time and mean lat/lon to a list to be plotted\n",
    "    # df_drop = pd.read_csv(drop_metric_filepath)\n",
    "    # df_drop_use = df_drop[df_drop['Date'] == int(file_date)].copy()\n",
    "\n",
    "    # for x in range(len(df_drop_use)):\n",
    "    #     date = str(df_drop_use['Date'].iloc[x])\n",
    "    #     time = str(df_drop_use['Time'].iloc[x]).zfill(6)\n",
    "    #     sonde_datetime = date[:4] + '-' + date[4:6] + '-' + date[6:] + ' ' + time[:2] + ':' + time[2:4] + ':' + time[4:]\n",
    "\n",
    "    #     drop_csv_use = drop_csv[drop_csv['Time [UTC]'] == sonde_datetime].copy()\n",
    "    #     drop_mean_lon = drop_csv_use['Longitude [deg]'].mean()\n",
    "    #     drop_mean_lat = drop_csv_use['Latitude [deg]'].mean()\n",
    "\n",
    "    #     sonde_info = [drop_mean_lon, drop_mean_lat, time[:4]]\n",
    "    #     drop_coords_and_time.append(sonde_info)\n",
    "        \n",
    "    hr2 = str(hr).zfill(2)\n",
    "\n",
    "    #MIMIC TPW data\n",
    "        ##https://bin.ssec.wisc.edu/pub/mtpw2/data/\n",
    "    tpw_folder = os.path.join(day_folder, 'MIMIC_TPW_files')\n",
    "    tpw_path = os.path.join(tpw_folder, 'comp' + file_date + '.' + hr2 + '0000.nc')\n",
    "    ds_tpw = xr.open_dataset(tpw_path)\n",
    "\n",
    "    #GPM IMERG data (see IMERG.ipynb for more how to more generally download and plot IMERG data)\n",
    "        ##https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Read%20IMERG%20Data%20Using%20Python\n",
    "        ##https://disc.gsfc.nasa.gov/datasets?keywords=imerg&page=1\n",
    "        #0.1 x 0.1 gridded data, half-hourly means, using the half hour BEFORE the desired hour\n",
    "    imerg_folder = os.path.join(day_folder, 'IMERG_files')\n",
    "\n",
    "    for x in os.listdir(imerg_folder):\n",
    "        if x[0:3] == '.DS':         #delete hidden .DS_Store files if they come up (will show up if you delete a file)\n",
    "            os.remove(os.path.join(imerg_folder, x))\n",
    "\n",
    "        #minutes and seconds automatically revert to zero (hour = 0, seconds = 0) for '%Y%m%d%H'\n",
    "        elif (datetime.strftime(datetime.strptime(file_date + hr2, '%Y%m%d%H') - timedelta(seconds = 1), '%H%M%S') in x) and (datetime.strftime(datetime.strptime(file_date + hr2, '%Y%m%d%H') - timedelta(minutes = 30), '%H%M%S') in x):\n",
    "            imerg_file = x\n",
    "            break\n",
    "        else:\n",
    "            imerg_file = 'Could not find the desired IMERG file'\n",
    "\n",
    "    #confirm that the IMERG file is from the correct day (if hr2 == '00', then this will be the previous day)\n",
    "    assert (file_date in imerg_file) or (hr2 == '00'), 'IMERG file not from the correct day'\n",
    "\n",
    "    imerg_path = os.path.join(imerg_folder, imerg_file)\n",
    "    ds_imerg = h5py.File(imerg_path, 'r')\n",
    "\n",
    "    imerg_lons = ds_imerg['Grid/lon'][:]   #Longitude Shape: (3600,)\n",
    "    imerg_lats = ds_imerg['Grid/lat'][:]   #Latitude Shape: (1800,)\n",
    "    imerg_lons, imerg_lats = np.meshgrid(imerg_lons, imerg_lats)  #Long and lat grid shape: (1800, 3600) \n",
    "\n",
    "    imerg_precip = ds_imerg['Grid/precipitation'][0][:][:]  #Original Precip Shape: (1, 3600, 1800) = (time, lon, lat)\n",
    "    imerg_precip = np.transpose(imerg_precip)               #New Precip Shape after transpose: (1800, 3600)\n",
    "\n",
    "    #mask blank data\n",
    "    imerg_precip_masked = np.ma.masked_where(imerg_precip < 0, imerg_precip)  #masks blank and bad data first (if blank data is -999 instead of NaN)\n",
    "    imerg_precip_masked = np.ma.masked_where(np.isnan(imerg_precip_masked), imerg_precip_masked)  #masks NaN values (not masked in previous line)\n",
    "\n",
    "    #creat the plot\n",
    "    ax = group_fig.add_subplot(2, 1, key+1, projection = data_proj)\n",
    "    \n",
    "    uwnd = ds.u.sel(time = file_date).sel(level = stream_pres)  #zonal winds (m/s)\n",
    "    uwnd = uwnd.sel(time = uwnd.time.dt.hour.isin(hr))       #zonal winds (m/s)\n",
    "\n",
    "    vwnd = ds.v.sel(time = file_date).sel(level = stream_pres)  #meridional winds (m/s)\n",
    "    vwnd = vwnd.sel(time = vwnd.time.dt.hour.isin(hr))       #meridional winds (m/s)\n",
    "    \n",
    "    rh = ds.r.sel(time = file_date).sel(level = stream_pres)  #meridional winds (m/s)\n",
    "    rh = rh.sel(time = rh.time.dt.hour.isin(hr))       #meridional winds (m/s)\n",
    "\n",
    "    ##Smoothing (source: Hannah Zanowski) --> not recommended, see top of document\n",
    "        ##Metpy smooth_n_point (data to be smoothed, number of points to use in smoothing (5 to 9 are valid), and number of times the smoother is applied)\n",
    "            ##see https://unidata.github.io/MetPy/latest/api/generated/metpy.calc.smooth_n_point.html for more info\n",
    "    #uwnd_smoothed = mpcalc.smooth_n_point(uwnd,9,10)\n",
    "    #vwnd_smoothed = mpcalc.smooth_n_point(vwnd,9,10)\n",
    "\n",
    "    #ax.set_title('%s %s UTC MIMIC TPW, GPM IMERG, and ERA5 %i hPa Streamlines' % (case_num, hr2, stream_pres))\n",
    "    if convective_type == 'Isolated':\n",
    "        ax.set_title('a) GPM IMERG and MIMIC TPW for an %s Convective Case' % (convective_type))\n",
    "    else:\n",
    "        ax.set_title('b) GPM IMERG and MIMIC TPW for an %s Convective Case' % (convective_type))\n",
    "    #ax.set_title('%s %s UTC ERA5 %i hPa Relative Humidity, \\nGPM IMERG, and ERA5 %i hPa Streamlines' % (case_num, hr2, stream_pres, stream_pres))\n",
    "    ax.set_extent(campaign_extent, ccrs.PlateCarree()) #lat/lon bounds are [West,East,South,North]\n",
    "\n",
    "    # Add land, coastlines, and borders\n",
    "    #ax.add_feature(cfeature.LAND, facecolor='0.8')\n",
    "    ax.coastlines(ls = '-', linewidth = 5, color = 'k')\n",
    "\n",
    "    #plot MIMIC TPW\n",
    "    tpw_levels = np.arange(0, 70.5, 1)\n",
    "    pm0 = ax.contourf(ds_tpw.lonArr, ds_tpw.latArr, ds_tpw.tpwGrid, levels = tpw_levels,\n",
    "                      extend = 'max', cmap = cm.jet, transform = data_proj, zorder = 0)\n",
    "#             pm0 = ax.pcolormesh(ds_tpw.lonArr, ds_tpw.latArr, ds_tpw.tpwGrid, vmin = 0, vmax = 70,\n",
    "#                                 cmap = cm.jet, transform = data_proj, zorder = 0)\n",
    "\n",
    "#     #plot ERA5 RH\n",
    "#     tpw_levels = np.arange(0, 100.1, 2)  #actually RH levels, but keeping the tpw_levels name because we use it elsewhere\n",
    "#     pm0 = ax.contourf(ds.longitude, ds.latitude, rh[0].values, levels = tpw_levels,\n",
    "#                       extend = 'max', cmap = cm.jet, transform = data_proj, alpha = 0.6, zorder = 1)\n",
    "# #             pm0 = ax.pcolormesh(ds.longitude, ds.latitude, rh[0].values, vmin = 0, vmax = 70,\n",
    "# #                                 cmap = cm.jet, transform = data_proj, zorder = 1)\n",
    "\n",
    "    #plot IMERG Rain Rate\n",
    "    pm1 = ax.contourf(imerg_lons, imerg_lats, imerg_precip_masked, \n",
    "                      levels = np.logspace(np.log10(0.1), np.log10(40), num = len(tpw_levels)), \n",
    "                      norm = 'log', extend = 'max', \n",
    "                      cmap = cm.jet, transform = data_proj, zorder = 1)\n",
    "#             pm1 = ax.pcolormesh(imerg_lons, imerg_lats, imerg_precip_masked, \n",
    "#                                 norm = mplc.LogNorm(vmin = 0.1, vmax = 40), \n",
    "#                                 cmap = cm.jet, transform = data_proj, zorder = 1)\n",
    "\n",
    "    #gridlines\n",
    "    gl = ax.gridlines(crs = ccrs.PlateCarree(), draw_labels = True, linewidth = 2, \n",
    "                      color = 'gray', alpha = 0.5, linestyle = '--', zorder = 3)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlabel_style = {'size':40, 'color':'black'}\n",
    "    gl.ylabel_style = {'size':40, 'color':'black'}\n",
    "\n",
    "    # #plot ERA5 streamlines\n",
    "    # ax.streamplot(ds.longitude, ds.latitude, uwnd[0].values, vwnd[0].values,\n",
    "    #               color = 'white', linewidth = 4, density = 1.0, \n",
    "    #               arrowsize = 6, arrowstyle = '->', \n",
    "    #               transform = data_proj, zorder = 3)  \n",
    "\n",
    "    #plot flight track\n",
    "    ax.plot(flight_lon, flight_lat, color = 'darkmagenta', linewidth = 5, zorder = 4)\n",
    "\n",
    "#     #plot dropsonde locations for the given flight\n",
    "#     for sonde in drop_coords_and_time:\n",
    "#         #ax.scatter(sonde[0], sonde[1], marker = f'${sonde[2]}$', color = 'b', s = 300)\n",
    "#         if sonde[2] in ['15:10', '15:43', '16:16', '16:42']:\n",
    "#             ax.scatter(sonde[0], sonde[1], marker = '*', color = 'white', edgecolor = 'k', zorder = 5, s = 6000)\n",
    "#         else:\n",
    "#             ax.scatter(sonde[0], sonde[1], marker = '*', color = 'k', zorder = 5, s = 6000)\n",
    "\n",
    "#     #same as above, but labeling the dropsondes by the order that they appear in the\n",
    "#         #Dropsonde_Metric_Calculations.csv, NOT IN CHRONOLOGICAL ORDER!!!\n",
    "#     #for z, sonde in enumerate(drop_coords_and_time):\n",
    "#         #ax.scatter(sonde[0], sonde[1], marker = f'${z + 1}$', color = 'b', s = 120, zorder = 5)\n",
    "\n",
    "    #plotting the colorbars\n",
    "    #cbar0 = group_fig.colorbar(pm0, ax = ax, orientation = 'vertical', shrink = 0.75, pad = 0.25)\n",
    "    #cbar0.set_label('TPW [mm]')\n",
    "    #cbar0.ax.yaxis.set_ticks_position('left')\n",
    "    #cbar0.ax.yaxis.set_label_position('left')\n",
    "\n",
    "    #this works with GeoAxes (i.e., Cartopy's map projections)\n",
    "    if key == len(case_dict_agu) - 1:\n",
    "\n",
    "        #MIMIC TPW colorbar\n",
    "        ticks_tpw = np.arange(0, 70.5, 10, dtype = int)\n",
    "        #cax = group_fig.add_axes([ax.get_position().x1+0.05, ax.get_position().y0, 0.02, ax.get_position().height])\n",
    "        cax0 = group_fig.add_axes([ax.get_position().x1 + 0.10, ax.get_position().y0, 0.04, 0.77])\n",
    "        cbar0 = group_fig.colorbar(pm0, cax = cax0, ticks = ticks_tpw)\n",
    "        cbar0.set_label('TPW [mm]')\n",
    "        cbar0.ax.set_yticklabels(list(map(str, list(ticks_tpw))))  #labels automatically default to tick values given to ticks parameter in fig.colorbar(), unless you're using a log scale I guess\n",
    "        cbar0.ax.yaxis.set_ticks_position('left')\n",
    "        cbar0.ax.yaxis.set_label_position('left')\n",
    "\n",
    "        # #ERA5 RH colorbar\n",
    "        # ticks_rh = np.arange(0, 100.5, 10, dtype = int)\n",
    "        # #cax = group_fig.add_axes([ax.get_position().x1+0.05, ax.get_position().y0, 0.02, ax.get_position().height])\n",
    "        # cax0 = group_fig.add_axes([ax.get_position().x1 + 0.10, ax.get_position().y0, 0.04, 0.77])\n",
    "        # cbar0 = group_fig.colorbar(pm0, cax = cax0, ticks = ticks_rh)\n",
    "        # cbar0.set_label('Relative Humidity [%]')\n",
    "        # cbar0.ax.set_yticklabels(list(map(str, list(ticks_rh))))  #labels automatically default to tick values given to ticks parameter in fig.colorbar(), unless you're using a log scale I guess\n",
    "        # cbar0.ax.yaxis.set_ticks_position('left')\n",
    "        # cbar0.ax.yaxis.set_label_position('left')\n",
    "\n",
    "        #IMERG colorbar\n",
    "        ticks_imerg = np.array([0.1, 1, 5, 10, 20, 40], dtype = float)\n",
    "        cax1 = group_fig.add_axes([ax.get_position().x1 + 0.10, ax.get_position().y0, 0.04, 0.77])\n",
    "        cbar1 = group_fig.colorbar(pm1, cax = cax1, ticks = ticks_imerg)\n",
    "        cbar1.set_label('IMERG [mm hr$\\\\bf{^{-1}}$]')\n",
    "        cbar1.ax.set_yticklabels(list(map(str, list(ticks_imerg))))  #labels automatically default to tick values given to ticks parameter in fig.colorbar(), unless you're using a log scale I guess\n",
    "        cbar1.ax.yaxis.set_ticks_position('right')\n",
    "        cbar1.ax.yaxis.set_label_position('right')\n",
    "                \n",
    "    ds_tpw.close()\n",
    "    ds_imerg.close()\n",
    "    ds.close()\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.subplots_adjust(hspace = 0.3)\n",
    "\n",
    "#save the figure\n",
    "\n",
    "#need to manually write out the whole filepath; for some reason, you can't use os.path.join() unless you're \n",
    "    #saving in the script's directory or you're saving within a subdirectory of the script's directory\n",
    "plot_save_name = '/Users/ben/Desktop/Figure2.png'\n",
    "\n",
    "plt.savefig(plot_save_name, bbox_inches = 'tight')\n",
    "#plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "#plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "plt.close()\n",
    "\n",
    "##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "im = Image.open(plot_save_name)\n",
    "\n",
    "try:\n",
    "    im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "except:\n",
    "    #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "    #though this line will have isolated, extremely minor image effects due to \n",
    "    #only using 256 colors instead of the 3-element RGB scale\n",
    "    im2 = im.convert('P')\n",
    "\n",
    "im2.save(plot_save_name)\n",
    "im.close()\n",
    "im2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f18b6-b3ba-4079-b5aa-611431eab8e2",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901b457-c0c2-4e4d-921a-1fcd5bea38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from CPEXCV_APR.ipynb and Figure 1 above\n",
    "    #see run_ecco_v_apr3_CPEXCV_AGU_paper.m, run_ecco_v_apr3_save1D.m, and eccoCases-cpexcv-apr3-full-flights_AGU.txt\n",
    "    #for the ECCO-V portion of this figure (Figure 3b-d)\n",
    "\n",
    "#the only cell with variables you need to change, unless you are not Ben (will need to change cell #3 too)\n",
    "file_date = '20220907'           #which date to create the APR plot for\n",
    "start_time0 = '20220907,165055'  #start date,time for the APR plot\n",
    "end_time0 = '20220907,171258'    #end date,time for the APR plot\n",
    "full = False                     #do the chosen time ranges below encapsulate the entire convective module?\n",
    "\n",
    "#other users will also have to change day_folder, apr_folder, dawn_csv_path, and drop_csv_path filepaths\n",
    "    #dawn_csv_path and drop_csv_path are based on created files from CPEXCV_DAWN.ipynb and CPEXCV_dropsonde(_metrics).ipynb\n",
    "\n",
    "start_time = datetime.strptime(start_time0, '%Y%m%d,%H%M%S')\n",
    "end_time = datetime.strptime(end_time0, '%Y%m%d,%H%M%S')\n",
    "\n",
    "#locations of the APR folder and APR files\n",
    "day_folder = os.path.join(os.getcwd(), file_date)\n",
    "apr_folder = os.path.join(day_folder, 'APR_files')\n",
    "\n",
    "#load the final DAWN and final Dropsonde CSVs\n",
    "dawn_csv_path = os.path.join(day_folder, 'final_dawn_' + file_date + '.csv')\n",
    "dawn_csv = pd.read_csv(dawn_csv_path)\n",
    "\n",
    "drop_csv_path = os.path.join(day_folder, 'final_dropsonde_' + file_date + '.csv')\n",
    "drop_csv = pd.read_csv(drop_csv_path)\n",
    "\n",
    "# #example CPEX-CV APR file\n",
    "# test = xr.open_dataset('/Users/ben/Desktop/CPEX/Coding/20220906/APR_files/cpexcv-APR3_DC8_20220906_R0_S20220906a121242_E20220906a121851_KUsKAsWns.nc')\n",
    "# test\n",
    "\n",
    "#create a list of all the given day's desired range's APR files:\n",
    "for x in os.listdir(apr_folder):\n",
    "    if x[0:3] == '.DS':         #delete hidden .DS_Store files if they come up (will show up if you delete a file)\n",
    "        os.remove(os.path.join(apr_folder, x))\n",
    "\n",
    "#find the starting APR file in apr_folder\n",
    "first_file_index = None \n",
    "\n",
    "#sorted() makes sure the code goes through the files in alphabetical (chronological) order\n",
    "#GOING THROUGH THE FILES IN CHRONOLOGICAL ORDER IS ESSENTIAL FOR THIS CELL TO WORK PROPERLY!!\n",
    "for i, x in enumerate(sorted(os.listdir(apr_folder))):  \n",
    "    file_start_time = datetime.strptime(x[29:37] + x[38:44], '%Y%m%d%H%M%S')\n",
    "    file_end_time = datetime.strptime(x[46:54] + x[55:61], '%Y%m%d%H%M%S')\n",
    "\n",
    "    if start_time <= file_start_time:  #if start_time is before the APR file start time and not within any previous APR file's time ranges\n",
    "        first_file_index = i\n",
    "        break\n",
    "    elif (start_time >= file_start_time) and (start_time < file_end_time):\n",
    "        first_file_index = i\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "if first_file_index == None:\n",
    "    sys.exit('Requested start_time is beyond all available APR files')\n",
    "    \n",
    "#find the ending APR file in apr_folder\n",
    "last_file_index = None    \n",
    "\n",
    "#sorted() makes sure the code goes through the files in alphabetical (chronological) order\n",
    "#GOING THROUGH THE FILES IN CHRONOLOGICAL ORDER IS ESSENTIAL FOR THIS CELL TO WORK PROPERLY!!\n",
    "for i, x in enumerate(sorted(os.listdir(apr_folder))):  \n",
    "    file_start_time = datetime.strptime(x[29:37] + x[38:44], '%Y%m%d%H%M%S')\n",
    "    file_end_time = datetime.strptime(x[46:54] + x[55:61], '%Y%m%d%H%M%S')\n",
    "\n",
    "    if end_time <= file_start_time:  #if end_time is before the APR file start time and not within any previous APR file's time ranges\n",
    "        last_file_index = i - 1\n",
    "        break\n",
    "    elif (end_time > file_start_time) and (end_time <= file_end_time):\n",
    "        last_file_index = i\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "if last_file_index == None:  #the end_time is after all available APR file ranges\n",
    "    last_file_index = len(sorted(os.listdir(apr_folder))) - 1  #the last available APR file's index\n",
    "if last_file_index == -1:\n",
    "    sys.exit('Requested end_time is before all available APR files')\n",
    "\n",
    "apr_file_list = sorted(os.listdir(apr_folder))[first_file_index:last_file_index + 1]\n",
    "#apr_file_list\n",
    "\n",
    "#Calculate time range in minutes, have a tick for each minute\n",
    "num_minutes = (end_time - start_time).total_seconds() // 60\n",
    "fig_length = num_minutes * 2\n",
    "\n",
    "#Calculate height range of the plot, based on the maximum height of all DAWN profiles\n",
    "height_max = dawn_csv['Height [m]'].max()\n",
    "y_max = math.ceil(height_max / 1000)  #round height_max up to the nearest 1000\n",
    "\n",
    "#matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 45\n",
    "matplotlib.rcParams['axes.titlesize'] = 55\n",
    "matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "matplotlib.rcParams['xtick.labelsize'] = 37\n",
    "matplotlib.rcParams['ytick.labelsize'] = 37\n",
    "matplotlib.rcParams['legend.fontsize'] = 35\n",
    "#matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "#matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "\n",
    "#plotting a time-series plot of just Ku-band reflectivity vs. height\n",
    "ylim=[0,y_max]\n",
    "vlim=[-10,40]\n",
    "vel_lim=[-10,10]\n",
    "\n",
    "#Low resolution ('lores') radar variables in APR hdf files\n",
    "ku_band = 'lores_zhh14' #Ku-band reflectivity\n",
    "ka_band = 'lores_zhh35' #Ka-band reflectivity\n",
    "vel = 'lores_vel14c' #Mean Doppler Velocity from Ku-band (surface Doppler velocity is subtracted and free of aliasing)\n",
    "w_band = 'lores_z95s' #W-band reflectivity\n",
    "w_band_nadir = 'hi2lo_z95n' #W-band reflectivity at nadir, upscaled to same resolution as 'lores'\n",
    "    #^^^for quantitative analysis, will likely want to switch to using 'hires_z95n'\n",
    "\n",
    "if full:\n",
    "    plot_save_name = file_date + '_SinglePanel_full.png'\n",
    "else:\n",
    "    plot_save_name = file_date + '_SinglePanel_' + start_time0[-6:] + '-' + end_time0[-6:] + '.png'\n",
    "\n",
    "#fig,axes = plt.subplots(1,1,figsize=(fig_length,14))\n",
    "fig,axes = plt.subplots(1,1,figsize=(48,14))   #use for AGU Paper Figure 12b (to match figsize for Figure 12a from CPEX_2017_APR.ipynb)\n",
    "\n",
    "for file in apr_file_list:\n",
    "    \n",
    "    #find the APR file of interest (for the desired datetime)\n",
    "    apr_filepath = os.path.join(apr_folder, file)\n",
    "    apr_file = xr.open_dataset(apr_filepath)\n",
    "\n",
    "    #grab the radar variables of interest\n",
    "    ku_good = False\n",
    "    if ku_band in apr_file.keys():\n",
    "        try:   #some APR files, at least in the preliminary data, have corrupted Ku-band data; \n",
    "               #if so, skip plotting the Ku-band for that file\n",
    "               #corrupted: \"OSError: Can't read data (inflate() failed)\"\n",
    "            ku_data = apr_file[ku_band][:]\n",
    "\n",
    "            #mask the missing Ku-band data (values of -99.99)\n",
    "            ku_masked = np.ma.masked_where(ku_data <= -99, ku_data)\n",
    "            ku_masked = np.ma.masked_where(np.isnan(ku_masked), ku_masked)  #masks NaN values (not masked in previous line)\n",
    "            ku_good = True\n",
    "        except:\n",
    "            ku_good = False\n",
    "    \n",
    "    #if Ku-band is available\n",
    "    if ku_good:            \n",
    "\n",
    "        #Convert 'lo-res' APR times to datetimes\n",
    "        time = apr_file['time'][:]  #For 'lores': Time of scan, in seconds since midnight UTC of [YYYY-mm-DD]\n",
    "        alt3d = apr_file['lores_alt3D'][:]\n",
    "\n",
    "        time_dates = np.empty(time.shape, dtype=object)\n",
    "        for i in np.arange(0, time.shape[0]):\n",
    "            #hour, second automatically revert to midnight (hour = 0, seconds = 0) for '%Y%m%d'\n",
    "            time_dates[i] = datetime.strptime(file_date, '%Y%m%d') + timedelta(seconds = float(time[i].values))\n",
    "\n",
    "        #Create a time at each gate (assuming time is the same for each ray of a given scan and down each ray)      \n",
    "        time_gate = np.empty(alt3d.shape, dtype=object)\n",
    "        for i in np.arange(0, alt3d.shape[0]):\n",
    "            time_gate[i,:,:] = time_dates[i]   #assign the same time to all of a given scan's rays and height bins     \n",
    "\n",
    "        time3d = np.copy(time_gate)\n",
    "\n",
    "#         print (time3d[0,0,:])  #times should be the same\n",
    "#         print ('')\n",
    "#         print (time3d[0,:,0])  #times should be the same\n",
    "#         print ('')\n",
    "#         print (time3d[:,0,0])  #times should be different\n",
    "#         sys.exit()\n",
    "\n",
    "        #plot the APR data factoring in the aircraft roll (ray adjustment)\n",
    "        #choose the \"pseudo-nadir\" ray factoring in aircraft roll\n",
    "        roll = apr_file['lores_roll'][:]\n",
    "        ray_angles = np.arange(-25,25.01,25/12)  #in degrees; 25 rays for each scan, with the middle (13th) scan being zero degrees\n",
    "        for scan in range(roll.shape[0]):\n",
    "            if (time_dates[scan] >= start_time) and (time_dates[scan] <= end_time):\n",
    "                ac_roll = np.nanmean(roll[scan,:])  #roll varies slightly w/ray, so take the average roll value for a given scan and use that for ray adjustment\n",
    "                ray_use = np.argmin(np.abs(ray_angles - ac_roll))  #the index of the ray whose angle is closest to that of ac_roll    \n",
    "\n",
    "                #scan + 2 (and not scan + 1) is needed because pcolormesh colors the grid cell from the  \n",
    "                #grid cell's time to the subsequent grid cell's time.  If a subsequent grid cell does not exist,  \n",
    "                #then pcolormesh cannot/doesn't color the grid cell (remember, slicing is right side EXCLUSIVE, \n",
    "                #so scan:scan + 1 is only 1 element and thus doesn't have a subsequent cell!)\n",
    "                #by this same logic, scan:scan + 2 will only color one grid cell, since the 2nd (and last) \n",
    "                #element/grid cell doesn't have a subsequent grid cell\n",
    "\n",
    "                if ku_good:\n",
    "                    pm0 = axes.pcolormesh(time3d[scan:scan+2,ray_use,:], alt3d[scan:scan+2,ray_use,:] / 1000,\n",
    "                                          ku_masked[scan:scan+2,ray_use,:], cmap=cm.Spectral_r, vmin=vlim[0], vmax=vlim[1])\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    apr_file.close()\n",
    "\n",
    "cbar0 = plt.colorbar(pm0, ax=axes, pad = 0.01, extend = 'both')\n",
    "cbar0.set_label('Z [dBZ]')\n",
    "#axes.set_title('ECCO-V Classifications and APR-3 Ku-band Reflectivity (Case 12, 07 September 2022)')\n",
    "\n",
    "# dawn_skip = slice(None,None,7)   #only plot every 7th value\n",
    "# drop_skip = slice(None,None,20)  #only plot every 20th value\n",
    "dawn_skip = slice(None,None,10)   #only plot every 10th value\n",
    "drop_skip = slice(None,None,50)  #only plot every 50th value\n",
    "\n",
    "#set the plot start time as the beginning time of the first APR file and the\n",
    "    #plot end time as the end time of the last APR file:\n",
    "# range_start = datetime.strptime(apr_file_list[0][32:40] + apr_file_list[0][41:47], '%Y%m%d%H%M%S') \n",
    "# range_end = datetime.strptime(apr_file_list[-1][49:57] + apr_file_list[-1][58:64], '%Y%m%d%H%M%S')\n",
    "# range_start = start_time\n",
    "# range_end = end_time\n",
    "range_start = datetime.strptime(start_time0, '%Y%m%d,%H%M%S') \n",
    "range_end = datetime.strptime(end_time0, '%Y%m%d,%H%M%S')\n",
    "\n",
    "for xx, dawn_profile in enumerate(dawn_csv['Time [UTC]'].unique()):\n",
    "    if xx % 2 == 0:  #skip plotting every other DAWN profile\n",
    "        dawn_csv_prof = dawn_csv[dawn_csv['Time [UTC]'] == dawn_profile]\n",
    "        axes.barbs(pd.to_datetime(dawn_csv_prof['Time [UTC]'])[dawn_skip], dawn_csv_prof['Height [m]'][dawn_skip] / 1000, \n",
    "                   dawn_csv_prof['U Comp of Wind [m/s]'][dawn_skip], dawn_csv_prof['V Comp of Wind [m/s]'][dawn_skip], \n",
    "                   fill_empty = True, length = 10, pivot='middle', sizes=dict(emptybarb=0.1), barbcolor = 'k')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# axes.barbs(pd.to_datetime(dawn_csv['Time [UTC]'])[dawn_skip], dawn_csv['Height [m]'][dawn_skip], \n",
    "#            dawn_csv['U Comp of Wind [m/s]'][dawn_skip], dawn_csv['V Comp of Wind [m/s]'][dawn_skip], \n",
    "#            fill_empty = True, length = 10, pivot='middle', sizes=dict(emptybarb=0.1), barbcolor = 'k')\n",
    "axes.barbs(pd.to_datetime(drop_csv['Time [UTC]'])[drop_skip], drop_csv['Height [m]'][drop_skip] / 1000, \n",
    "           drop_csv['U Comp of Wind [m/s]'][drop_skip], drop_csv['V Comp of Wind [m/s]'][drop_skip], \n",
    "           fill_empty = True, length = 10, pivot='middle', sizes=dict(emptybarb=0.1), barbcolor = 'b')\n",
    "\n",
    "##########################\n",
    "\n",
    "axes.set_ylabel('Altitude [km]')\n",
    "axes.set_xlabel('Time [UTC]')\n",
    "axes.set_ylim([ylim[0],ylim[1]])\n",
    "axes.tick_params(axis='x', rotation = 50)\n",
    "axes.tick_params(length = 15, width = 5)\n",
    "axes.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%H:%M:%S\"))\n",
    "axes.xaxis.set_major_locator(ticker.MaxNLocator(num_minutes))      #sets number of ticks\n",
    "axes.set_xlim([np.datetime64(range_start),np.datetime64(range_end)])    \n",
    "    #use the above line to narrow the plot's time range (even within a file!!)\n",
    "        #range_start and range_end must be a datetime object or a string with the \n",
    "            #format: 'YYYY-MM-DD HH:MM:SS' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#save the figure\n",
    "#plt.savefig(os.path.join(day_folder, plot_save_name), bbox_inches = 'tight')\n",
    "plt.savefig('/Users/ben/Desktop/Figure3a.png', bbox_inches = 'tight')\n",
    "#plt.show()  #if you want to also show the image in the output cell, plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "plt.close()\n",
    "\n",
    "\n",
    "##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "from PIL import Image\n",
    "im = Image.open('/Users/ben/Desktop/Figure3a.png')\n",
    "\n",
    "try:\n",
    "    im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "except:\n",
    "    #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "    #though this line will have isolated, extremely minor image effects due to \n",
    "    #only using 256 colors instead of the 3-element RGB scale\n",
    "    im2 = im.convert('P')\n",
    "\n",
    "im2.save('/Users/ben/Desktop/Figure3a.png')\n",
    "im.close()\n",
    "im2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8681554-ce4b-4d15-b415-02ca87e1a2e8",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figures 4, 6, and 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaf20c4-c8d2-4ad7-9735-10ed64c6d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from metric_boxwhisker_4panel_CPEXCV.py\n",
    "\n",
    "# matplotlib.rcParams['axes.labelsize'] = 14\n",
    "# matplotlib.rcParams['axes.titlesize'] = 14\n",
    "# matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "# matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "# matplotlib.rcParams['legend.fontsize'] = 12\n",
    "# matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "\n",
    "TC_days = [20170619, 20170620, 20210826, 20210828, 20210901, 20210904, 20220923]  #including TDs as TCs (from both CPEX and CPEX-AW and CPEX-CV)\n",
    "\n",
    "#drop_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations.csv')\n",
    "drop_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "df = pd.read_csv(drop_filepath)\n",
    "df_noTC = df[~df['Date'].isin(TC_days)].copy()  #filter out cases associated with a TD/TC\n",
    "\n",
    "#dawn_filepath = os.path.join(os.getcwd(), 'DAWN_Shear_Calculations.csv')\n",
    "dawn_filepath = os.path.join(os.getcwd(), 'DAWN_Shear_Calculations_CPEXCV.csv')\n",
    "df_dawn = pd.read_csv(dawn_filepath)\n",
    "\n",
    "df1 = pd.concat([df, df_dawn], ignore_index = True)  #concatenates fields with same heading\n",
    "df1_noTC = df1[~df1['Date'].isin(TC_days)].copy()  #filter out cases associated with a TD/TC\n",
    "\n",
    "#df1_noTC = df_noTC.copy() #not sure what this was for....probably can delete\n",
    "\n",
    "rh_layers = ['Deep Layer RH [%]', 'PBL RH [%]', 'Mid Layer RH [%]', 'Upper Layer RH [%]']\n",
    "speed_shear_layers = ['SHARPpy Direct Method Deep Layer Speed Shear [kts]', 'SHARPpy Direct Method PBL Speed Shear [kts]', \n",
    "                      'SHARPpy Direct Method Mid Layer Speed Shear [kts]', 'SHARPpy Direct Method Upper Layer Speed Shear [kts]']\n",
    "cape_layers = ['Deep Layer MUCAPE [J/kg]', 'Deep Layer MLCAPE [J/kg]', 'Above FZL MUCAPE [J/kg]', 'Above FZL MLCAPE [J/kg]']\n",
    "\n",
    "use_alpha = 0.25  #alpha for box plots with all sondes, not just inflow sondes\n",
    "\n",
    "#for convective type comparison of All Lifecycles dropsondes\n",
    "def box_plot_onlyIsoOrg_RH(plotting_metric_name, ax):  #parameter will be a data column from df\n",
    "    \n",
    "    if plotting_metric_name == 'Upper Layer RH [%]':  #include \"In Precip\" sondes if \"Partially In Precip\" == Yes\n",
    "        df_use0 = df_noTC[df_noTC['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "        df_use1 = df_use0[(df_use0['Environment Falling In'] != 'In Precip') | (df_use0['Partially In Precip'] == 'Yes')].copy()  #filter out In Precip, unless \"Partially In Precip\" == Yes\n",
    "        df_use = df_use1[(df_use1['Low-level Inflow Sonde'] == 'Yes') | (df_use1['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "        #df_use = df_noTC[(df_noTC['Environment Falling In'] != 'In Precip') & (df_noTC['Environment Falling In'] != 'Clear Far')].copy()  #filter out In Precip and Clear Far dropsondes\n",
    "    else:\n",
    "        df_use1 = df_noTC[(df_noTC['Environment Falling In'] != 'In Precip') & (df_noTC['Environment Falling In'] != 'Clear Far')].copy()  #filter out In Precip and Clear Far dropsondes\n",
    "        df_use = df_use1[(df_use1['Low-level Inflow Sonde'] == 'Yes') | (df_use1['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "\n",
    "    #all sondes\n",
    "    df_iso_all = df_use1[df_use1['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all = df_use1[df_use1['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all = df_use1[df_use1['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso = df_use[df_use['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org = df_use[df_use['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat = df_use[df_use['Primary Convective Type'] == 'Scattered'].copy()\n",
    "    \n",
    "    #need to filter out NaN values (using dropna()), otherwise the boxplot() won't create anything\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_scat[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                 patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated', 'Organized', 'Scattered'])\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_scat[plotting_metric_name].dropna().values, df_scat_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                  patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized', 'Scattered\\n(Inflow)', 'Scattered'])\n",
    "    bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "                     df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "                     patch_artist = True, vert = True, widths = 0.65, tick_labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized'])\n",
    "    \n",
    "        \n",
    "    print (f'Isolated {plotting_metric_name} median:', df_iso[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median:', df_org[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median:', df_scat[plotting_metric_name].median(skipna = True))\n",
    "\n",
    "    # colors = ['red', 'blue', 'black']\n",
    "    #colors = ['red', 'red', 'blue', 'blue', 'black', 'black']\n",
    "    colors = ['red', 'red', 'blue', 'blue']\n",
    "    nums = list(range(len(colors)))\n",
    "     \n",
    "    for ii, patch, color in zip(nums, bp['boxes'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            patch.set_facecolor(color)\n",
    "        else:\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(use_alpha)\n",
    "\n",
    "    #changing color and linewidth of medians\n",
    "    for ii, median in enumerate(bp['medians']):\n",
    "        if ii % 2 == 0:\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "        else:\n",
    "            #median.set(color = 'white', alpha = use_alpha, linewidth = 3)\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "\n",
    "    #changing color and linewidth of whiskers\n",
    "    for ii, whisker in enumerate(bp['whiskers']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 whiskers, not 4\n",
    "            whisker.set(color = 'k', linewidth = 2, linestyle = \"-\")\n",
    "        else:\n",
    "            whisker.set(color = 'k', alpha = use_alpha, linewidth = 2, linestyle = \"-\")\n",
    "     \n",
    "    #changing color and linewidth of caps\n",
    "    for ii, cap in enumerate(bp['caps']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 caps, not 4\n",
    "            cap.set(color = 'k', linewidth = 2)\n",
    "        else:\n",
    "            cap.set(color = 'k', alpha = use_alpha, linewidth = 2)\n",
    "     \n",
    "    #changing style of fliers\n",
    "    for ii, flier, color in zip(nums, bp['fliers'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            flier.set(marker = 'o', color = 'k', markersize = 13, markerfacecolor = color)\n",
    "        else:\n",
    "            flier.set(marker = 'o', color = 'k', alpha = use_alpha, markersize = 13, markerfacecolor = color)\n",
    "            \n",
    "    ax.grid(True, axis = 'y')\n",
    "    ax.set_title(plotting_metric_name[:-4], fontsize = 33, fontweight = 'bold')\n",
    "    #ax.set_xlabel('Convective Type', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[%]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([15,100])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    " \n",
    "    \n",
    "#group_fig = plt.figure(figsize=(36,24))\n",
    "group_fig = plt.figure(figsize=(22,24))\n",
    "\n",
    "for i, layer in enumerate(rh_layers):\n",
    "    ax = group_fig.add_subplot(2,2,i+1)\n",
    "    box_plot_onlyIsoOrg_RH(layer, ax)\n",
    "\n",
    "#custom legend\n",
    "# legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 's', markersize = 13, label='Isolated (In Precip Profiles Excluded)'),\n",
    "#                    Line2D([], [], color='blue', linewidth = 0, marker = 's', markersize = 13, label='Organized (In Precip Profiles Excluded)')]\n",
    "# group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 25)\n",
    "\n",
    "group_fig.text(0.5, 0.484, 'In Precip Profiles Excluded', horizontalalignment='center', verticalalignment='center', \n",
    "               fontsize = 40, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "plt.subplots_adjust(hspace = 0.45, wspace = 0.35)\n",
    "plt.savefig('/Users/ben/Desktop/RH_box_4panel.png', bbox_inches = 'tight')\n",
    "plt.close()\n",
    "print ('')\n",
    "\n",
    "\n",
    "#for convective type comparison of All Lifecycles dropsondes\n",
    "def box_plot_onlyIsoOrg_SS(plotting_metric_name, ax):  #parameter will be a data column from df\n",
    "    \n",
    "    df_use1 = df_noTC[df_noTC['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "    df_use = df_use1[(df_use1['Low-level Inflow Sonde'] == 'Yes') | (df_use1['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "    \n",
    "    #all sondes\n",
    "    df_iso_all = df_use1[df_use1['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all = df_use1[df_use1['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all = df_use1[df_use1['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso = df_use[df_use['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org = df_use[df_use['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat = df_use[df_use['Primary Convective Type'] == 'Scattered'].copy()\n",
    "    \n",
    "    #need to filter out NaN values (using dropna()), otherwise the boxplot() won't create anything\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_scat[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                 patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated', 'Organized', 'Scattered'])\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_scat[plotting_metric_name].dropna().values, df_scat_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                  patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized', 'Scattered\\n(Inflow)', 'Scattered'])\n",
    "    bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "                     df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "                     patch_artist = True, vert = True, widths = 0.65, tick_labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized'])\n",
    "        \n",
    "    print (f'Isolated {plotting_metric_name} median:', df_iso[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median:', df_org[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median:', df_scat[plotting_metric_name].median(skipna = True))\n",
    "\n",
    "    # colors = ['red', 'blue', 'black']\n",
    "    #colors = ['red', 'red', 'blue', 'blue', 'black', 'black']\n",
    "    colors = ['red', 'red', 'blue', 'blue']\n",
    "    nums = list(range(len(colors)))\n",
    "     \n",
    "    for ii, patch, color in zip(nums, bp['boxes'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            patch.set_facecolor(color)\n",
    "        else:\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(use_alpha)\n",
    "\n",
    "    #changing color and linewidth of medians\n",
    "    for ii, median in enumerate(bp['medians']):\n",
    "        if ii % 2 == 0:\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "        else:\n",
    "            #median.set(color = 'white', alpha = use_alpha, linewidth = 3)\n",
    "            median.set(color = 'white', linewidth = 3)        \n",
    "\n",
    "    #changing color and linewidth of whiskers\n",
    "    for ii, whisker in enumerate(bp['whiskers']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 whiskers, not 4\n",
    "            whisker.set(color = 'k', linewidth = 2, linestyle = \"-\")\n",
    "        else:\n",
    "            whisker.set(color = 'k', alpha = use_alpha, linewidth = 2, linestyle = \"-\")\n",
    "     \n",
    "    #changing color and linewidth of caps\n",
    "    for ii, cap in enumerate(bp['caps']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 caps, not 4\n",
    "            cap.set(color = 'k', linewidth = 2)\n",
    "        else:\n",
    "            cap.set(color = 'k', alpha = use_alpha, linewidth = 2)\n",
    "     \n",
    "    #changing style of fliers\n",
    "    for ii, flier, color in zip(nums, bp['fliers'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            flier.set(marker = 'o', color = 'k', markersize = 13, markerfacecolor = color)\n",
    "        else:\n",
    "            flier.set(marker = 'o', color = 'k', alpha = use_alpha, markersize = 13, markerfacecolor = color)\n",
    "            \n",
    "    ax.grid(True, axis = 'y')\n",
    "    ax.set_title(plotting_metric_name[22:-18] + ' Shear', fontsize = 33, fontweight = 'bold')\n",
    "    #ax.set_xlabel('Convective Type', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[kts]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([0,50])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    " \n",
    "#group_fig = plt.figure(figsize=(36,24))\n",
    "group_fig = plt.figure(figsize=(22,24))\n",
    "\n",
    "for i, layer in enumerate(speed_shear_layers):\n",
    "    ax = group_fig.add_subplot(2,2,i+1)\n",
    "    box_plot_onlyIsoOrg_SS(layer, ax)\n",
    "\n",
    "#custom legend\n",
    "# legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 's', markersize = 13, label='Isolated'),\n",
    "#                    Line2D([], [], color='blue', linewidth = 0, marker = 's', markersize = 13, label='Organized')]\n",
    "# group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 25)\n",
    "\n",
    "group_fig.text(0.5, 0.484, 'In Precip Profiles Included', horizontalalignment='center', verticalalignment='center', \n",
    "               fontsize = 40, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "plt.subplots_adjust(hspace = 0.45, wspace = 0.35)\n",
    "plt.savefig('/Users/ben/Desktop/SS_box_4panel.png', bbox_inches = 'tight')\n",
    "plt.close()\n",
    "print ('')\n",
    "\n",
    "\n",
    "#for convective type comparison of All Lifecycles dropsondes\n",
    "def box_plot_onlyIsoOrg_CAPE(plotting_metric_name, ax):  #parameter will be a data column from df\n",
    "    \n",
    "    df_use1 = df_noTC[(df_noTC['Environment Falling In'] != 'In Precip') & (df_noTC['Environment Falling In'] != 'Clear Far')].copy()  #filter out In Precip and Clear Far dropsondes\n",
    "    df_use = df_use1[(df_use1['Low-level Inflow Sonde'] == 'Yes') | (df_use1['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "    \n",
    "    #all sondes\n",
    "    df_iso_all = df_use1[df_use1['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all = df_use1[df_use1['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all = df_use1[df_use1['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso = df_use[df_use['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org = df_use[df_use['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat = df_use[df_use['Primary Convective Type'] == 'Scattered'].copy()\n",
    "    \n",
    "    #need to filter out NaN values (using dropna()), otherwise the boxplot() won't create anything\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_scat[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                 patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated', 'Organized', 'Scattered'])\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_scat[plotting_metric_name].dropna().values, df_scat_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                  patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized', 'Scattered\\n(Inflow)', 'Scattered'])\n",
    "    bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "                     df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "                     patch_artist = True, vert = True, widths = 0.65, tick_labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized'])\n",
    "        \n",
    "    print (f'Isolated {plotting_metric_name} median:', df_iso[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median:', df_org[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median:', df_scat[plotting_metric_name].median(skipna = True))\n",
    "\n",
    "    # colors = ['red', 'blue', 'black']\n",
    "    #colors = ['red', 'red', 'blue', 'blue', 'black', 'black']\n",
    "    colors = ['red', 'red', 'blue', 'blue']\n",
    "    nums = list(range(len(colors)))\n",
    "     \n",
    "    for ii, patch, color in zip(nums, bp['boxes'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            patch.set_facecolor(color)\n",
    "        else:\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(use_alpha)\n",
    "\n",
    "    #changing color and linewidth of medians\n",
    "    for ii, median in enumerate(bp['medians']):\n",
    "        if ii % 2 == 0:\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "        else:\n",
    "            #median.set(color = 'white', alpha = use_alpha, linewidth = 3)\n",
    "            median.set(color = 'white', linewidth = 3)        \n",
    "\n",
    "    #changing color and linewidth of whiskers\n",
    "    for ii, whisker in enumerate(bp['whiskers']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 whiskers, not 4\n",
    "            whisker.set(color = 'k', linewidth = 2, linestyle = \"-\")\n",
    "        else:\n",
    "            whisker.set(color = 'k', alpha = use_alpha, linewidth = 2, linestyle = \"-\")\n",
    "     \n",
    "    #changing color and linewidth of caps\n",
    "    for ii, cap in enumerate(bp['caps']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 caps, not 4\n",
    "            cap.set(color = 'k', linewidth = 2)\n",
    "        else:\n",
    "            cap.set(color = 'k', alpha = use_alpha, linewidth = 2)\n",
    "     \n",
    "    #changing style of fliers\n",
    "    for ii, flier, color in zip(nums, bp['fliers'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            flier.set(marker = 'o', color = 'k', markersize = 13, markerfacecolor = color)\n",
    "        else:\n",
    "            flier.set(marker = 'o', color = 'k', alpha = use_alpha, markersize = 13, markerfacecolor = color)\n",
    "            \n",
    "    ax.grid(True, axis = 'y')\n",
    "    if 'Above FZL' in plotting_metric_name:\n",
    "        ax.set_title('Upper Layer' + plotting_metric_name[9:-7], fontsize = 33, fontweight = 'bold')\n",
    "    else:\n",
    "        ax.set_title(plotting_metric_name[:-7], fontsize = 33, fontweight = 'bold')\n",
    "    #ax.set_xlabel('Convective Type', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[J kg$^{-1}$]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([15,100])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    \n",
    "    # #make the deep layer plot axes range the same and the above FZL plot axes the same for easier comparison\n",
    "    # if plotting_metric_name[:4] == 'Deep':\n",
    "    #     ax.set_ylim([-30,1450])\n",
    "    # else:\n",
    "    #     ax.set_ylim([-30,1000])\n",
    " \n",
    "#group_fig = plt.figure(figsize=(36,24))\n",
    "group_fig = plt.figure(figsize=(22,24))\n",
    "\n",
    "for i, layer in enumerate(cape_layers):\n",
    "    ax = group_fig.add_subplot(2,2,i+1)\n",
    "    box_plot_onlyIsoOrg_CAPE(layer, ax)\n",
    "    \n",
    "#custom legend\n",
    "# legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 's', markersize = 13, label='Isolated (In Precip Profiles Excluded)'),\n",
    "#                    Line2D([], [], color='blue', linewidth = 0, marker = 's', markersize = 13, label='Organized (In Precip Profiles Excluded)')]\n",
    "# group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 25)\n",
    "\n",
    "group_fig.text(0.5, 0.484, 'In Precip Profiles Excluded', horizontalalignment='center', verticalalignment='center', \n",
    "               fontsize = 40, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "plt.subplots_adjust(hspace = 0.45, wspace = 0.35)\n",
    "plt.savefig('/Users/ben/Desktop/CAPE_box_4panel.png', bbox_inches = 'tight')\n",
    "plt.close()\n",
    "print ('')\n",
    "\n",
    "\n",
    "#for convective type comparison of All Lifecycles dropsondes\n",
    "def box_plot_onlyIsoOrg_PBL(plotting_metric_name):  #parameter will be a data column from df\n",
    "\n",
    "    #group_fig = plt.figure(figsize=(40,21))\n",
    "    group_fig = plt.figure(figsize=(32,21))\n",
    "    \n",
    "    #plot the PBL Top box and whisker plot\n",
    "    ax = group_fig.add_subplot(1,2,1)\n",
    "    \n",
    "    df_use1 = df_noTC[(df_noTC['Environment Falling In'] != 'In Precip') & (df_noTC['Environment Falling In'] != 'Clear Far')].copy()  #filter out In Precip and Clear Far dropsondes\n",
    "    df_use = df_use1[(df_use1['Low-level Inflow Sonde'] == 'Yes') | (df_use1['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "    \n",
    "    #all sondes\n",
    "    df_iso_all = df_use1[df_use1['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all = df_use1[df_use1['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all = df_use1[df_use1['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso = df_use[df_use['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org = df_use[df_use['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat = df_use[df_use['Primary Convective Type'] == 'Scattered'].copy()\n",
    "    \n",
    "    #need to filter out NaN values (using dropna()), otherwise the boxplot() won't create anything\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_scat[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                 patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated', 'Organized', 'Scattered'])\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_scat[plotting_metric_name].dropna().values, df_scat_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                  patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized', 'Scattered\\n(Inflow)', 'Scattered'])\n",
    "    bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "                     df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "                     patch_artist = True, vert = True, widths = 0.65, tick_labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized'])\n",
    "        \n",
    "    print (f'Isolated {plotting_metric_name} median:', df_iso[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median:', df_org[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median:', df_scat[plotting_metric_name].median(skipna = True))\n",
    "\n",
    "    # colors = ['red', 'blue', 'black']\n",
    "    #colors = ['red', 'red', 'blue', 'blue', 'black', 'black']\n",
    "    colors = ['red', 'red', 'blue', 'blue']\n",
    "    nums = list(range(len(colors)))\n",
    "     \n",
    "    for ii, patch, color in zip(nums, bp['boxes'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            patch.set_facecolor(color)\n",
    "        else:\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(use_alpha)\n",
    "\n",
    "    #changing color and linewidth of medians\n",
    "    for ii, median in enumerate(bp['medians']):\n",
    "        if ii % 2 == 0:\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "        else:\n",
    "            #median.set(color = 'white', alpha = use_alpha, linewidth = 3)\n",
    "            median.set(color = 'white', linewidth = 3)        \n",
    "\n",
    "    #changing color and linewidth of whiskers\n",
    "    for ii, whisker in enumerate(bp['whiskers']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 whiskers, not 4\n",
    "            whisker.set(color = 'k', linewidth = 2, linestyle = \"-\")\n",
    "        else:\n",
    "            whisker.set(color = 'k', alpha = use_alpha, linewidth = 2, linestyle = \"-\")\n",
    "     \n",
    "    #changing color and linewidth of caps\n",
    "    for ii, cap in enumerate(bp['caps']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 caps, not 4\n",
    "            cap.set(color = 'k', linewidth = 2)\n",
    "        else:\n",
    "            cap.set(color = 'k', alpha = use_alpha, linewidth = 2)\n",
    "     \n",
    "    #changing style of fliers\n",
    "    for ii, flier, color in zip(nums, bp['fliers'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            flier.set(marker = 'o', color = 'k', markersize = 13, markerfacecolor = color)\n",
    "        else:\n",
    "            flier.set(marker = 'o', color = 'k', alpha = use_alpha, markersize = 13, markerfacecolor = color)\n",
    "            \n",
    "    ax.grid(True, axis = 'y')\n",
    "    ax.set_title('PBL Depth', fontsize = 33, fontweight = 'bold')\n",
    "    ax.set_xlabel('Convective Type', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[mb]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([1010,900])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks(np.arange(1010,899,-10))\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    ax.text(0.50, 0.97, 'In Precip Profiles Excluded', horizontalalignment='center', verticalalignment='center', \n",
    "            transform=ax.transAxes, fontsize = 30, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 5})\n",
    "    \n",
    "    #plot the PBL Top scatter plot\n",
    "    ax = group_fig.add_subplot(1,2,2)\n",
    "    \n",
    "    plotting_metric = df[plotting_metric_name]\n",
    "    xlist = []\n",
    "    for i in range(len(df)):\n",
    "        xstring = str(df['Case'][i])\n",
    "        xlist.append(xstring)\n",
    "    \n",
    "    # legend_elements_PBL = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 13, label='Isolated (Clear)'),\n",
    "    #                       Line2D([], [], color='red', linewidth = 0, marker = '$C$', markersize = 13, label='Isolated (In Cloud)'),\n",
    "    #                       Line2D([], [], color='red', linewidth = 0, marker = '$P$', markersize = 13, label='Isolated (In Precip)'),\n",
    "    #                       Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 13, label='Organized (Clear)'),\n",
    "    #                       Line2D([], [], color='blue', linewidth = 0, marker = '$C$', markersize = 13, label='Organized (In Cloud)'),\n",
    "    #                       Line2D([], [], color='blue', linewidth = 0, marker = '$P$', markersize = 13, label='Organized (In Precip)'),\n",
    "    #                       Line2D([], [], color='black', linewidth = 0, marker = 'o', markersize = 13, label='Scattered (Clear)'),\n",
    "    #                       Line2D([], [], color='black', linewidth = 0, marker = '$C$', markersize = 13, label='Scattered (In Cloud)'),\n",
    "    #                       Line2D([], [], color='black', linewidth = 0, marker = '$P$', markersize = 13, label='Scattered (In Precip)')]\n",
    "    \n",
    "    # legend_elements_PBL = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 13, label='Isolated'),\n",
    "    #                Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 13, label='Organized'),\n",
    "    #                Line2D([], [], color='black', linewidth = 0, marker = 'o', markersize = 13, label='Scattered')]\n",
    "\n",
    "    legend_elements_PBL = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 13, label='Isolated (Clear)'),\n",
    "                          Line2D([], [], color='red', linewidth = 0, marker = '$C$', markersize = 13, label='Isolated (In Cloud)'),\n",
    "                          Line2D([], [], color='red', linewidth = 0, marker = '$P$', markersize = 13, label='Isolated (In Precip)'),\n",
    "                          Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 13, label='Organized (Clear)'),\n",
    "                          Line2D([], [], color='blue', linewidth = 0, marker = '$C$', markersize = 13, label='Organized (In Cloud)'),\n",
    "                          Line2D([], [], color='blue', linewidth = 0, marker = '$P$', markersize = 13, label='Organized (In Precip)')]\n",
    "    \n",
    "    #the lighter shades account for TS Cindy organized cases and, further, TS Cindy cases that were away from the main organized convection and instead near the cyclonic center\n",
    "    for j in range(len(plotting_metric)):\n",
    "        #if (df['Convective Lifecycle'][j] != 'Weakening') and (df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip'):\n",
    "        if df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip':\n",
    "        #if (df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip') and (df['Low-level Inflow Sonde'][j] == 'Yes' or df['Mid-level Inflow Sonde'][j] == 'Yes'):\n",
    "            if df['Primary Convective Type'][j] == 'Isolated':\n",
    "                color = 'red'\n",
    "                if df['Date'][j] in TC_days:\n",
    "                    continue\n",
    "            elif df['Primary Convective Type'][j] == 'Organized':\n",
    "                color = 'blue'\n",
    "                if df['Date'][j] in TC_days:\n",
    "                    continue\n",
    "            elif df['Primary Convective Type'][j] == 'Scattered':\n",
    "                color = 'black'\n",
    "                if df['Date'][j] in TC_days:\n",
    "                    continue               \n",
    "            else:\n",
    "                continue\n",
    "                  \n",
    "            if df['Environment Falling In'][j] == 'In Precip':\n",
    "                if df['Partially In Precip'][j] == 'Yes':\n",
    "                    mark = '$*P$'\n",
    "                    mark = '$P$'\n",
    "                else:\n",
    "                    mark = '$P$'\n",
    "                outline = None\n",
    "            elif df['Environment Falling In'][j] == 'In Cloud':\n",
    "                if df['Falling Through Weak Stratiform (Onion-ish Profile (Typically On Outer Edge of Storm))'][j] == 'Yes':\n",
    "                    mark = '$*C$'\n",
    "                    mark = '$C$'\n",
    "                else:\n",
    "                    mark = '$C$'\n",
    "                outline = None\n",
    "            else:\n",
    "                mark = 'o'\n",
    "                outline = 'black'\n",
    "\n",
    "            #if the metric is not NaN (need this filter, otherwise Python raises an error when showing/saving the figure)\n",
    "            if not np.isnan(plotting_metric[j]):  #could also use pd.isna(plotting_metric[j])\n",
    "                if df['Low-level Inflow Sonde'][j] == 'Yes' or df['Mid-level Inflow Sonde'][j] == 'Yes':               \n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, s = 150, marker = mark, edgecolor = outline)\n",
    "                else:\n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, alpha = use_alpha, s = 150, marker = mark, edgecolor = outline)\n",
    "            \n",
    "    ax.grid(True)\n",
    "    ax.set_title('PBL Depth', fontsize = 33, fontweight = 'bold')\n",
    "    ax.set_xlabel('Case', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[mb]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([1010,900])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks(np.arange(1010,899,-10))\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    ax.legend(handles = legend_elements_PBL, fontsize = 18, loc = 'upper left', ncol = 1)\n",
    "\n",
    "box_plot_onlyIsoOrg_PBL('PBL Top [mb]')\n",
    "plt.subplots_adjust(wspace = 0.20)\n",
    "plt.savefig('/Users/ben/Desktop/PBL_box_scatter.png', bbox_inches = 'tight')\n",
    "plt.close()\n",
    "print ('')\n",
    "\n",
    "#for convective type comparison of All Lifecycles dropsondes\n",
    "def box_plot_onlyIsoOrg_SSwDAWN(plotting_metric_name):  #parameter will be a data column from df1\n",
    "\n",
    "    #group_fig = plt.figure(figsize=(40,21))\n",
    "    group_fig = plt.figure(figsize=(32,21))\n",
    "    ax = group_fig.add_subplot(1,2,1)\n",
    "    \n",
    "    df_use1 = df1_noTC[df1_noTC['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "    df_use = df_use1[(df_use1['Low-level Inflow Sonde'] != 'No') | (df_use1['Mid-level Inflow Sonde'] != 'No')].copy()\n",
    "    \n",
    "    #all sondes\n",
    "    df_iso_all = df_use1[df_use1['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all = df_use1[df_use1['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all = df_use1[df_use1['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso = df_use[df_use['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org = df_use[df_use['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat = df_use[df_use['Primary Convective Type'] == 'Scattered'].copy()\n",
    "    \n",
    "    #need to filter out NaN values (using dropna()), otherwise the boxplot() won't create anything\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_scat[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                 patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated', 'Organized', 'Scattered'])\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_scat[plotting_metric_name].dropna().values, df_scat_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                  patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized', 'Scattered\\n(Inflow)', 'Scattered'])\n",
    "    bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "                     df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "                     patch_artist = True, vert = True, widths = 0.65, tick_labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized'])\n",
    "        \n",
    "    print (f'Isolated {plotting_metric_name} median:', df_iso[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median:', df_org[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median:', df_scat[plotting_metric_name].median(skipna = True))\n",
    "\n",
    "    # colors = ['red', 'blue', 'black']\n",
    "    #colors = ['red', 'red', 'blue', 'blue', 'black', 'black']\n",
    "    colors = ['red', 'red', 'blue', 'blue']\n",
    "    nums = list(range(len(colors)))\n",
    "     \n",
    "    for ii, patch, color in zip(nums, bp['boxes'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            patch.set_facecolor(color)\n",
    "        else:\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(use_alpha)\n",
    "\n",
    "    #changing color and linewidth of medians\n",
    "    for ii, median in enumerate(bp['medians']):\n",
    "        if ii % 2 == 0:\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "        else:\n",
    "            #median.set(color = 'white', alpha = use_alpha, linewidth = 3)\n",
    "            median.set(color = 'white', linewidth = 3)        \n",
    "\n",
    "    #changing color and linewidth of whiskers\n",
    "    for ii, whisker in enumerate(bp['whiskers']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 whiskers, not 4\n",
    "            whisker.set(color = 'k', linewidth = 2, linestyle = \"-\")\n",
    "        else:\n",
    "            whisker.set(color = 'k', alpha = use_alpha, linewidth = 2, linestyle = \"-\")\n",
    "     \n",
    "    #changing color and linewidth of caps\n",
    "    for ii, cap in enumerate(bp['caps']):\n",
    "        if ii in [0, 1, 4, 5, 8, 9]:  #8 caps, not 4\n",
    "            cap.set(color = 'k', linewidth = 2)\n",
    "        else:\n",
    "            cap.set(color = 'k', alpha = use_alpha, linewidth = 2)\n",
    "     \n",
    "    #changing style of fliers\n",
    "    for ii, flier, color in zip(nums, bp['fliers'], colors):\n",
    "        if ii % 2 == 0:\n",
    "            flier.set(marker = 'o', color = 'k', markersize = 13, markerfacecolor = color)\n",
    "        else:\n",
    "            flier.set(marker = 'o', color = 'k', alpha = use_alpha, markersize = 13, markerfacecolor = color)\n",
    "        \n",
    "    ax.grid(True, axis = 'y')\n",
    "    ax.set_title('a) Deep Layer Shear (Dropsonde & DAWN, 0.5km - 7.6km)', fontsize = 29.5, fontweight = 'bold')\n",
    "    ax.set_xlabel('Convective Type', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[kts]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([0,50])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    ax.text(0.2405, 0.97, 'In Precip Profiles Included', horizontalalignment='center', verticalalignment='center', \n",
    "            transform=ax.transAxes, fontsize = 30, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 5})\n",
    "    \n",
    "    #plot the DAWN/dropsonde speed shear scatter plot\n",
    "    ax = group_fig.add_subplot(1,2,2)\n",
    "    \n",
    "    plotting_metric = df1[plotting_metric_name]   #both dropsonde and DAWN data\n",
    "    xlist = []\n",
    "    for i in range(len(df1)):\n",
    "        xstring = str(df1['Case'][i])\n",
    "        xlist.append(xstring)\n",
    "    \n",
    "    #custom legend\n",
    "    # legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 15, label='Isolated'),\n",
    "    #                     Line2D([], [], color='red', linewidth = 0, marker = '$P$', markersize = 15, label='Isolated (In Precip)'),\n",
    "    #                     Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 15, label='Organized'),\n",
    "    #                     Line2D([], [], color='blue', linewidth = 0, marker = '$P$', markersize = 15, label='Organized (In Precip)'),\n",
    "    #                     Line2D([], [], color='black', linewidth = 0, marker = 'o', markersize = 15, label='Scattered'),\n",
    "    #                     Line2D([], [], color='black', linewidth = 0, marker = '$P$', markersize = 15, label='Scattered (In Precip)')]\n",
    "    legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 13, label='Isolated (Clear)'),\n",
    "                       Line2D([], [], color='red', linewidth = 0, marker = '$C$', markersize = 13, label='Isolated (In Cloud)'),\n",
    "                       Line2D([], [], color='red', linewidth = 0, marker = '$P$', markersize = 13, label='Isolated (In Precip)'),\n",
    "                       Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 13, label='Organized (Clear)'),\n",
    "                       Line2D([], [], color='blue', linewidth = 0, marker = '$C$', markersize = 13, label='Organized (In Cloud)'),\n",
    "                       Line2D([], [], color='blue', linewidth = 0, marker = '$P$', markersize = 13, label='Organized (In Precip)')]\n",
    "    \n",
    "    \n",
    "    #the lighter shades account for TS Cindy organized cases and, further, TS Cindy cases that were away from the main organized convection and instead near the cyclonic center\n",
    "    for j in range(len(plotting_metric)):\n",
    "        #if (df1['Convective Lifecycle'][j] != 'Weakening') and (df1['Environment Falling In'][j] == 'Clear Near' or df1['Environment Falling In'][j] == 'In Cloud' or df1['Environment Falling In'][j] == 'In Precip'):\n",
    "        if df1['Environment Falling In'][j] == 'Clear Near' or df1['Environment Falling In'][j] == 'In Cloud' or df1['Environment Falling In'][j] == 'In Precip':\n",
    "        #if (df1['Environment Falling In'][j] == 'Clear Near' or df1['Environment Falling In'][j] == 'In Cloud' or df1['Environment Falling In'][j] == 'In Precip') and (df1['Low-level Inflow Sonde'][j] != 'No' or df1['Mid-level Inflow Sonde'][j] != 'No'):\n",
    "            if df1['Primary Convective Type'][j] == 'Isolated':\n",
    "                color = 'red'\n",
    "                if df1['Date'][j] in TC_days:\n",
    "                    continue\n",
    "            elif df1['Primary Convective Type'][j] == 'Organized':\n",
    "                color = 'blue'\n",
    "                if df1['Date'][j] in TC_days:\n",
    "                    continue\n",
    "            # elif df1['Primary Convective Type'][j] == 'Scattered':\n",
    "            #     color = 'black'\n",
    "            #     if df1['Date'][j] in TC_days:\n",
    "            #         continue \n",
    "            else:\n",
    "                continue\n",
    "                  \n",
    "            if df1['Environment Falling In'][j] == 'In Precip':\n",
    "                if df1['Partially In Precip'][j] == 'Yes':\n",
    "                    mark = '$*P$'\n",
    "                    mark = '$P$'\n",
    "                else:\n",
    "                    mark = '$P$'\n",
    "                outline = None\n",
    "            elif df1['Environment Falling In'][j] == 'In Cloud':\n",
    "                if df1['Falling Through Weak Stratiform (Onion-ish Profile (Typically On Outer Edge of Storm))'][j] == 'Yes':\n",
    "                    mark = '$*C$'\n",
    "                    mark = '$C$'\n",
    "                else:\n",
    "                    mark = '$C$'\n",
    "                outline = None\n",
    "            else:\n",
    "                mark = 'o'\n",
    "                outline = 'black'\n",
    "\n",
    "            #if the metric is not NaN (need this filter, otherwise Python raises an error when showing/saving the figure)\n",
    "            if not np.isnan(plotting_metric[j]):  #could also use pd.isna(plotting_metric[j]) \n",
    "                if df1['Low-level Inflow Sonde'][j] != 'No' or df1['Mid-level Inflow Sonde'][j] != 'No':               \n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, s = 150, marker = mark, edgecolor = outline)\n",
    "                else:\n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, alpha = use_alpha, s = 150, marker = mark, edgecolor = outline)\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.set_title('b) Deep Layer Shear (Dropsonde & DAWN, 0.5km - 7.6km)', fontsize = 29.5, fontweight = 'bold')\n",
    "    ax.set_xlabel('Case', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[kts]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([0,50])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    ax.legend(handles = legend_elements, fontsize = 22, ncol = 1, loc = 'upper left')\n",
    "\n",
    "box_plot_onlyIsoOrg_SSwDAWN('500m Bottom Cap Deep Layer Speed Shear [kts]')\n",
    "plt.subplots_adjust(wspace = 0.20)\n",
    "plt.savefig('/Users/ben/Desktop/SSwDAWN_box_scatter.png', bbox_inches = 'tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6ae72-2803-4055-ab79-81f77dd623f0",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figures 5 and 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd57b80-bc07-4280-9fcd-8777713881cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from metric_scatter_4panel_allCPEX.py\n",
    "\n",
    "# matplotlib.rcParams['axes.labelsize'] = 14\n",
    "# matplotlib.rcParams['axes.titlesize'] = 14\n",
    "# matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "# matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "# matplotlib.rcParams['legend.fontsize'] = 12\n",
    "#matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "\n",
    "#drop_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations.csv')\n",
    "drop_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "df = pd.read_csv(drop_filepath)\n",
    "\n",
    "# #change the old index (which still references non-sorted rows) to a new index (which references the sorted rows)\n",
    "# new_index = np.arange(0, len(df1), 1)\n",
    "# df_sort = df.sort_values(['Region', 'Case']).set_index(new_index)\n",
    "# #^^^orders by region first, then by case within each region group\n",
    "\n",
    "TC_days = ['20170619','20170620','20210826','20210828','20210901','20210904', '20220923']  #including TDs as TCs (from both CPEX and CPEX-AW and CPEX-CV)\n",
    "\n",
    "rh_layers = ['Deep Layer RH [%]', 'PBL RH [%]', 'Mid Layer RH [%]', 'Upper Layer RH [%]']\n",
    "speed_shear_layers = ['SHARPpy Direct Method Deep Layer Speed Shear [kts]', 'SHARPpy Direct Method PBL Speed Shear [kts]', \n",
    "                      'SHARPpy Direct Method Mid Layer Speed Shear [kts]', 'SHARPpy Direct Method Upper Layer Speed Shear [kts]']\n",
    "cape_layers = ['Deep Layer MUCAPE [J/kg]', 'Deep Layer MLCAPE [J/kg]', 'Above FZL MUCAPE [J/kg]', 'Above FZL MLCAPE [J/kg]']\n",
    "\n",
    "use_alpha = 0.25  #alpha for non-inflow sondes\n",
    "#use_alpha = 1.0\n",
    "\n",
    "#custom legend\n",
    "legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 15, label='Isolated (Clear)'),\n",
    "                    Line2D([], [], color='red', linewidth = 0, marker = '$C$', markersize = 15, label='Isolated (In Cloud)'),\n",
    "                    Line2D([], [], color='red', linewidth = 0, marker = '$P$', markersize = 15, label='Isolated (In Precip)'),\n",
    "                    Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 15, label='Organized (Clear)'),\n",
    "                    Line2D([], [], color='blue', linewidth = 0, marker = '$C$', markersize = 15, label='Organized (In Cloud)'),\n",
    "                    Line2D([], [], color='blue', linewidth = 0, marker = '$P$', markersize = 15, label='Organized (In Precip)')]\n",
    "\n",
    "# legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 15, label='Isolated (Clear)'),\n",
    "#                     Line2D([], [], color='red', linewidth = 0, marker = '$C$', markersize = 15, label='Isolated (In Cloud)'),\n",
    "#                     Line2D([], [], color='red', linewidth = 0, marker = '$P$', markersize = 15, label='Isolated (In Precip)'),\n",
    "#                     Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 15, label='Organized (Clear)'),\n",
    "#                     Line2D([], [], color='blue', linewidth = 0, marker = '$C$', markersize = 15, label='Organized (In Cloud)'),\n",
    "#                     Line2D([], [], color='blue', linewidth = 0, marker = '$P$', markersize = 15, label='Organized (In Precip)'),\n",
    "#                     Line2D([], [], color='black', linewidth = 0, marker = 'o', markersize = 15, label='Scattered (Clear)'),\n",
    "#                     Line2D([], [], color='black', linewidth = 0, marker = '$C$', markersize = 15, label='Scattered (In Cloud)'),\n",
    "#                     Line2D([], [], color='black', linewidth = 0, marker = '$P$', markersize = 15, label='Scattered (In Precip)')]\n",
    "\n",
    "# legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 15, label='Isolated'),\n",
    "#                    Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 15, label='Organized'),\n",
    "#                    Line2D([], [], color='black', linewidth = 0, marker = 'o', markersize = 15, label='Scattered')]\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "#IMPORTANT:  HOW TO FIX 'KeyError: 0' error:  NEED .iloc[j] AND NOT JUST [j] BECAUSE [j] refers to the index, \n",
    "#not the integer position (IP), and groupby function only adjusts IP when making groups and not index!\n",
    " \n",
    "#for convective type comparison of All Lifecycles dropsondes\n",
    "def make_plot_onlyIsoOrg_RH(plotting_metric_name, ax):  #parameter will be a data column from df\n",
    "    \n",
    "    plotting_metric = df[plotting_metric_name]\n",
    "    xlist = []\n",
    "    for i in range(len(df)):\n",
    "        xstring = str(df['Case'][i])\n",
    "        xlist.append(xstring)\n",
    "    \n",
    "    for j in range(len(plotting_metric)):\n",
    "        #if (df['Convective Lifecycle'][j] != 'Weakening') and (df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip'):\n",
    "        if df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip':\n",
    "        #if (df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip') and (df['Low-level Inflow Sonde'][j] == 'Yes' or df['Mid-level Inflow Sonde'][j] == 'Yes'):\n",
    "            if df['Primary Convective Type'][j] == 'Isolated':\n",
    "                color = 'red'\n",
    "                if str(df['Date'][j]) in TC_days:\n",
    "                    continue\n",
    "            elif df['Primary Convective Type'][j] == 'Organized':\n",
    "                color = 'blue'\n",
    "                if str(df['Date'][j]) in TC_days:\n",
    "                    continue\n",
    "            # elif df['Primary Convective Type'][j] == 'Scattered':\n",
    "            #     color = 'black'\n",
    "            #     if str(df['Date'][j]) in TC_days:\n",
    "            #         continue                \n",
    "            else:\n",
    "                continue                \n",
    "            \n",
    "            if df['Environment Falling In'][j] == 'In Precip':\n",
    "                if df['Partially In Precip'][j] == 'Yes':\n",
    "                    mark = '$*P$'\n",
    "                    mark = '$P$'\n",
    "                else:\n",
    "                    mark = '$P$'\n",
    "                outline = None\n",
    "            elif df['Environment Falling In'][j] == 'In Cloud':\n",
    "                if df['Falling Through Weak Stratiform (Onion-ish Profile (Typically On Outer Edge of Storm))'][j] == 'Yes':\n",
    "                    mark = '$*C$'\n",
    "                    mark = '$C$'\n",
    "                else:\n",
    "                    mark = '$C$'\n",
    "                outline = None\n",
    "            else:\n",
    "                mark = 'o'\n",
    "                outline = 'black'\n",
    "            \n",
    "            #if the metric is not NaN (need this filter, otherwise Python raises an error when showing/saving the figure)\n",
    "            if not np.isnan(plotting_metric[j]):  #could also use pd.isna(plotting_metric[j])\n",
    "                if df['Low-level Inflow Sonde'][j] == 'Yes' or df['Mid-level Inflow Sonde'][j] == 'Yes':\n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, s = 150, marker = mark, edgecolor = outline)\n",
    "                else:\n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, alpha = use_alpha, s = 150, marker = mark, edgecolor = outline)\n",
    "            else:   #use this if you want each case to show up in the plot, regardless if it has data or not\n",
    "                ax.scatter(xlist[j], 80, c = color, alpha = 0.0, s = 150)\n",
    "            \n",
    "    ax.grid(True)\n",
    "    ax.set_title(plotting_metric_name[:-4], fontsize = 33, fontweight = 'bold')\n",
    "    ax.set_xlabel('Case', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[%]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([15,100])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    " \n",
    "#group_fig = plt.figure(figsize=(15,24))   #CPEX(-AW) with isolated and organized cases\n",
    "group_fig = plt.figure(figsize=(32,24))   #CPEX-CV with isolated, organized, (and scattered if you want) cases\n",
    "\n",
    "for i, layer in enumerate(rh_layers):\n",
    "    ax = group_fig.add_subplot(2,2,i+1)\n",
    "    make_plot_onlyIsoOrg_RH(layer, ax)\n",
    "\n",
    "#group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 28, title = '$\\\\bf{Convective Type}$', title_fontsize = 30, ncol = 3)\n",
    "group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 28, ncol = 2)\n",
    "plt.subplots_adjust(hspace = 0.6, wspace = 0.22)\n",
    "plt.savefig('/Users/ben/Desktop/RH_4panel.png', bbox_inches = 'tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "def make_plot_onlyIsoOrg_SS(plotting_metric_name, ax):  #parameter will be a data column from df\n",
    "    \n",
    "    plotting_metric = df[plotting_metric_name]\n",
    "    xlist = []\n",
    "    for i in range(len(df)):\n",
    "        xstring = str(df['Case'][i])\n",
    "        xlist.append(xstring)\n",
    "    \n",
    "    for j in range(len(plotting_metric)):\n",
    "        #if (df['Convective Lifecycle'][j] != 'Weakening') and (df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip'):\n",
    "        if df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip':\n",
    "        #if (df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip') and (df['Low-level Inflow Sonde'][j] == 'Yes' or df['Mid-level Inflow Sonde'][j] == 'Yes'):\n",
    "            if df['Primary Convective Type'][j] == 'Isolated':\n",
    "                color = 'red'\n",
    "                if str(df['Date'][j]) in TC_days:\n",
    "                    continue\n",
    "            elif df['Primary Convective Type'][j] == 'Organized':\n",
    "                color = 'blue'\n",
    "                if str(df['Date'][j]) in TC_days:\n",
    "                    continue\n",
    "            # elif df['Primary Convective Type'][j] == 'Scattered':\n",
    "            #     color = 'black'\n",
    "            #     if str(df['Date'][j]) in TC_days:\n",
    "            #         continue                \n",
    "            else:\n",
    "                continue              \n",
    "                  \n",
    "            if df['Environment Falling In'][j] == 'In Precip':\n",
    "                if df['Partially In Precip'][j] == 'Yes':\n",
    "                    mark = '$*P$'\n",
    "                    mark = '$P$'\n",
    "                else:\n",
    "                    mark = '$P$'\n",
    "                outline = None\n",
    "            elif df['Environment Falling In'][j] == 'In Cloud':\n",
    "                if df['Falling Through Weak Stratiform (Onion-ish Profile (Typically On Outer Edge of Storm))'][j] == 'Yes':\n",
    "                    mark = '$*C$'\n",
    "                    mark = '$C$'\n",
    "                else:\n",
    "                    mark = '$C$'\n",
    "                outline = None\n",
    "            else:\n",
    "                mark = 'o'\n",
    "                outline = 'black'\n",
    "        \n",
    "            #if the metric is not NaN (need this filter, otherwise Python raises an error when showing/saving the figure)\n",
    "            if not np.isnan(plotting_metric[j]):  #could also use pd.isna(plotting_metric[j])\n",
    "                if df['Low-level Inflow Sonde'][j] == 'Yes' or df['Mid-level Inflow Sonde'][j] == 'Yes':\n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, s = 150, marker = mark, edgecolor = outline)\n",
    "                else:\n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, alpha = use_alpha, s = 150, marker = mark, edgecolor = outline)\n",
    "            else:   #use this if you want each case to show up in the plot, regardless if it has data or not\n",
    "                ax.scatter(xlist[j], 10, c = color, alpha = 0.0, s = 150)\n",
    "                \n",
    "    ax.grid(True)\n",
    "    ax.set_title(plotting_metric_name[22:-18] + ' Shear', fontsize = 33, fontweight = 'bold')\n",
    "    ax.set_xlabel('Case', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[kts]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([0,50])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    " \n",
    "#group_fig = plt.figure(figsize=(15,24))   #CPEX(-AW) with isolated and organized cases\n",
    "group_fig = plt.figure(figsize=(32,24))   #CPEX-CV with isolated, organized, (and scattered if you want) cases\n",
    "\n",
    "for i, layer in enumerate(speed_shear_layers):\n",
    "    ax = group_fig.add_subplot(2,2,i+1)\n",
    "    make_plot_onlyIsoOrg_SS(layer, ax)\n",
    "\n",
    "#group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 28, title = '$\\\\bf{Convective Type}$', title_fontsize = 30, ncol = 3)\n",
    "group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 28, ncol = 2)\n",
    "plt.subplots_adjust(hspace = 0.6, wspace = 0.22)\n",
    "plt.savefig('/Users/ben/Desktop/SS_4panel.png', bbox_inches = 'tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "def make_plot_onlyIsoOrg_CAPE(plotting_metric_name, ax):  #parameter will be a data column from df\n",
    "    \n",
    "    plotting_metric = df[plotting_metric_name]\n",
    "    xlist = []\n",
    "    for i in range(len(df)):\n",
    "        xstring = str(df['Case'][i])\n",
    "        xlist.append(xstring)\n",
    "    \n",
    "    for j in range(len(plotting_metric)):\n",
    "        #if (df['Convective Lifecycle'][j] != 'Weakening') and (df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip'):\n",
    "        if df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip':\n",
    "        #if (df['Environment Falling In'][j] == 'Clear Near' or df['Environment Falling In'][j] == 'In Cloud' or df['Environment Falling In'][j] == 'In Precip') and (df['Low-level Inflow Sonde'][j] == 'Yes' or df['Mid-level Inflow Sonde'][j] == 'Yes'):            \n",
    "            if df['Primary Convective Type'][j] == 'Isolated':\n",
    "                color = 'red'\n",
    "                if str(df['Date'][j]) in TC_days:\n",
    "                    continue\n",
    "            elif df['Primary Convective Type'][j] == 'Organized':\n",
    "                color = 'blue'\n",
    "                if str(df['Date'][j]) in TC_days:\n",
    "                    continue\n",
    "            # elif df['Primary Convective Type'][j] == 'Scattered':\n",
    "            #     color = 'black'\n",
    "            #     if str(df['Date'][j]) in TC_days:\n",
    "            #         continue                \n",
    "            else:\n",
    "                continue\n",
    "                  \n",
    "            if df['Environment Falling In'][j] == 'In Precip':\n",
    "                if df['Partially In Precip'][j] == 'Yes':\n",
    "                    mark = '$*P$'\n",
    "                    mark = '$P$'\n",
    "                else:\n",
    "                    mark = '$P$'\n",
    "                outline = None\n",
    "            elif df['Environment Falling In'][j] == 'In Cloud':\n",
    "                if df['Falling Through Weak Stratiform (Onion-ish Profile (Typically On Outer Edge of Storm))'][j] == 'Yes':\n",
    "                    mark = '$*C$'\n",
    "                    mark = '$C$'\n",
    "                else:\n",
    "                    mark = '$C$'\n",
    "                outline = None\n",
    "            else:\n",
    "                mark = 'o'\n",
    "                outline = 'black'\n",
    "\n",
    "            #if the metric is not NaN (need this filter, otherwise Python raises an error when showing/saving the figure)\n",
    "            if not np.isnan(plotting_metric[j]):  #could also use pd.isna(plotting_metric[j])\n",
    "                if df['Low-level Inflow Sonde'][j] == 'Yes' or df['Mid-level Inflow Sonde'][j] == 'Yes':\n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, s = 150, marker = mark, edgecolor = outline)\n",
    "                else:\n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, alpha = use_alpha, s = 150, marker = mark, edgecolor = outline)\n",
    "            else:   #use this if you want each case to show up in the plot, regardless if it has data or not\n",
    "                ax.scatter(xlist[j], 200, c = color, alpha = 0.0, s = 150)\n",
    "                \n",
    "    ax.grid(True)\n",
    "    if 'Above FZL' in plotting_metric_name:\n",
    "        ax.set_title('Upper Layer' + plotting_metric_name[9:-7], fontsize = 33, fontweight = 'bold')\n",
    "    else:\n",
    "        ax.set_title(plotting_metric_name[:-7], fontsize = 33, fontweight = 'bold')\n",
    "    ax.set_xlabel('Case', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[J kg$^{-1}$]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    \n",
    "    # #make the deep layer plot axes range the same and the above FZL plot axes the same for easier comparison\n",
    "    # if plotting_metric_name[:4] == 'Deep':\n",
    "    #     ax.set_ylim([-30,1450])\n",
    "    # else:\n",
    "    #     ax.set_ylim([-30,1000])\n",
    "\n",
    " \n",
    "#group_fig = plt.figure(figsize=(15,24))   #CPEX(-AW) with isolated and organized cases\n",
    "group_fig = plt.figure(figsize=(32,24))   #CPEX-CV with isolated, organized, (and scattered if you want) cases\n",
    "\n",
    "for i, layer in enumerate(cape_layers):\n",
    "    ax = group_fig.add_subplot(2,2,i+1)\n",
    "    make_plot_onlyIsoOrg_CAPE(layer, ax)\n",
    "\n",
    "#group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 28, title = '$\\\\bf{Convective Type}$', title_fontsize = 30, ncol = 3)\n",
    "group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 28, ncol = 2)\n",
    "plt.subplots_adjust(hspace=0.6, wspace = 0.22)\n",
    "plt.savefig('/Users/ben/Desktop/CAPE_4panel.png', bbox_inches = 'tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45908b-9837-4400-a6b1-f6e34c604c78",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figure 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1330f4-06fc-457f-9465-18fdf5166bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CFADs (with and without convective-stratiform partitioning)\n",
    "#adapted from CFAD_AGUpapers_only_pseudonadir_ray_and_ECCO.py\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "# matplotlib.rcParams['axes.labelsize'] = 14\n",
    "# matplotlib.rcParams['axes.titlesize'] = 14\n",
    "# matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "# matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['legend.fontsize'] = 24\n",
    "#matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "\n",
    "#create the dictionary of cases for which you want to plot a total CFAD and create the height/reflectivity bin edges\n",
    "\n",
    "#CPEX-CV cases (Cases 1-7 are Isolated; Cases 8-22 are Organized (ignore Case 19 (TC)); Cases 23-24 are Scattered)\n",
    "case1_dict = {1: ['20220909','161000','171000']}\n",
    "case2_dict = {2: ['20220909','173500','191000']}\n",
    "case3_dict = {3: ['20220910','190500','194200']}\n",
    "case4_dict = {4: ['20220910', '202500', '204500']}\n",
    "case5_dict = {5: ['20220916', '155500', '163500' ]}\n",
    "case6_dict = {6: ['20220920', '071000', '073500']}\n",
    "case7_dict = {7: ['20220920', '083000', '090000']}\n",
    "case8_dict = {8: ['20220906', '110000', '120000']}\n",
    "case9_dict = {'9a': ['20220906', '133000', '143000'], '9b': ['20220906', '153000', '161000']}\n",
    "case10_dict = {10: ['20220906', '161000', '180000']}\n",
    "case11_dict = {11: ['20220907', '130000', '134500']}\n",
    "case12_dict = {'12a': ['20220907', '134500', '151300'], '12b': ['20220907', '161800', '174500']}\n",
    "case13_dict = {13: ['20220907', '151300', '161800']}\n",
    "case14_dict = {'14a': ['20220914', '101000', '115500'], '14b': ['20220914', '134000', '142800']}\n",
    "case15_dict = {'15a': ['20220914','115500','131000'], '15b': ['20220914','142800','164500']}\n",
    "case16_dict = {16: ['20220916','143000','154000']}\n",
    "case17_dict = {17: ['20220916','164000','183500']}\n",
    "case18_dict = {'18a': ['20220922', '054000', '061500'], '18b': ['20220922', '063500', '073600'], '18c': ['20220922', '080000', '083000']}\n",
    "case19_dict = {19: ['20220923', '092000', '143000']}\n",
    "case20_dict = {20: ['20220926', '072000', '111500']}\n",
    "case21_dict = {21: ['20220929', '103500', '134500']}\n",
    "case22_dict = {22: ['20220930', '134800', '143000']}\n",
    "case23_dict = {23: ['20220910', '153500', '183000']}\n",
    "case24_dict = {'24a': ['20220930', '092000', '102000'], '24b': ['20220930', '111000', '123000'], '24c': ['20220930', '131800', '134000']}\n",
    "\n",
    "# #CPEX(-AW) cases\n",
    "# case1_dict = {1: ['20170610','194655','221900']}\n",
    "# case2_dict = {2: ['20170624','180000','194800']}\n",
    "# case3_dict = {3: ['20170624','201200','220000']}\n",
    "# case4_dict = {4: ['20170615', '184840', '205000']}\n",
    "# case5_dict = {5: ['20170616', '182451', '220600' ]}\n",
    "# case6_dict = {6: ['20170601', '175849', '220700']}\n",
    "# case7_dict = {7: ['20170606', '185211', '215000']}\n",
    "# case8_dict = {8: ['20170617', '184650', '220000']}\n",
    "# case9_dict = {9: ['20170619', '173229', '223000']}\n",
    "# case10_dict = {10: ['20170620', '171600', '220600']}\n",
    "# case11_dict = {11: ['20170602', '174415', '221221']}\n",
    "# case12_dict1 = {12: ['20170611', '170000', '174500']}\n",
    "# case12_dict2 = {12: ['20170611', '210000', '214500']}\n",
    "# case13_dict = {13: ['20170611', '180100', '203400']}\n",
    "# case14_dict = {14: ['20210821', '221800', '234145']}\n",
    "# case16_dict = {16: ['20210824', '181545', '195745']}\n",
    "\n",
    "height_edges = np.arange(1500, 8001, 500)\n",
    "dbz_edges = np.arange(-20, 70.1, 5)\n",
    "vel_edges = np.arange(-25, 25.1, 2)\n",
    "# dbz_edges = np.arange(-20, 60.1, 5)\n",
    "# vel_edges = np.arange(-13, 15.1, 2)\n",
    "\n",
    "def plot_CFAD(cfad_array, contours, colorbar_label, save_label, var_label, var_centers_meshgrid, height_centers_meshgrid, var_edges, case_dict, median_height_profile_above1500m, medianDBZ_profile_above1500m, Q1DBZ_profile_above1500m, Q3DBZ_profile_above1500m, dbz_plot = True):\n",
    "    \n",
    "    \"\"\"Plot a contourf CFAD given a CFAD 2-D array, contour levels, plot/image labels, and variable/height meshgrids\"\"\"\n",
    "    \n",
    "    #plot the CFAD\n",
    "    fig, ax = plt.subplots(1,1, figsize=(21,21))\n",
    "    cmap = mplc.ListedColormap(['#ffffff', '#d8fcfa', '#bef5f6', '#aaedf1', '#98e4ec', '#89dae7', '#7cd0e2', \n",
    "                                '#70c6dd', '#65bcd9', '#5cb2d4', '#53a8cf', '#4b9dca', '#4393c5', '#3c89c0', \n",
    "                                '#357ebb', '#2f74b6', '#286ab1', '#2160ac', '#1956a7', '#0f4ca2', '#00429d'])\n",
    "    \n",
    "    #the cmap below omits the white fill at the beginnning\n",
    "    # cmap = mplc.ListedColormap(['#d8fcfa', '#bef5f6', '#aaedf1', '#98e4ec', '#89dae7', '#7cd0e2', \n",
    "    #                             '#70c6dd', '#65bcd9', '#5cb2d4', '#53a8cf', '#4b9dca', '#4393c5', '#3c89c0', \n",
    "    #                             '#357ebb', '#2f74b6', '#286ab1', '#2160ac', '#1956a7', '#0f4ca2', '#00429d'])\n",
    "    \n",
    "    #cs = ax.pcolormesh(dbz_meshgrid, height_meshgrid, cfad_array, cmap = cmap)\n",
    "    #cs = ax.contour(dbz_centers_meshgrid, height_centers_meshgrid, cfad_array, levels = contours, cmap = cmap, linewidths = 1)\n",
    "    cs = ax.contourf(var_centers_meshgrid, height_centers_meshgrid, cfad_array, levels = contours, cmap = cmap)\n",
    "    if dbz_plot:\n",
    "        ax.plot(medianDBZ_profile_above1500m, median_height_profile_above1500m, color = 'k', linestyle = '-', linewidth = 5, label = 'Case ' + ','.join(map(str, list(case_dict.keys()))) + ' Median Profile') \n",
    "        ax.plot(Q1DBZ_profile_above1500m, median_height_profile_above1500m, color = 'k', linestyle = '--', linewidth = 5, label = 'Case ' + ','.join(map(str, list(case_dict.keys()))) + ' Q1 and Q3 Profiles')\n",
    "        ax.plot(Q3DBZ_profile_above1500m, median_height_profile_above1500m, color = 'k', linestyle = '--', linewidth = 5)\n",
    "        ax.legend(loc = 'upper right')\n",
    "    ax.set_ylabel('Altitude [m]', fontsize=30, fontweight = 'bold')\n",
    "    ax.set_xlabel(var_label, fontsize=30, fontweight = 'bold')\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    #ax.set_title('Total CFAD for Case ' + ','.join(map(str, list(case_dict.keys()))), fontsize=35, fontweight = 'bold')\n",
    "    ax.set_title('Case ' + ','.join(map(str, list(case_dict.keys()))) + ' Normalized CFAD', fontsize=35, fontweight = 'bold')\n",
    "    ax.set_ylim([height_edges[0] + 250, height_edges[-1] - 250])\n",
    "    #ax.set_xlim([var_edges[0],var_edges[-1]])\n",
    "    ax.set_xlim([5,55])  #for Ku-band\n",
    "    \n",
    "    #set the colorbar axis\n",
    "    cax = fig.add_axes([ax.get_position().x0, ax.get_position().y0 - 0.08,\n",
    "                       ax.get_position().x1-ax.get_position().x0, 0.02])    #Left, bottom, width, height (all [0,1])\n",
    "\n",
    "    #create the colorbar\n",
    "    cbar = plt.colorbar(cs, cax = cax, orientation = 'horizontal')\n",
    "    cbar.ax.tick_params(length = 10, width = 3, labelsize = 23)\n",
    "    cbar.set_label(label = colorbar_label, fontsize = 30, fontweight = 'bold')\n",
    "    \n",
    "    #save the figure\n",
    "    plt.savefig(''.join(['/Users/ben/Desktop/', save_label]), bbox_inches = 'tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def CFAD(case_dict, height_bin_edges, dbz_bin_edges, vel_bin_edges, normalize = True):\n",
    "    \n",
    "    \"\"\"\"Calculate and plot Ku-band and Doppler Velocity CFAD 2-D arrays for \n",
    "        the given cases and their respective time ranges\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    case_dict:  dictionary of cases for which to calculate the total CFAD; \n",
    "                keys should be case numbers;\n",
    "                values should be 3-element lists of case date and start/end times, \n",
    "                with date strings formatted as YYYYMMDD and time strings formatted as HHMMSS\n",
    "    \n",
    "    height_bin_edges:  a 1-D array of height [m] bin edges used to create the 2-D CFAD array/histogram    \n",
    "\n",
    "    dbz_bin_edges:  a 1-D array of reflectivity [dBZ] bin edges used to create the 2-D CFAD array/histogram \n",
    "\n",
    "    vel_bin_edges:  a 1-D array of Doppler velocity [m/s] bin edges used to create the 2-D CFAD array/histogram       \n",
    "                        \n",
    "    normalized:  True/False; determines whether to normalize the CFAD array by maximum bin at any level \n",
    "                 (method from Zagrodnik et al., 2019) and create an additional, normalized CFAD plot\n",
    "                        \n",
    "    return:  2-D (normalized) CFAD arrays and plots \"\"\"\n",
    "\n",
    "    assert type(case_dict) == dict, \"case_dict must be a dictionary\"\n",
    "    \n",
    "    total_apr_profiles = 0 \n",
    "    apr_profile_roll10 = 0\n",
    "    new_ku_array = True\n",
    "    new_vel_array = True\n",
    "    \n",
    "    for key in case_dict:\n",
    "        print ('Processing Case {}...'.format(key))\n",
    "        \n",
    "        first_good_Ku_file_index = 0    #used to determine first usable Ku-band file for CPEX-CV cases below; without this, an error is brought up due to the first APR file in apr_files_use only being W-band data (i.e., _Wn.nc)\n",
    "        \n",
    "        #grab the case's date and start/end times from the dictionary\n",
    "        assert type(case_dict[key]) == list, \"A key's values must be a list of date, start time, and end time\" \n",
    "        desired_date = case_dict[key][0]\n",
    "        start_time = case_dict[key][1]\n",
    "        end_time = case_dict[key][2]\n",
    "        assert start_time < end_time, \"Start time must precede end time in a key's list\"\n",
    "        \n",
    "        #grab the ECCO-V convective-stratiform classification file for the given date\n",
    "        use_file_list = []\n",
    "        for file in os.listdir(os.path.join(os.getcwd(), 'ECCO-V_output', 'ECCO-V_classification_1D')):\n",
    "            if desired_date in file:\n",
    "                use_file_list.append(file)\n",
    "        assert len(use_file_list) == 1, \"Found either 0 or multiple ECCO-V files for the given date\"\n",
    "        use_file = use_file_list[0]\n",
    "        \n",
    "        ecco_filepath = os.path.join(os.getcwd(), 'ECCO-V_output', 'ECCO-V_classification_1D', use_file)\n",
    "        ecco_df = pd.read_csv(ecco_filepath, sep = ',', dtype = str)\n",
    "        ecco_df['Full_Datetime'] = pd.to_datetime(ecco_df['Year'].str.zfill(4) + ecco_df['Month'].str.zfill(2) + ecco_df['Day'].str.zfill(2) + ecco_df['Hour'].str.zfill(2) + ecco_df['Minute'].str.zfill(2) + ecco_df['Second'].astype(float).round().astype(int).astype(str), format = '%Y%m%d%H%M%S')\n",
    "        \n",
    "        #find the APR files of interest (for the desired date and time ranges)\n",
    "        apr_folder = os.path.join(desired_date, 'APR_files')\n",
    "        apr_file_list = sorted(os.listdir(apr_folder))\n",
    "        \n",
    "        #angles for each of the 24 rays in a given scan (used for ray adjustment; index order goes from left to right in the scan when looking ahead in the direction that the aircraft is headed)\n",
    "        if desired_date[:4] == '2017':\n",
    "            ray_angles = np.linspace(-25,25,24)[:-1]  #in degrees; omits 24th ray, which doesn't have data for Ku/Ka bands\n",
    "        \n",
    "            #find the list of APR files that have data for the desired time range\n",
    "            apr_files_use = []\n",
    "            first_file = 'blank'\n",
    "            for file in apr_file_list:        #sorted() makes sure the code goes through the files in alphabetical order\n",
    "                if file[0:3] == '.DS':         #delete possible .DS_Store files\n",
    "                    os.remove(os.path.join(apr_folder, file))\n",
    "                elif file[22:28] <= start_time:\n",
    "                    first_file = file     #first_file will always be the file immediately before (or equal to) range_start\n",
    "                elif file[22:28] >= end_time:\n",
    "                    continue\n",
    "                else:\n",
    "                    if (first_file not in apr_files_use) and (first_file != 'blank'):\n",
    "                        apr_files_use.append(first_file)\n",
    "                    apr_files_use.append(file)\n",
    "                    \n",
    "            if apr_files_use == []:             #accounts for if start_time is greater than all of the file times, but still within the last file's time range; also accounts for start/end times equaling the times of adjacent files\n",
    "                apr_files_use.append(first_file)\n",
    "                \n",
    "        elif desired_date[:4] == '2021':\n",
    "            ray_angles = np.linspace(-25,25,25)\n",
    "            \n",
    "            #create a list of all the given day's desired range's APR files:    #for APR_plots.py\n",
    "            start_time1 = datetime.strptime(desired_date + start_time, '%Y%m%d%H%M%S')\n",
    "            end_time1 = datetime.strptime(desired_date + end_time, '%Y%m%d%H%M%S')\n",
    "            \n",
    "            for x in os.listdir(apr_folder):\n",
    "                if x[0:3] == '.DS':         #delete hidden .DS_Store files if they come up (will show up if you delete a file)\n",
    "                    os.remove(os.path.join(apr_folder, x))\n",
    "            \n",
    "            #find the starting APR file in apr_folder\n",
    "            first_file_index = None       \n",
    "            for i, x in enumerate(apr_file_list):\n",
    "                file_start_time = datetime.strptime(x[13:21] + x[22:28], '%Y%m%d%H%M%S')\n",
    "                file_end_time = datetime.strptime(x[29:37] + x[38:44], '%Y%m%d%H%M%S')\n",
    "            \n",
    "                if start_time1 <= file_start_time:  #if start_time1 is before the APR file start time and not within any previous APR file's time ranges\n",
    "                    first_file_index = i\n",
    "                    break\n",
    "                elif (start_time1 >= file_start_time) and (start_time1 < file_end_time):\n",
    "                    first_file_index = i\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            if first_file_index == None:\n",
    "                sys.exit('Requested start_time is beyond all available APR files')\n",
    "                \n",
    "            #find the ending APR file in apr_folder\n",
    "            last_file_index = None       \n",
    "            for i, x in enumerate(apr_file_list):  \n",
    "                file_start_time = datetime.strptime(x[13:21] + x[22:28], '%Y%m%d%H%M%S')\n",
    "                file_end_time = datetime.strptime(x[29:37] + x[38:44], '%Y%m%d%H%M%S')\n",
    "            \n",
    "                if end_time1 <= file_start_time:  #if end_time1 is before the APR file start time and not within any previous APR file's time ranges\n",
    "                    last_file_index = i - 1\n",
    "                    break\n",
    "                elif (end_time1 > file_start_time) and (end_time1 <= file_end_time):\n",
    "                    last_file_index = i\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            if last_file_index == None:  #the end_time is after all available APR files\n",
    "                last_file_index = len(apr_file_list) - 1  #the last available APR file's index\n",
    "            if last_file_index == -1:\n",
    "                sys.exit('Requested end_time is before all available APR files')\n",
    "            \n",
    "            apr_files_use = apr_file_list[first_file_index:last_file_index + 1]\n",
    "        \n",
    "        elif desired_date[:4] == '2022':\n",
    "            ray_angles = np.linspace(-25,25,25)\n",
    "            \n",
    "            #create a list of all the given day's desired range's APR files:    #for APR_plots.py\n",
    "            start_time1 = datetime.strptime(desired_date + start_time, '%Y%m%d%H%M%S')\n",
    "            end_time1 = datetime.strptime(desired_date + end_time, '%Y%m%d%H%M%S')\n",
    "            \n",
    "            #create a list of all the given day's desired range's APR files:\n",
    "            for x in os.listdir(apr_folder):\n",
    "                if x[0:3] == '.DS':         #delete hidden .DS_Store files if they come up (will show up if you delete a file)\n",
    "                    os.remove(os.path.join(apr_folder, x))\n",
    "            \n",
    "            #find the starting APR file in apr_folder\n",
    "            first_file_index = None \n",
    "            \n",
    "            #sorted() makes sure the code goes through the files in alphabetical (chronological) order\n",
    "            #GOING THROUGH THE FILES IN CHRONOLOGICAL ORDER IS ESSENTIAL FOR THIS CELL TO WORK PROPERLY!!\n",
    "            for i, x in enumerate(apr_file_list):  \n",
    "                file_start_time = datetime.strptime(x[29:37] + x[38:44], '%Y%m%d%H%M%S')\n",
    "                file_end_time = datetime.strptime(x[46:54] + x[55:61], '%Y%m%d%H%M%S')\n",
    "            \n",
    "                if start_time1 <= file_start_time:  #if start_time is before the APR file start time and not within any previous APR file's time ranges\n",
    "                    first_file_index = i\n",
    "                    break\n",
    "                elif (start_time1 >= file_start_time) and (start_time1 < file_end_time):\n",
    "                    first_file_index = i\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            if first_file_index == None:\n",
    "                sys.exit('Requested start_time is beyond all available APR files')\n",
    "                \n",
    "            #find the ending APR file in apr_folder\n",
    "            last_file_index = None    \n",
    "            \n",
    "            #sorted() makes sure the code goes through the files in alphabetical (chronological) order\n",
    "            #GOING THROUGH THE FILES IN CHRONOLOGICAL ORDER IS ESSENTIAL FOR THIS CELL TO WORK PROPERLY!!\n",
    "            for i, x in enumerate(apr_file_list):  \n",
    "                file_start_time = datetime.strptime(x[29:37] + x[38:44], '%Y%m%d%H%M%S')\n",
    "                file_end_time = datetime.strptime(x[46:54] + x[55:61], '%Y%m%d%H%M%S')\n",
    "            \n",
    "                if end_time1 <= file_start_time:  #if end_time is before the APR file start time and not within any previous APR file's time ranges\n",
    "                    last_file_index = i - 1\n",
    "                    break\n",
    "                elif (end_time1 > file_start_time) and (end_time1 <= file_end_time):\n",
    "                    last_file_index = i\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            if last_file_index == None:  #the end_time is after all available APR file ranges\n",
    "                last_file_index = len(apr_file_list) - 1  #the last available APR file's index\n",
    "            if last_file_index == -1:\n",
    "                sys.exit('Requested end_time is before all available APR files')\n",
    "            \n",
    "            apr_files_use = apr_file_list[first_file_index:last_file_index + 1]            \n",
    "        \n",
    "        else:\n",
    "            sys.exit('Not a CPEX or CPEX-AW or CPEX-CV case')\n",
    "    \n",
    "        #print (apr_files_use)  #sanity check to make sure you grabbed the right files\n",
    "\n",
    "        for apr_filepath in apr_files_use:\n",
    "            \n",
    "            if desired_date[:4] == '2017' or desired_date[:4] == '2021':    #CPEX(-AW)\n",
    "            \n",
    "                #Low resolution ('lores') radar variables in APR hdf files\n",
    "                ku_band = 'zhh14' #Ku-band reflectivity\n",
    "                vel = 'vel14c' #mean Doppler Velocity dealiased and from Ku&Ka band\n",
    "            \n",
    "                apr_file = h5py.File(os.path.join(apr_folder, apr_filepath), 'r')\n",
    "                \n",
    "                if ('lores' in apr_file.keys()) and (ku_band in apr_file['lores'].keys()):\n",
    "                    \n",
    "                    try:   #some CPEX-AW APR files have corrupted Ku-band data; if so, skip the Ku-band for \n",
    "                           #that file (corrupted: \"OSError: Can't read data (inflate() failed)\")\n",
    "                        ku_data = apr_file['lores'][ku_band][:]\n",
    "                    except:\n",
    "                        apr_file.close()\n",
    "                        continue  #both Ku-band and velocity CFADs rely on Ku-band data availability\n",
    "    \n",
    "                    try:   #some CPEX-AW APR files have corrupted velocity data; if so, skip the Ku-band for \n",
    "                           #that file (corrupted: \"OSError: Can't read data (inflate() failed)\")\n",
    "                        vel_data = apr_file['lores'][vel][:]\n",
    "                        vel_good = True\n",
    "                    except:\n",
    "                        vel_good = False                    \n",
    "                \n",
    "                    #grab the radar variables of interest\n",
    "                    time = apr_file['lores']['scantime'][:]\n",
    "                    alt3d = apr_file['lores']['alt3D'][:]\n",
    "                    roll = apr_file['lores']['roll'][:]\n",
    "                    \n",
    "                    if apr_filepath == apr_files_use[0] or apr_filepath == apr_files_use[-1]:\n",
    "                    #the complete APR file time range may not need to be used, so need to locate the closest time (and corresponding index) to the desired start/end time\n",
    "                        \n",
    "                        #Convert APR times to datetimes\n",
    "                        time_dates = np.empty(time.shape[1], dtype=object)\n",
    "                        for j in np.arange(0, time.shape[1]):\n",
    "                            #tmp = datetime(time[12,j])\n",
    "                            tmp = datetime.utcfromtimestamp(time[12,j])  #12 could be any ray, as it is the ray number and each ray of a given scan has the same time\n",
    "                            time_dates[j] = tmp\n",
    "                        \n",
    "                        unique_apr_times = time_dates  #all the times in the given APR file\n",
    "                        \n",
    "                        if len(apr_files_use) == 1:  #i.e. apr_filepath == apr_files_use[0] and apr_filepath == apr_files_use[-1]\n",
    "                        \n",
    "                            #find the closest time (and its corresponding index) to the desired start_time\n",
    "                            desired_start_time = datetime.strptime(desired_date + start_time, '%Y%m%d%H%M%S')\n",
    "                            start_time_idx = np.argmin(abs(unique_apr_times - desired_start_time))\n",
    "                        \n",
    "                            #find the closest time (and its corresponding index) to the desired end_time\n",
    "                            desired_end_time = datetime.strptime(desired_date + end_time, '%Y%m%d%H%M%S')\n",
    "                            end_time_idx = np.argmin(abs(unique_apr_times - desired_end_time))\n",
    "                                    \n",
    "                        elif apr_filepath == apr_files_use[0]:  #i.e. the first APR file in the apr_files_use list\n",
    "                            end_time_idx = time.shape[1] - 1\n",
    "                            \n",
    "                            #find the closest time (and its corresponding index) to the desired start_time\n",
    "                            desired_start_time = datetime.strptime(desired_date + start_time, '%Y%m%d%H%M%S')\n",
    "                            start_time_idx = np.argmin(abs(unique_apr_times - desired_start_time))\n",
    "                                    \n",
    "                        else:  #apr_filepath == apr_files_use[-1]  #i.e. the last APR file in the apr_files_use list\n",
    "                            start_time_idx = 0\n",
    "                            \n",
    "                            #find the closest time (and its corresponding index) to the desired end_time\n",
    "                            desired_end_time = datetime.strptime(desired_date + end_time, '%Y%m%d%H%M%S')\n",
    "                            end_time_idx = np.argmin(abs(unique_apr_times - desired_end_time))                \n",
    "                            \n",
    "                    else:  #the entire APR file is within the desired time range, so set the start/end indices to the first/last indices of the APR file\n",
    "                        start_time_idx = 0\n",
    "                        end_time_idx = time.shape[1] - 1\n",
    "                            \n",
    "                    #numpy.histogram2d(dBZ, z) (Zagrodnik et al., 2019) CFAD method:\n",
    "\n",
    "                    #loop through the given APR files valid times/scans/profiles\n",
    "                    for time_idx in range(start_time_idx, end_time_idx + 1):          #time_idx represents the scan number\n",
    "                    \n",
    "                        profile_time = datetime.utcfromtimestamp(time[12, time_idx])  #12 could be any ray, as it is the ray number and each ray of a given scan has the same time\n",
    "                        ecco_ip = np.argmin(abs(ecco_df['Full_Datetime'] - profile_time))  #abs() necessary to properly subtract datetime objects\n",
    "                                                                                           #returns the integer position of the minimum value\n",
    "                        #print (ecco_df['Full_Datetime'].iloc[ecco_index])           #to check that the correct time was chosen\n",
    "                        classification = ecco_df['Classification'].iloc[ecco_ip]     #grab the classification of the ECCO-V time closest to the given profile time (may/should be the exact time)\n",
    "                        \n",
    "                        if classification not in ['0', '1', '2']:\n",
    "                            raise ValueError(f'{classification} is not a valid ECCO-V classification.')\n",
    "                    \n",
    "                        # #skip profiles with an ECCO-V classification of \"no classification\" (0) or \"stratiform\" (1)\n",
    "                        # #i.e., only use \"convective\" (2) profiles\n",
    "                        # if classification == '0' or classification == '1':\n",
    "                        #     continue\n",
    "                        \n",
    "                        #skip profiles with an ECCO-V classification of \"no classification\" (0) or \"convective\" (2)\n",
    "                        #i.e., only use \"stratiform\" (1) profiles\n",
    "                        if classification == '0' or classification == '2':\n",
    "                            continue\n",
    "                        \n",
    "                        total_apr_profiles += 1\n",
    "                        \n",
    "                        #choose the \"nadir\" ray factoring in aircraft roll\n",
    "                        ac_roll = np.nanmean(roll[:,time_idx])  #roll varies slightly w/ray, so take the average roll value for a given scan and use that for ray adjustment\n",
    "                        ray_use = np.argmin(np.abs(ray_angles - ac_roll))  #the index of the ray whose angle is closest to that of ac_roll\n",
    "                        \n",
    "                        if abs(ac_roll) >= 10:\n",
    "                            apr_profile_roll10 += 1\n",
    "                        \n",
    "                        prof_height = alt3d[:,ray_use,time_idx]\n",
    "                        prof_dbz = ku_data[:,ray_use,time_idx]\n",
    "                        \n",
    "                        #grab the height/dbz data above 1.5km\n",
    "                        idx_1500m = np.argmin(np.abs(prof_height - 1500))\n",
    "                        prof_height_above1500m = np.flip(prof_height[:idx_1500m + 1])  #height profile above 1.5km, in ascending order\n",
    "                        prof_dbz_above1500m = np.flip(prof_dbz[:idx_1500m + 1])  #Ku-band profile above 1.5km, in ascending order\n",
    "                        \n",
    "                        #store all the profile's height/dbz data in long 1-D concatenated arrays\n",
    "                        if new_ku_array:  #if this is the first qualifying time and ray of the first APR file of the first case in case_dict\n",
    "                            height_concat = prof_height.copy()\n",
    "                            dbz_concat = prof_dbz.copy()\n",
    "                            \n",
    "                            #create a DataFrame of values above 1.5km, in order to calculate meadian/quartile reflectivity profile for the given case\n",
    "                                #NOTE: WE ARE ASSUMING THAT EACH ROW CORRESPONDS TO A NEAR IDENTICAL HEIGHT LEVEL\n",
    "                                    #this is the best we can do, and should be fine given that each profile is starting from ~1500m and the height resolution is the same for each profile     \n",
    "                            height_profiles_above1500m_df = pd.DataFrame(prof_height_above1500m)  #height profile from 1500m to top of profile\n",
    "                            dbz_profiles_above1500m_df = pd.DataFrame(prof_dbz_above1500m)        #dbz profile from 1500m to top of profile\n",
    "                            new_ku_array = False\n",
    "                        else:\n",
    "                            height_concat = np.concatenate((height_concat, prof_height))\n",
    "                            dbz_concat = np.concatenate((dbz_concat, prof_dbz))\n",
    "                            \n",
    "                            #NOTE: WE ARE ASSUMING THAT EACH ROW CORRESPONDS TO A NEAR IDENTICAL HEIGHT LEVEL\n",
    "                                #this is the best we can do, and should be fine given that each profile is starting from ~1500m and the height resolution is the same for each profile\n",
    "                            height_profiles_above1500m_df = pd.concat((height_profiles_above1500m_df, pd.Series(prof_height_above1500m)), axis = 1, ignore_index = True)  #add profile as a new column to the df; differences in profile lengths are filled in with NaNs\n",
    "                            dbz_profiles_above1500m_df = pd.concat((dbz_profiles_above1500m_df, pd.Series(prof_dbz_above1500m)), axis = 1, ignore_index = True)  #add profile as a new column to the df; differences in profile lengths are filled in with NaNs\n",
    "                        \n",
    "                        if vel_good:\n",
    "                            prof_vel = vel_data[:,ray_use,time_idx]\n",
    "                            \n",
    "                            #store all the profile's velocity data in long 1-D concatenated arrays if \n",
    "                            #the profile has Ku data > 0 dBZ above 1.5km (i.e. omits clear profiles)\n",
    "                            \n",
    "                            if np.nanmax(prof_dbz_above1500m) > 0:\n",
    "                                if new_vel_array:  #if this is the first qualifying time and ray of the first APR file of the first case in case_dict\n",
    "                                    vel_concat = prof_vel.copy()\n",
    "                                    height_vel_concat = prof_height.copy()\n",
    "                                    new_vel_array = False\n",
    "                                else:\n",
    "                                    vel_concat = np.concatenate((vel_concat, prof_vel))\n",
    "                                    height_vel_concat = np.concatenate((height_vel_concat, prof_height))\n",
    "                            else:  #clear profiles (which may have noisy velocity data) are omitted from the velocity CFAD\n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "                apr_file.close()\n",
    "                \n",
    "            elif desired_date[:4] == '2022':    #CPEX-CV\n",
    "            \n",
    "                ku_band = 'lores_zhh14' #Ku-band reflectivity\n",
    "                vel = 'lores_vel14c' #Mean Doppler Velocity from Ku-band (surface Doppler velocity is subtracted and free of aliasing)\n",
    "            \n",
    "                apr_file = xr.open_dataset(os.path.join(apr_folder, apr_filepath))\n",
    "                \n",
    "                if ku_band in apr_file.keys():\n",
    "                    \n",
    "                    try:   #some APR files, at least in the preliminary data, have corrupted Ku-band data; \n",
    "                           #if so, skip plotting the Ku-band for that file \n",
    "                           #corrupted: \"OSError: Can't read data (inflate() failed)\"\n",
    "                        ku_data = apr_file[ku_band][:]\n",
    "                    except:\n",
    "                        first_good_Ku_file_index += 1    #used to determine first usable Ku-band file for CPEX-CV cases below; without this, an error is brought up due to the first APR file in apr_files_use only being W-band data (i.e., _Wn.nc)\n",
    "                        apr_file.close()\n",
    "                        continue  #both Ku-band and velocity CFADs rely on Ku-band data availability\n",
    "    \n",
    "                    try:   #some APR files, at least in the preliminary data, have corrupted velocity data; \n",
    "                           #if so, skip plotting the Ku-band for that file \n",
    "                           #corrupted: \"OSError: Can't read data (inflate() failed)\"\n",
    "                        vel_data = apr_file[vel][:]\n",
    "                        vel_good = True\n",
    "                    except:\n",
    "                        vel_good = False                    \n",
    "                \n",
    "                    #grab the radar variables of interest\n",
    "                    time = apr_file['time'][:]          #For 'lores': Time of scan, in seconds since midnight UTC of [YYYY-mm-DD]\n",
    "                    alt3d = apr_file['lores_alt3D'][:]\n",
    "                    roll = apr_file['lores_roll'][:]\n",
    "                    \n",
    "                    if apr_filepath == apr_files_use[first_good_Ku_file_index] or apr_filepath == apr_files_use[-1]:\n",
    "                    #the complete APR file time range may not need to be used, so need to locate the closest time (and corresponding index) to the desired start/end time\n",
    "                        \n",
    "                        #Convert APR times to datetimes\n",
    "                        time_dates = np.empty(time.shape, dtype=object)\n",
    "                        for i in np.arange(0, time.shape[0]):\n",
    "                            #hour, second automatically revert to midnight (hour = 0, seconds = 0) for '%Y%m%d'\n",
    "                            time_dates[i] = datetime.strptime(desired_date, '%Y%m%d') + timedelta(seconds = float(time[i].values))\n",
    "                        \n",
    "                        unique_apr_times = time_dates[:]  #all the times in the given APR file\n",
    "                        \n",
    "                        if len(apr_files_use) == 1:  #i.e. apr_filepath == apr_files_use[first_good_Ku_file_index] and apr_filepath == apr_files_use[-1]\n",
    "                        \n",
    "                            #find the closest time (and its corresponding index) to the desired start_time\n",
    "                            desired_start_time = datetime.strptime(desired_date + start_time, '%Y%m%d%H%M%S')\n",
    "                            start_time_idx = np.argmin(abs(unique_apr_times - desired_start_time))\n",
    "                        \n",
    "                            #find the closest time (and its corresponding index) to the desired end_time\n",
    "                            desired_end_time = datetime.strptime(desired_date + end_time, '%Y%m%d%H%M%S')\n",
    "                            end_time_idx = np.argmin(abs(unique_apr_times - desired_end_time))\n",
    "                                    \n",
    "                        elif apr_filepath == apr_files_use[first_good_Ku_file_index]:  #i.e. the first APR file in the apr_files_use list\n",
    "                            end_time_idx = time.shape[0] - 1\n",
    "                            \n",
    "                            #find the closest time (and its corresponding index) to the desired start_time\n",
    "                            desired_start_time = datetime.strptime(desired_date + start_time, '%Y%m%d%H%M%S')\n",
    "                            start_time_idx = np.argmin(abs(unique_apr_times - desired_start_time))\n",
    "                                    \n",
    "                        else:  #apr_filepath == apr_files_use[-1]  #i.e. the last APR file in the apr_files_use list\n",
    "                            start_time_idx = 0\n",
    "                            \n",
    "                            #find the closest time (and its corresponding index) to the desired end_time\n",
    "                            desired_end_time = datetime.strptime(desired_date + end_time, '%Y%m%d%H%M%S')\n",
    "                            end_time_idx = np.argmin(abs(unique_apr_times - desired_end_time))                \n",
    "                            \n",
    "                    else:  #the entire APR file is within the desired time range, so set the start/end indices to the first/last indices of the APR file\n",
    "                        start_time_idx = 0\n",
    "                        end_time_idx = time.shape[0] - 1\n",
    "                            \n",
    "                    #numpy.histogram2d(dBZ, z) (Zagrodnik et al., 2019) CFAD method:\n",
    "                    \n",
    "########################################################################################################################### \n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################                       \n",
    "                    \n",
    "                    #loop through the given APR files valid times/scans/profiles\n",
    "                    for time_idx in range(start_time_idx, end_time_idx + 1):          #time_idx represents the scan number\n",
    "                        \n",
    "                        profile_time = datetime.strptime(desired_date, '%Y%m%d') + timedelta(seconds = float(time[time_idx].values))\n",
    "                        ecco_ip = np.argmin(abs(ecco_df['Full_Datetime'] - profile_time))  #abs() necessary to properly subtract datetime objects\n",
    "                                                                                           #returns the integer position of the minimum value\n",
    "                        #print (ecco_df['Full_Datetime'].iloc[ecco_index])           #to check that the correct time was chosen\n",
    "                        classification = ecco_df['Classification'].iloc[ecco_ip]     #grab the classification of the ECCO-V time closest to the given profile time (may/should be the exact time)\n",
    "                        \n",
    "                        if classification not in ['0', '1', '2']:\n",
    "                            raise ValueError(f'{classification} is not a valid ECCO-V classification.')\n",
    "                    \n",
    "                        # #skip profiles with an ECCO-V classification of \"no classification\" (0) or \"stratiform\" (1)\n",
    "                        # #(i.e., only use \"convective\" (2) profiles)\n",
    "                        # if classification == '0' or classification == '1':\n",
    "                        #     continue\n",
    "                        \n",
    "                        #skip profiles with an ECCO-V classification of \"no classification\" (0) or \"convective\" (2)\n",
    "                        #(i.e., only use \"stratiform\" (1) profiles)\n",
    "                        if classification == '0' or classification == '2':\n",
    "                            continue\n",
    "                        \n",
    "                        total_apr_profiles += 1\n",
    "                        \n",
    "                        #choose the \"nadir\" ray factoring in aircraft roll\n",
    "                        ac_roll = np.nanmean(roll[time_idx,:])  #roll varies slightly w/ray, so take the average roll value for a given scan and use that for ray adjustment\n",
    "                        ray_use = np.argmin(np.abs(ray_angles - ac_roll))  #the index of the ray whose angle is closest to that of ac_roll; i.e., the \"pseudo-nadir\" ray\n",
    "                        \n",
    "                        if abs(ac_roll) >= 10:\n",
    "                            apr_profile_roll10 += 1\n",
    "                        \n",
    "                        prof_height = alt3d[time_idx,ray_use,:].values\n",
    "                        prof_dbz = ku_data[time_idx,ray_use,:].values\n",
    "                        \n",
    "                        #grab the height/dbz data above 1.5km\n",
    "                        idx_1500m = np.argmin(np.abs(prof_height - 1500))\n",
    "                        prof_height_above1500m = np.flip(prof_height[:idx_1500m + 1])  #height profile above 1.5km, in ascending order\n",
    "                        prof_dbz_above1500m = np.flip(prof_dbz[:idx_1500m + 1])  #Ku-band profile above 1.5km, in ascending order\n",
    "                        \n",
    "                        #store all the profile's height/dbz data in long 1-D concatenated arrays\n",
    "                        if new_ku_array:  #if this is the first qualifying time and ray of the first APR file of the first case in case_dict\n",
    "                            height_concat = prof_height.copy()\n",
    "                            dbz_concat = prof_dbz.copy()\n",
    "                            \n",
    "                            #create a DataFrame of values above 1.5km, in order to calculate median/quartile reflectivity profile for the given case\n",
    "                                #NOTE: WE ARE ASSUMING THAT EACH ROW CORRESPONDS TO A NEAR IDENTICAL HEIGHT LEVEL\n",
    "                                    #this is the best we can do, and should be fine given that each profile is starting from ~1500m and the height resolution is the same for each profile     \n",
    "                            height_profiles_above1500m_df = pd.DataFrame(prof_height_above1500m)  #height profile from 1500m to top of profile\n",
    "                            dbz_profiles_above1500m_df = pd.DataFrame(prof_dbz_above1500m)        #dbz profile from 1500m to top of profile\n",
    "                            new_ku_array = False\n",
    "                        else:\n",
    "                            height_concat = np.concatenate((height_concat, prof_height))\n",
    "                            dbz_concat = np.concatenate((dbz_concat, prof_dbz))\n",
    "                            \n",
    "                            #NOTE: WE ARE ASSUMING THAT EACH ROW CORRESPONDS TO A NEAR IDENTICAL HEIGHT LEVEL\n",
    "                                #this is the best we can do, and should be fine given that each profile is starting from ~1500m and the height resolution is the same for each profile\n",
    "                            height_profiles_above1500m_df = pd.concat((height_profiles_above1500m_df, pd.Series(prof_height_above1500m)), axis = 1, ignore_index = True)  #add profile as a new column to the df; differences in profile lengths are filled in with NaNs\n",
    "                            dbz_profiles_above1500m_df = pd.concat((dbz_profiles_above1500m_df, pd.Series(prof_dbz_above1500m)), axis = 1, ignore_index = True)  #add profile as a new column to the df; differences in profile lengths are filled in with NaNs\n",
    "                        \n",
    "                        if vel_good:\n",
    "                            prof_vel = vel_data[time_idx,ray_use,:].values\n",
    "                            \n",
    "                            #store all the profile's velocity data in long 1-D concatenated arrays if \n",
    "                            #the profile has Ku data > 0 dBZ above 1.5km (i.e. omits clear profiles)\n",
    "                            \n",
    "                            if np.nanmax(prof_dbz_above1500m) > 0:\n",
    "                                if new_vel_array:  #if this is the first qualifying time and ray of the first APR file of the first case in case_dict\n",
    "                                    vel_concat = prof_vel.copy()\n",
    "                                    height_vel_concat = prof_height.copy()\n",
    "                                    new_vel_array = False\n",
    "                                else:\n",
    "                                    vel_concat = np.concatenate((vel_concat, prof_vel))\n",
    "                                    height_vel_concat = np.concatenate((height_vel_concat, prof_height))\n",
    "                            else:  #clear profiles (which may have noisy velocity data) are omitted from the velocity CFAD\n",
    "                                pass\n",
    "                        else:\n",
    "                            pass\n",
    "                            \n",
    "########################################################################################################################### \n",
    "###########################################################################################################################\n",
    "###########################################################################################################################\n",
    "###########################################################################################################################                            \n",
    "                            \n",
    "                else:\n",
    "                    first_good_Ku_file_index += 1    #used to determine first usable Ku-band file for CPEX-CV cases below; without this, an error is brought up due to the first APR file in apr_files_use only being W-band data (i.e., _Wn.nc)\n",
    "                    \n",
    "                apr_file.close()    \n",
    "                \n",
    "            else:\n",
    "                sys.exit('Not a CPEX or CPEX-AW or CPEX-CV case')\n",
    "                \n",
    "        print ('Case {} complete'.format(key))\n",
    "        \n",
    "    #calculate median/quantile dBZ profiles for the given case\n",
    "    \n",
    "    #replace blank data (-99.99) with NaNs\n",
    "    dbz_profiles_above1500m_df[dbz_profiles_above1500m_df <= -99] = np.nan\n",
    "    height_profiles_above1500m_df[height_profiles_above1500m_df < 0] = np.nan\n",
    "    \n",
    "    #convert all the dropsonde's Ku-band data from dBZ to mm^6 m^-3\n",
    "    Z_df = 10**(dbz_profiles_above1500m_df / 10)\n",
    "    \n",
    "    #calculate the dropsonde's median/quantile heights and reflectivity (in mm^6 m^-3) and standard deviation at each height level\n",
    "    #NOTE: WE ARE ASSUMING THAT EACH ROW CORRESPONDS TO A NEAR IDENTICAL HEIGHT LEVEL\n",
    "        #this is the best we can do, and should be fine given that each profile is starting from ~1500m and the height resolution is the same for each profile\n",
    "        \n",
    "    median_height_profile_above1500m = height_profiles_above1500m_df.median(axis = 1)  #axis of 1 = across all columns (so for each row); NaNs are ignored by default in Pandas\n",
    "    Z_median = Z_df.median(axis = 1)  #axis of 1 = across all columns (so for each row); NaNs are ignored by default in Pandas\n",
    "    Z_Q1 = Z_df.quantile(q = 0.25, axis = 1)\n",
    "    Z_Q3 = Z_df.quantile(q = 0.75, axis = 1)\n",
    "    #Z_std = Z_df.std(axis = 1)\n",
    "    \n",
    "    #convert the dropsonde's median/quartile reflectivity and standard deviation profile back to dBZ\n",
    "    medianDBZ_profile_above1500m = 10 * np.log10(Z_median)\n",
    "    Q1DBZ_profile_above1500m = 10 * np.log10(Z_Q1)\n",
    "    Q3DBZ_profile_above1500m = 10 * np.log10(Z_Q3)\n",
    "    #stdDBZ_profile_above1500m = 10 * np.log10(Z_std)      \n",
    "        \n",
    "    \n",
    "    #setting the height, reflectivity, and velocity bin edges, along with their associated meshgrids\n",
    "    height_edges = height_bin_edges\n",
    "    dbz_edges = dbz_bin_edges\n",
    "    #dbz_meshgrid, height_meshgrid = np.meshgrid(dbz_edges, height_edges)\n",
    "    \n",
    "    vel_edges = vel_bin_edges\n",
    "    #vel_meshgrid, height_vel_meshgrid = np.meshgrid(vel_edges, height_edges)\n",
    "    \n",
    "    #setting the height, reflectivity, and velocity bin centers and their associated meshgrids for contourf plotting\n",
    "    height_centers = (height_edges[:-1] + height_edges[1:]) / 2    \n",
    "    dbz_centers = (dbz_edges[:-1] + dbz_edges[1:]) / 2\n",
    "    dbz_centers_meshgrid, height_centers_meshgrid = np.meshgrid(dbz_centers, height_centers)\n",
    "    \n",
    "    vel_centers = (vel_edges[:-1] + vel_edges[1:]) / 2\n",
    "    vel_centers_meshgrid, height_vel_centers_meshgrid = np.meshgrid(vel_centers, height_centers)\n",
    "  \n",
    "    #create the 2D histogram of values/frequencies for Ku-band data\n",
    "    cfad_array, xedges, yedges = np.histogram2d(dbz_concat, height_concat, bins = (dbz_edges,height_edges))\n",
    "    \n",
    "    #transpose cfad_array shape (rows, columns) to be (height, dbz) instead of (dbz, height)\n",
    "    cfad_array = cfad_array.T\n",
    "    #cfad_array = np.log10(cfad_array)  #creates log-weighted CFADs\n",
    "    contours = np.linspace(0, np.nanmax(cfad_array), 21)\n",
    "    colorbar_label = 'Total Frequency [#]'\n",
    "    var_label = 'Ku-band Reflectivity [dBZ]'\n",
    "    save_label = 'CFAD_Cases' + '-'.join(map(str, list(case_dict.keys()))) + '_Ku.png'\n",
    "    plot_CFAD(cfad_array, contours, colorbar_label, save_label, var_label, dbz_centers_meshgrid, height_centers_meshgrid, dbz_edges, case_dict, median_height_profile_above1500m, medianDBZ_profile_above1500m, Q1DBZ_profile_above1500m, Q3DBZ_profile_above1500m, dbz_plot = True)\n",
    "    \n",
    "    #normalize the CFAD and make a normalized CFAD plot if that is also desired\n",
    "    if normalize:\n",
    "        #cfad_array = cfad_array / np.nanmax(cfad_array) * 100\n",
    "        height_bin_maxs = np.nanmax(cfad_array, axis = 1)   #normalize the CFAD by max count in each height bin\n",
    "        cfad_array = cfad_array / height_bin_maxs[:, np.newaxis] * 100\n",
    "        contours = np.arange(0,101,5)\n",
    "        colorbar_label = 'Normalized Frequency [%]'\n",
    "        save_label = 'CFADnorm_Cases' + '-'.join(map(str, list(case_dict.keys()))) + '_Ku.png'\n",
    "        plot_CFAD(cfad_array, contours, colorbar_label, save_label, var_label, dbz_centers_meshgrid, height_centers_meshgrid, dbz_edges, case_dict, median_height_profile_above1500m, medianDBZ_profile_above1500m, Q1DBZ_profile_above1500m, Q3DBZ_profile_above1500m, dbz_plot = True)\n",
    "        \n",
    "        \n",
    "    #create the 2D histogram of values/frequencies for Doppler Velocity data\n",
    "    cfad_array_vel, xedges, yedges = np.histogram2d(vel_concat, height_vel_concat, bins = (vel_edges,height_edges))\n",
    "    \n",
    "    #transpose cfad_array shape (rows, columns) to be (height, vel) instead of (vel, height)\n",
    "    cfad_array_vel = cfad_array_vel.T\n",
    "    #cfad_array_vel = np.log10(cfad_array_vel)  #creates log-weighted CFADs\n",
    "    contours = np.linspace(0, np.nanmax(cfad_array_vel), 21)\n",
    "    colorbar_label = 'Total Frequency [#]'\n",
    "    var_label = 'Mean Doppler Velocity [m/s]'\n",
    "    save_label = 'CFAD_Cases' + '-'.join(map(str, list(case_dict.keys()))) + '_Vel.png'\n",
    "    plot_CFAD(cfad_array_vel, contours, colorbar_label, save_label, var_label, vel_centers_meshgrid, height_vel_centers_meshgrid, vel_edges, case_dict, median_height_profile_above1500m, medianDBZ_profile_above1500m, Q1DBZ_profile_above1500m, Q3DBZ_profile_above1500m, dbz_plot = False)\n",
    "    \n",
    "    #normalize the CFAD and make a normalized CFAD plot if that is also desired\n",
    "    if normalize:\n",
    "        #cfad_array_vel = cfad_array_vel / np.nanmax(cfad_array_vel) * 100\n",
    "        height_bin_maxs = np.nanmax(cfad_array_vel, axis = 1)   #normalize the CFAD by max count in each height bin\n",
    "        cfad_array_vel = cfad_array_vel / height_bin_maxs[:, np.newaxis] * 100\n",
    "        contours = np.arange(0,101,5)\n",
    "        colorbar_label = 'Normalized Frequency [%]'\n",
    "        save_label = 'CFADnorm_Cases' + '-'.join(map(str, list(case_dict.keys()))) + '_Vel.png'\n",
    "        plot_CFAD(cfad_array_vel, contours, colorbar_label, save_label, var_label, vel_centers_meshgrid, height_vel_centers_meshgrid, vel_edges, case_dict, median_height_profile_above1500m, medianDBZ_profile_above1500m, Q1DBZ_profile_above1500m, Q3DBZ_profile_above1500m, dbz_plot = False)\n",
    "        \n",
    "    #print ('Percent of profiles with A/C roll >= 10 degrees:', apr_profile_roll10 / total_apr_profiles * 100)\n",
    "    \n",
    "    return cfad_array, cfad_array_vel, dbz_centers_meshgrid, height_centers_meshgrid, vel_centers_meshgrid, height_vel_centers_meshgrid, median_height_profile_above1500m, medianDBZ_profile_above1500m, Q1DBZ_profile_above1500m, Q3DBZ_profile_above1500m\n",
    "    \n",
    "\n",
    "def plot_diffCFAD(cfad_array, contours, colorbar_label, save_label, var_label, var_centers_meshgrid, height_centers_meshgrid, var_edges, height_edges, case1_num, case2_num, first_median_height_profile_above1500m, first_medianDBZ_profile_above1500m, first_Q1DBZ_profile_above1500m, first_Q3DBZ_profile_above1500m, second_median_height_profile_above1500m, second_medianDBZ_profile_above1500m, second_Q1DBZ_profile_above1500m, second_Q3DBZ_profile_above1500m, dbz_plot = True):\n",
    "    \n",
    "    \"\"\"Plot a contourf difference CFAD given a difference CFAD 2-D array, contour levels, plot/image labels, and variable/height meshgrids\"\"\"\n",
    "    \n",
    "    #plot the CFAD\n",
    "    fig, ax = plt.subplots(1,1, figsize=(21,21))\n",
    "    # cmap = mplc.ListedColormap(['#00429d', '#2855a6', '#3e68af', '#507bb8', '#618fc1', '#73a3ca', '#85b8d3', \n",
    "    #                             '#99ccdc', '#b0e0e6', '#cef2f1', '#ffffff', '#ffe5e7', '#ffcbcf', '#ffafb7', \n",
    "    #                             '#ff929e', '#f57789', '#e95d76', '#d94364', '#c62a54', '#af1046', '#93003a'])\n",
    "    \n",
    "    cmap = mplc.ListedColormap(['#00429d', '#074ca2', '#0e57a8', '#1561ad', '#1d6bb2', '#2475b8', '#2b7fbd', \n",
    "                                '#3289c2', '#3994c8', '#409ecd', '#48a8d2', '#4fb3d8', '#56bddd', '#5ec8e3', \n",
    "                                '#65d3e8', '#6dddee', '#74e8f4', '#7cf3f9', '#83feff', '#cbffff', '#ffffff', \n",
    "                                '#fef3f0', '#fce7e0', '#fbdad0', '#facec1', '#f8c2b1', '#f7b5a1', '#f5a890', \n",
    "                                '#f49b7f', '#f28d6e', '#f17f5c', '#ef6f48', '#ed5f33', '#eb4c1b', '#e13f18', \n",
    "                                '#d5351e', '#c82b23', '#bc2128', '#af162e', '#a10b34', '#93003a'])\n",
    "    \n",
    "    #cs = ax.pcolormesh(dbz_meshgrid, height_meshgrid, cfad_array, cmap = cmap)\n",
    "    #cs = ax.contour(dbz_centers_meshgrid, height_centers_meshgrid, cfad_array, levels = contours, cmap = cmap, linewidths = 1)\n",
    "    cs = ax.contourf(var_centers_meshgrid, height_centers_meshgrid, cfad_array, levels = contours, cmap = cmap)\n",
    "    if dbz_plot:\n",
    "        ax.plot(second_medianDBZ_profile_above1500m, second_median_height_profile_above1500m, color = 'k', linestyle = '-', linewidth = 5, label = 'Case ' + case2_num + ' Median Profile') \n",
    "        ax.plot(second_Q1DBZ_profile_above1500m, second_median_height_profile_above1500m, color = 'k', linestyle = '--', linewidth = 5, label = 'Case ' + case2_num + ' Q1 and Q3 Profiles')\n",
    "        ax.plot(second_Q3DBZ_profile_above1500m, second_median_height_profile_above1500m, color = 'k', linestyle = '--', linewidth = 5)\n",
    "        \n",
    "        ax.plot(first_medianDBZ_profile_above1500m, first_median_height_profile_above1500m, color = 'darkgoldenrod', linestyle = '-', linewidth = 5, label = 'Case ' + case1_num + ' Median Profile') \n",
    "        ax.plot(first_Q1DBZ_profile_above1500m, first_median_height_profile_above1500m, color = 'darkgoldenrod', linestyle = '--', linewidth = 5, label = 'Case ' + case1_num + ' Q1 and Q3 Profiles')\n",
    "        ax.plot(first_Q3DBZ_profile_above1500m, first_median_height_profile_above1500m, color = 'darkgoldenrod', linestyle = '--', linewidth = 5)\n",
    "        \n",
    "        ax.legend(loc = 'upper right')\n",
    "        \n",
    "    ax.set_ylabel('Altitude [m]', fontsize=30, fontweight = 'bold')\n",
    "    ax.set_xlabel(var_label, fontsize=30, fontweight = 'bold')\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    #ax.set_title('Difference CFAD (Case ' + case2_num + ' minus ' + 'Case ' + case1_num + ')', fontsize=35, fontweight = 'bold')\n",
    "    ax.set_title('b) Difference CFAD (Case 12 minus Case 10, Only Stratiform Regions)', fontsize=34.5, fontweight = 'bold')\n",
    "    ax.set_ylim([height_edges[0] + 250, height_edges[-1] - 250])\n",
    "    #ax.set_xlim([var_edges[0],var_edges[-1]])\n",
    "    ax.set_xlim([5,55])  #for Ku-band\n",
    "    \n",
    "    #set the colorbar axis\n",
    "    cax = fig.add_axes([ax.get_position().x0, ax.get_position().y0 - 0.08,\n",
    "                       ax.get_position().x1-ax.get_position().x0, 0.02])    #Left, bottom, width, height (all [0,1])\n",
    "\n",
    "    #create the colorbar\n",
    "    cbar = plt.colorbar(cs, cax = cax, orientation = 'horizontal')\n",
    "    cbar.ax.tick_params(length = 10, width = 3, labelsize = 23)\n",
    "    cbar.set_label(label = colorbar_label, fontsize = 30, fontweight = 'bold')\n",
    "    \n",
    "    #save the figure\n",
    "    #plt.savefig(''.join(['/Users/brodenkirch/Desktop/CPEX-CV/Coding/CFAD_plots/Case', case1_num, 'and', case2_num, '_Comparison/', save_label]), bbox_inches = 'tight')\n",
    "    plt.savefig(''.join(['/Users/ben/Desktop/', save_label]), bbox_inches = 'tight')    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def difference_CFAD(case_dict_1, case_dict_2, height_bin_edges, dbz_bin_edges, vel_bin_edges):\n",
    "\n",
    "    \"\"\"\"Calculate and plot Ku-band and Doppler Velocity difference (normalized) CFAD 2-D arrays for \n",
    "        the given pair of dictionaries containing given cases and their respective time ranges\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    case_dict_1:  dictionary of cases for which to calculate the first normalized CFAD; \n",
    "                  keys should be case numbers;\n",
    "                  values should be 3-element lists of case date and start/end times, \n",
    "                  with date strings formatted as YYYYMMDD and time strings formatted as HHMMSS\n",
    "                  \n",
    "    case_dict_2:  dictionary of cases for which to calculate the second normalized CFAD; \n",
    "                  keys should be case numbers;\n",
    "                  values should be 3-element lists of case date and start/end times, \n",
    "                  with date strings formatted as YYYYMMDD and time strings formatted as HHMMSS\n",
    "    \n",
    "    height_bin_edges:  a 1-D array of height [m] bin edges used to create the 2-D difference CFAD array/histogram    \n",
    "\n",
    "    dbz_bin_edges:  a 1-D array of reflectivity [dBZ] bin edges used to create the 2-D difference CFAD array/histogram \n",
    "\n",
    "    vel_bin_edges:  a 1-D array of Doppler velocity [m/s] bin edges used to create the 2-D difference CFAD array/histogram\n",
    "                        \n",
    "    return:  2-D (normalized) difference CFAD plots (case_dict_2 - case_dict_1)\"\"\"\n",
    "\n",
    "    #calculate the Ku-band and Doppler Velocity 2-D normalized CFADs for each case\n",
    "    first_dbz_CFAD, first_vel_CFAD, dbz_centers_meshgrid, height_centers_meshgrid, vel_centers_meshgrid, height_vel_centers_meshgrid, first_median_height_profile_above1500m, first_medianDBZ_profile_above1500m, first_Q1DBZ_profile_above1500m, first_Q3DBZ_profile_above1500m = CFAD(case_dict_1, height_bin_edges, dbz_bin_edges, vel_bin_edges, normalize = True)\n",
    "    second_dbz_CFAD, second_vel_CFAD, dbz_centers_meshgrid, height_centers_meshgrid, vel_centers_meshgrid, height_vel_centers_meshgrid, second_median_height_profile_above1500m, second_medianDBZ_profile_above1500m, second_Q1DBZ_profile_above1500m, second_Q3DBZ_profile_above1500m = CFAD(case_dict_2, height_bin_edges, dbz_bin_edges, vel_bin_edges, normalize = True)\n",
    "    \n",
    "    #calculate the Ku-band and Doppler Velocity 2-D difference CFADs\n",
    "    dbz_diff_CFAD = second_dbz_CFAD - first_dbz_CFAD\n",
    "    vel_diff_CFAD = second_vel_CFAD - first_vel_CFAD\n",
    "    \n",
    "    #calculate the maximum magnitude for each CFAD and create contours for diverging colormaps accordingly\n",
    "    \n",
    "    #dbz_highest_mag = np.nanmax(np.abs(dbz_diff_CFAD))  #creates unique colorbar range for each difference CFAD plot\n",
    "    dbz_highest_mag = 100  #creates uniform colorbar range across all difference CFAD plots\n",
    "    dbz_contours = np.linspace(-dbz_highest_mag, dbz_highest_mag, 41)  #an odd number of intervals (21) guarantees a contour at 0 for a range from -x to x\n",
    "    \n",
    "    #vel_highest_mag = np.nanmax(np.abs(vel_diff_CFAD))  #creates unique colorbar range for each difference CFAD plot\n",
    "    vel_highest_mag = 100  #creates uniform colorbar range across all difference CFAD plots\n",
    "    vel_contours = np.linspace(-vel_highest_mag, vel_highest_mag, 41)  #an odd number of intervals (21) guarantees a contour at 0 for a range from -x to x\n",
    "    \n",
    "    #grab the case numbers of the 2 cases/case sets being differenced and create appropriate image names\n",
    "    case1_num = ','.join(map(str, list(case_dict_1.keys())))\n",
    "    case2_num = ','.join(map(str, list(case_dict_2.keys())))\n",
    "    cases = case1_num + 'and' + case2_num\n",
    "    colorbar_label = 'Differential Normalized Frequency [%]'\n",
    "    dbz_save_label = 'diff_CFADnorm_Cases_' + cases + '_Ku.png'\n",
    "    vel_save_label = 'diff_CFADnorm_Cases_' + cases + '_Vel.png'\n",
    "\n",
    "    #plot the Ku-band and Doppler Velocity 2-D difference CFADs\n",
    "    plot_diffCFAD(dbz_diff_CFAD, dbz_contours, colorbar_label, dbz_save_label, 'Ku-band Reflectivity [dBZ]', dbz_centers_meshgrid, height_centers_meshgrid, dbz_bin_edges, height_bin_edges, case1_num, case2_num, first_median_height_profile_above1500m, first_medianDBZ_profile_above1500m, first_Q1DBZ_profile_above1500m, first_Q3DBZ_profile_above1500m, second_median_height_profile_above1500m, second_medianDBZ_profile_above1500m, second_Q1DBZ_profile_above1500m, second_Q3DBZ_profile_above1500m, dbz_plot = False) \n",
    "    plot_diffCFAD(vel_diff_CFAD, vel_contours, colorbar_label, vel_save_label, 'Mean Doppler Velocity [m/s]', vel_centers_meshgrid, height_vel_centers_meshgrid, vel_bin_edges, height_bin_edges, case1_num, case2_num, first_median_height_profile_above1500m, first_medianDBZ_profile_above1500m, first_Q1DBZ_profile_above1500m, first_Q3DBZ_profile_above1500m, second_median_height_profile_above1500m, second_medianDBZ_profile_above1500m, second_Q1DBZ_profile_above1500m, second_Q3DBZ_profile_above1500m, dbz_plot = False)\n",
    "  \n",
    "\n",
    "#run the CFAD function to plot the CFADs\n",
    "\n",
    "#Note: CPEX-CV Case 4 does not have any stratiform nor convective data, as identified by the ECCO-V algorithm\n",
    "#Note: CPEX-CV Cases 5, 7, and 8 do not have any convective data, as identified by the ECCO-V algorithm\n",
    "\n",
    "\n",
    "#Isolated Difference CFADs\n",
    "# difference_CFAD(case1_dict, case2_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case1_dict, case3_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case1_dict, case4_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case1_dict, case5_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case1_dict, case6_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case1_dict, case7_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case2_dict, case3_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case2_dict, case4_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case2_dict, case5_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case2_dict, case6_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case2_dict, case7_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case3_dict, case4_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case3_dict, case5_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case3_dict, case6_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case3_dict, case7_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case4_dict, case5_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case4_dict, case6_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case4_dict, case7_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case5_dict, case6_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case5_dict, case7_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case6_dict, case7_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "#Organized Difference CFADs\n",
    "# difference_CFAD(case8_dict, case9_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case10_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case11_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case12_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case13_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case14_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case15_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case16_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case17_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case8_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case9_dict, case10_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case11_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case12_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case13_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case14_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case15_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case16_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case17_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case9_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case10_dict, case11_dict, height_edges, dbz_edges, vel_edges)\n",
    "difference_CFAD(case10_dict, case12_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case10_dict, case13_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case10_dict, case14_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case10_dict, case15_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case10_dict, case16_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case10_dict, case17_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case10_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case10_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case10_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case10_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case11_dict, case12_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case11_dict, case13_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case11_dict, case14_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case11_dict, case15_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case11_dict, case16_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case11_dict, case17_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case11_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case11_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case11_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case11_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case12_dict, case13_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case12_dict, case14_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case12_dict, case15_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case12_dict, case16_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case12_dict, case17_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case12_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case12_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case12_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case12_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case13_dict, case14_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case13_dict, case15_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case13_dict, case16_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case13_dict, case17_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case13_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case13_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case13_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case13_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case14_dict, case15_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case14_dict, case16_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case14_dict, case17_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case14_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case14_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case14_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case14_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case15_dict, case16_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case15_dict, case17_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case15_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case15_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case15_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case15_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case16_dict, case17_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case16_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case16_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case16_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case16_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case17_dict, case18_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case17_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case17_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case17_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case18_dict, case20_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case18_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case18_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case20_dict, case21_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case20_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case21_dict, case22_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "#Scattered Difference CFADs\n",
    "# difference_CFAD(case23_dict, case24_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "#Scattered vs. Isolated Difference CFADs\n",
    "# difference_CFAD(case1_dict, case23_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case2_dict, case23_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case3_dict, case23_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case4_dict, case23_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case5_dict, case23_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case6_dict, case23_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case7_dict, case23_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# difference_CFAD(case1_dict, case24_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case2_dict, case24_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case3_dict, case24_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case4_dict, case24_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case5_dict, case24_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case6_dict, case24_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case7_dict, case24_dict, height_edges, dbz_edges, vel_edges)\n",
    "\n",
    "# # CFADs for CPEX(-AW) AGU Paper\n",
    "# difference_CFAD(case2_dict, case1_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case7_dict, case8_dict, height_edges, dbz_edges, vel_edges)\n",
    "# difference_CFAD(case13_dict, case16_dict, height_edges, dbz_edges, vel_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc1b71f-86df-47db-ba34-0dada2de3c55",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figure 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9a750-6c17-44eb-9e41-2a4384b2a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from ERA5_Analysis.ipynb\n",
    "#Case 10 and 12 GPM IMERG, MIMIC TPW, and ERA5 precip. for top 4 panels, then smoothed TIMPS lifecycle time series for bottom panel\n",
    "\n",
    "#plot time-mean total precipitation rate with flight track and dropsondes overlaid for each desired flight & time ranges in case_dict_stream\n",
    "\n",
    "#list of desired flight datetimes to plot IMERG, ERA5 precip, and MIMIC TPW for (format: YYYYMMDDHH)\n",
    "case_dict_stream = ['2022090617', '2022090717', '2022090621', '2022090721']\n",
    "\n",
    "#set some baseline plot displays\n",
    "\n",
    "#matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 32\n",
    "matplotlib.rcParams['axes.titlesize'] = 36\n",
    "matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "matplotlib.rcParams['xtick.labelsize'] = 30\n",
    "matplotlib.rcParams['ytick.labelsize'] = 30\n",
    "matplotlib.rcParams['legend.fontsize'] = 30\n",
    "#matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "#matplotlib.rcParams['axes.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "\n",
    "#group_fig = plt.figure(figsize = (24,24))   #initialize the figure\n",
    "group_fig = plt.figure(figsize = (24,36))   #initialize the figure\n",
    "data_proj = ccrs.PlateCarree()\n",
    "\n",
    "#define a GridSpec with 3 rows and 2 columns\n",
    "gs = gridspec.GridSpec(3, 2, figure = group_fig)\n",
    "\n",
    "#file_date is the date and hour on which the desired flight took place\n",
    "for ii, file_date in enumerate(case_dict_stream):\n",
    "\n",
    "    print (file_date + ' precip. rate plot in progress...')\n",
    "\n",
    "    #ax = group_fig.add_subplot(2,2,ii+1, projection = data_proj)\n",
    "    if ii == 0:\n",
    "        ax = group_fig.add_subplot(gs[0, 0], projection = data_proj)\n",
    "        letter_label = 'a)'\n",
    "    elif ii == 1:\n",
    "        ax = group_fig.add_subplot(gs[0, 1], projection = data_proj)\n",
    "        letter_label = 'b)'\n",
    "    elif ii == 2:\n",
    "        ax = group_fig.add_subplot(gs[1, 0], projection = data_proj)\n",
    "        letter_label = 'c)'\n",
    "    elif ii == 3:\n",
    "        ax = group_fig.add_subplot(gs[1, 1], projection = data_proj)\n",
    "        letter_label = 'd)'\n",
    "    \n",
    "    ###get locations of the dropsonde/Navigation/ERA5 folder and read the appropriate files in\n",
    "    day_folder = os.path.join(os.getcwd(), file_date[:-2])\n",
    "\n",
    "    #dropsonde data\n",
    "    drop_csv_path = os.path.join(day_folder, 'final_dropsonde_' + file_date[:-2] + '.csv')\n",
    "    drop_csv = pd.read_csv(drop_csv_path)\n",
    "\n",
    "    if file_date[:4] == '2017':\n",
    "        campaign = 'CPEX'\n",
    "        drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations.csv')\n",
    "    elif file_date[:4] == '2021':\n",
    "        campaign = 'CPEXAW'\n",
    "        drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations.csv')\n",
    "    elif file_date[:4] == '2022':\n",
    "        campaign = 'CPEXCV'\n",
    "        drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #ERA5 data\n",
    "    era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "    era5_path = os.path.join(era5_folder, campaign + '_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "    ds = xr.open_dataset(era5_path)\n",
    "\n",
    "    era5_path_precip = os.path.join(era5_folder, campaign + '_ERA5_Reanalysis_Hourly_Precip_Avg.nc')\n",
    "    ds_precip = xr.open_dataset(era5_path_precip)\n",
    "\n",
    "    #Navigation data\n",
    "    nav_folder = os.path.join(day_folder, 'Nav_files')\n",
    "\n",
    "    for x in os.listdir(nav_folder):\n",
    "        if x[0:3] == '.DS':         #delete hidden .DS_Store files if they come up (will show up if you delete a file)\n",
    "            os.remove(os.path.join(nav_folder, x))\n",
    "\n",
    "    nav_ict_path = os.path.join(nav_folder, os.listdir(nav_folder)[0])  #only one nav file per flight\n",
    "\n",
    "    if campaign != 'CPEX':  #campaign either CPEXAW or CPEXCV\n",
    "\n",
    "        nav_ict = icartt.Dataset(nav_ict_path)    #open the ict file with icartt\n",
    "        flight_lat = nav_ict.data[\"Latitude\"]     #nav latitude, just a normal 1-D array\n",
    "        flight_lon = nav_ict.data[\"Longitude\"]    #nav longitude, just a normal 1-D array\n",
    "\n",
    "    else:  #for CPEX navigation files, open the CSV file with pandas\n",
    "           #(Navigation files for CPEX (2017) are originally .kmz not .ict,\n",
    "           #so I converted them to CSV for free using https://www.gpsvisualizer.com/convert_input\n",
    "\n",
    "        nav_ict = pd.read_csv(nav_ict_path)       #open the ict file with pandas instead\n",
    "        flight_lat = nav_ict[\"latitude\"].values   #nav latitude, just a normal 1-D array\n",
    "        flight_lon = nav_ict[\"longitude\"].values  #nav longitude, just a normal 1-D array    \n",
    "\n",
    "    #flight track lat/lon extent [West,East,South,North] for plotting, giving an XX degree buffer around the flight track   \n",
    "    #campaign_extent = [flight_lon.min() - 2.5, flight_lon.max() + 2.5, flight_lat.min() - 2.5, flight_lat.max() + 2.5]\n",
    "    campaign_extent = [-42, -22, 4.5, 24.5]\n",
    "\n",
    "\n",
    "    ###calculate each near-storm dropsonde's mean lat/lon and add the sonde's time and mean lat/lon to a list to be plotted\n",
    "\n",
    "    drop_coords_and_time = []   #format: longitude, latitude, time (HHSS)\n",
    "\n",
    "    df_drop = pd.read_csv(drop_metric_filepath)\n",
    "    df_drop_use = df_drop[df_drop['Date'] == int(file_date[:-2])].copy()\n",
    "\n",
    "    for x in range(len(df_drop_use)):\n",
    "        date = str(df_drop_use['Date'].iloc[x])\n",
    "        time = str(df_drop_use['Time'].iloc[x]).zfill(6)\n",
    "        sonde_datetime = date[:4] + '-' + date[4:6] + '-' + date[6:] + ' ' + time[:2] + ':' + time[2:4] + ':' + time[4:]\n",
    "\n",
    "        drop_csv_use = drop_csv[drop_csv['Time [UTC]'] == sonde_datetime].copy()\n",
    "        drop_mean_lon = drop_csv_use['Longitude [deg]'].mean()\n",
    "        drop_mean_lat = drop_csv_use['Latitude [deg]'].mean()\n",
    "\n",
    "        sonde_info = [drop_mean_lon, drop_mean_lat, time[:4]]\n",
    "        drop_coords_and_time.append(sonde_info)\n",
    "\n",
    "        \n",
    "    ###create an XX-panel plot of streamlines at XX-different pressure levels \n",
    "        ###for each desired hour of the given day\n",
    "\n",
    "    #half_pres_levs = math.ceil(len(pressures_to_plot_stream) / 2)   #to determine size of figure and number of subplots to generate (rounded up to the nearest multiple of 2)\n",
    "\n",
    "    hr = int(file_date[-2:])\n",
    "    hr2 = str(hr).zfill(2)\n",
    "    hour_prior = str(hr - 1).zfill(2)\n",
    "\n",
    "    #MIMIC TPW data\n",
    "       ##https://bin.ssec.wisc.edu/pub/mtpw2/data/\n",
    "    tpw_folder = os.path.join(day_folder, 'MIMIC_TPW_files')\n",
    "    tpw_path = os.path.join(tpw_folder, 'comp' + file_date[:-2] + '.' + hr2 + '0000.nc')\n",
    "    ds_tpw = xr.open_dataset(tpw_path)\n",
    "\n",
    "    #GPM IMERG data (see IMERG.ipynb for more how to more generally download and plot IMERG data)\n",
    "       ##https://disc.gsfc.nasa.gov/information/howto?title=How%20to%20Read%20IMERG%20Data%20Using%20Python\n",
    "       ##https://disc.gsfc.nasa.gov/datasets?keywords=imerg&page=1\n",
    "       #0.1 x 0.1 gridded data, half-hourly means, using the half hour BEFORE the desired hour\n",
    "    imerg_folder = os.path.join(day_folder, 'IMERG_files')\n",
    "    \n",
    "    for x in os.listdir(imerg_folder):\n",
    "        if x[0:3] == '.DS':         #delete hidden .DS_Store files if they come up (will show up if you delete a file)\n",
    "            os.remove(os.path.join(imerg_folder, x))\n",
    "            \n",
    "        #minutes and seconds automatically revert to zero (hour = 0, seconds = 0) for '%Y%m%d%H'\n",
    "        elif (datetime.strftime(datetime.strptime(file_date[:-2] + hr2, '%Y%m%d%H') - timedelta(seconds = 1), '%H%M%S') in x) and (datetime.strftime(datetime.strptime(file_date[:-2] + hr2, '%Y%m%d%H') - timedelta(minutes = 30), '%H%M%S') in x):\n",
    "            imerg_file = x\n",
    "            break\n",
    "        else:\n",
    "            imerg_file = 'Could not find the desired IMERG file'\n",
    "    \n",
    "    #confirm that the IMERG file is from the correct day (if hr2 == '00', then this will be the previous day)\n",
    "    assert (file_date[:-2] in imerg_file) or (hr2 == '00'), 'IMERG file not from the correct day'\n",
    "    \n",
    "    imerg_path = os.path.join(imerg_folder, imerg_file)\n",
    "    ds_imerg = h5py.File(imerg_path, 'r')\n",
    "    \n",
    "    imerg_lons = ds_imerg['Grid/lon'][:]   #Longitude Shape: (3600,)\n",
    "    imerg_lats = ds_imerg['Grid/lat'][:]   #Latitude Shape: (1800,)\n",
    "    imerg_lons, imerg_lats = np.meshgrid(imerg_lons, imerg_lats)  #Long and lat grid shape: (1800, 3600) \n",
    "    \n",
    "    imerg_precip = ds_imerg['Grid/precipitation'][0][:][:]  #Original Precip Shape: (1, 3600, 1800) = (time, lon, lat)\n",
    "    imerg_precip = np.transpose(imerg_precip)               #New Precip Shape after transpose: (1800, 3600)\n",
    "    \n",
    "    #mask blank data\n",
    "    imerg_precip_masked = np.ma.masked_where(imerg_precip < 0, imerg_precip)  #masks blank and bad data first (if blank data is -999 instead of NaN)\n",
    "    imerg_precip_masked = np.ma.masked_where(np.isnan(imerg_precip_masked), imerg_precip_masked)  #masks NaN values (not masked in previous line)        \n",
    "\n",
    "    precip_rate = ds_precip.avg_tprate.sel(valid_time = file_date[:-2])                            #Time-mean total precipitation rate (kg/m2/s)\n",
    "    precip_rate = precip_rate.sel(valid_time = precip_rate.valid_time.dt.hour.isin(hr)) * 3600     #Time-mean total precipitation rate (mm/hr)        \n",
    "\n",
    "    # uwnd = ds.u.sel(time = file_date[:-2]).sel(level = pres_lev)  #zonal winds (m/s)\n",
    "    # uwnd = uwnd.sel(time = uwnd.time.dt.hour.isin(hr))       #zonal winds (m/s)\n",
    "\n",
    "    # vwnd = ds.v.sel(time = file_date[:-2]).sel(level = pres_lev)  #meridional winds (m/s)\n",
    "    # vwnd = vwnd.sel(time = vwnd.time.dt.hour.isin(hr))       #meridional winds (m/s)\n",
    "    \n",
    "    # rh = ds.r.sel(time = file_date[:-2]).sel(level = pres_lev)    #relative humidity (%)\n",
    "    # rh = rh.sel(time = rh.time.dt.hour.isin(hr))             #relative humidity (%)\n",
    "\n",
    "    # ##Smoothing (source: Hannah Zanowski) --> not recommended, see top of document\n",
    "    #     ##Metpy smooth_n_point (data to be smoothed, number of points to use in smoothing (5 to 9 are valid), and number of times the smoother is applied)\n",
    "    #         ##see https://unidata.github.io/MetPy/latest/api/generated/metpy.calc.smooth_n_point.html for more info\n",
    "    # #uwnd_smoothed = mpcalc.smooth_n_point(uwnd,9,10)\n",
    "    # #vwnd_smoothed = mpcalc.smooth_n_point(vwnd,9,10)\n",
    "\n",
    "    if file_date[:-2] == '20220906':\n",
    "        case = '10'\n",
    "    elif file_date[:-2] == '20220907':\n",
    "        case = '12'\n",
    "    else:\n",
    "        sys.exit('Case is not Case 10 or Case 12')\n",
    "        \n",
    "    ax.set_title('%s Case %s, %s UTC' % (letter_label, case, hr2))\n",
    "    #ax.set_title('MIMIC TPW, GPM IMERG, and \\nERA5 Precip. Rate (%s, %s UTC)' % (file_date[:-2], hr2))\n",
    "    #ax.set_title('ERA5 %i hPa RH, GPM IMERG, and \\nERA5 Precip. Rate (%s, %s UTC)' % (pres_lev, file_date[:-2], hr2))\n",
    "    ax.set_extent(campaign_extent, ccrs.PlateCarree()) #lat/lon bounds are [West,East,South,North]\n",
    "\n",
    "    # Add land, coastlines, and borders\n",
    "    #ax.add_feature(cfeature.LAND, facecolor='0.8')\n",
    "    ax.coastlines(ls = '-', linewidth = 4, color = 'gray')\n",
    "    \n",
    "    #plot MIMIC TPW\n",
    "    tpw_levels = np.arange(0, 70.5, 2)\n",
    "    pm0 = ax.contourf(ds_tpw.lonArr, ds_tpw.latArr, ds_tpw.tpwGrid, levels = tpw_levels,\n",
    "                      extend = 'max', cmap = cm.jet, transform = data_proj, zorder = 1)\n",
    "#             pm0 = ax.pcolormesh(ds_tpw.lonArr, ds_tpw.latArr, ds_tpw.tpwGrid, vmin = 0, vmax = 70,\n",
    "#                                 cmap = cm.jet, transform = data_proj, zorder = 1)\n",
    "\n",
    "#     #plot ERA5 RH\n",
    "#     tpw_levels = np.arange(0, 100.1, 2)  #actually RH levels, but keeping the tpw_levels name because we use it elsewhere\n",
    "#     pm0 = ax.contourf(ds.longitude, ds.latitude, rh[0].values, levels = tpw_levels,\n",
    "#                       extend = 'max', cmap = cm.jet, transform = data_proj, alpha = 0.6)\n",
    "# #             pm0 = ax.pcolormesh(ds.longitude, ds.latitude, rh[0].values, vmin = 0, vmax = 70,\n",
    "# #                                 cmap = cm.jet, transform = data_proj, zorder = 1)\n",
    "    \n",
    "    #plot IMERG Rain Rate\n",
    "    pm1 = ax.contourf(imerg_lons, imerg_lats, imerg_precip_masked, \n",
    "                      levels = np.logspace(np.log10(0.1), np.log10(40), num = len(tpw_levels)), \n",
    "                      norm = 'log', extend = 'max', \n",
    "                      cmap = cm.jet, transform = data_proj, zorder = 2)\n",
    "#             pm1 = ax.pcolormesh(imerg_lons, imerg_lats, imerg_precip_masked, \n",
    "#                                 norm = mplc.LogNorm(vmin = 0.1, vmax = 40), \n",
    "#                                 cmap = cm.jet, transform = data_proj, zorder = 2) \n",
    "\n",
    "    #Gridlines\n",
    "    gl = ax.gridlines(crs = ccrs.PlateCarree(), draw_labels = True, linewidth = 2,\n",
    "                      color = 'gray', alpha = 0.5, linestyle = '--', zorder = 3)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlabel_style = {'size':30, 'color':'black'}\n",
    "    gl.ylabel_style = {'size':30, 'color':'black'}\n",
    "\n",
    "    # #plot ERA5 streamlines\n",
    "    # ax.streamplot(ds.longitude, ds.latitude, uwnd[0].values, vwnd[0].values,\n",
    "    #               color = 'k', linewidth = 0.6, density = 1.0, transform = data_proj, zorder = 4)\n",
    "\n",
    "    #plot ERA5 time-mean total precipitation rate (mm/hr)\n",
    "    ax.contour(ds_precip.longitude, ds_precip.latitude, precip_rate[0].values, levels = [0.1, 1, 5, 10, 20, 40],\n",
    "               colors = 'k', linewidths = 2.5, transform = data_proj, zorder = 4)\n",
    "\n",
    "    #plot flight track\n",
    "    ax.plot(flight_lon, flight_lat, color = 'darkmagenta', linewidth = 4, zorder = 5)\n",
    "    \n",
    "    #plot near-storm dropsonde locations for the given flight \n",
    "        #if the dropsonde was deployed within 30 minutes (1-hr total range) of the given hour\n",
    "            #NOTE: Dropsondes with no wind data don't have GPS data either (5 of them total)\n",
    "    for sonde in drop_coords_and_time:\n",
    "        if (sonde[2][:2] == hour_prior and int(sonde[2][2:4]) >= 30) or (sonde[2][:2] == hr2 and int(sonde[2][2:4]) < 30):\n",
    "            #ax.scatter(sonde[0], sonde[1], marker = f'${sonde[2]}$', color = 'b', s = 500)\n",
    "            ax.scatter(sonde[0], sonde[1], marker = '*', color = 'k', zorder = 5, s = 500)\n",
    "\n",
    "    #same as above, but labeling the dropsondes by the order that they appear in the\n",
    "        #Dropsonde_Metric_Calculations.csv, NOT IN CHRONOLOGICAL ORDER!!!\n",
    "    #for z, sonde in enumerate(drop_coords_and_time):\n",
    "        #ax.scatter(sonde[0], sonde[1], marker = f'${z + 1}$', color = 'b', s = 120, zorder = 5) \n",
    "        \n",
    "    #plotting the colorbars\n",
    "    #cbar0 = group_fig.colorbar(pm0, ax = ax, orientation = 'vertical', shrink = 0.75, pad = 0.25)\n",
    "    #cbar0.set_label('TPW [mm]')\n",
    "    #cbar0.ax.yaxis.set_ticks_position('left')\n",
    "    #cbar0.ax.yaxis.set_label_position('left')\n",
    "        \n",
    "    # #this works with GeoAxes (i.e., Cartopy's map projections)\n",
    "    # if ii == 3:  #4th panel (last panel where the colorbar is applicable)\n",
    "        \n",
    "    #     #MIMIC TPW colorbar\n",
    "    #     ticks_tpw = np.arange(0, 70.5, 10, dtype = int)\n",
    "    #     #cax = group_fig.add_axes([ax.get_position().x1+0.08, ax.get_position().y0 + 0.007, 0.03, ax.get_position().height])\n",
    "    #     cax0 = group_fig.add_axes([ax.get_position().x1 + 0.08, ax.get_position().y0 + 0.007, 0.03, 0.756])\n",
    "    #     cbar0 = group_fig.colorbar(pm0, cax = cax0, ticks = ticks_tpw)\n",
    "    #     cbar0.set_label('TPW [mm]')\n",
    "    #     cbar0.ax.set_yticklabels(list(map(str, list(ticks_tpw))))  #labels automatically default to tick values given to ticks parameter in fig.colorbar(), unless you're using a log scale I guess\n",
    "    #     cbar0.ax.yaxis.set_ticks_position('left')\n",
    "    #     cbar0.ax.yaxis.set_label_position('left')\n",
    "\n",
    "    #     # #ERA5 RH colorbar\n",
    "    #     # ticks_rh = np.arange(0, 100.5, 10, dtype = int)\n",
    "    #     # #cax = group_fig.add_axes([ax.get_position().x1+0.08, ax.get_position().y0 + 0.007, 0.03, ax.get_position().height])\n",
    "    #     # cax0 = group_fig.add_axes([ax.get_position().x1 + 0.08, ax.get_position().y0 + 0.007, 0.03, 0.756])\n",
    "    #     # cbar0 = group_fig.colorbar(pm0, cax = cax0, ticks = ticks_rh)\n",
    "    #     # cbar0.set_label('Relative Humidity [%]')\n",
    "    #     # cbar0.ax.set_yticklabels(list(map(str, list(ticks_rh))))  #labels automatically default to tick values given to ticks parameter in fig.colorbar(), unless you're using a log scale I guess\n",
    "    #     # cbar0.ax.yaxis.set_ticks_position('left')\n",
    "    #     # cbar0.ax.yaxis.set_label_position('left')\n",
    "        \n",
    "    #     #IMERG colorbar\n",
    "    #     ticks_imerg = np.array([0.1, 1, 5, 10, 20, 40], dtype = float)\n",
    "    #     cax1 = group_fig.add_axes([ax.get_position().x1 + 0.08, ax.get_position().y0 + 0.007, 0.03, 0.756])\n",
    "    #     cbar1 = group_fig.colorbar(pm1, cax = cax1, ticks = ticks_imerg)\n",
    "    #     cbar1.set_label('IMERG [mm hr$\\\\bf{^{-1}}$]')\n",
    "    #     cbar1.ax.set_yticklabels(list(map(str, list(ticks_imerg))))  #labels automatically default to tick values given to ticks parameter in fig.colorbar(), unless you're using a log scale I guess\n",
    "    #     cbar1.ax.yaxis.set_ticks_position('right')\n",
    "    #     cbar1.ax.yaxis.set_label_position('right')\n",
    "                \n",
    "    ds_tpw.close()\n",
    "    ds_imerg.close() \n",
    "    ds.close()\n",
    "    ds_precip.close()\n",
    "\n",
    "    print (file_date + ' precip. rate plot complete!\\n')\n",
    "\n",
    "#TIMPS data\n",
    "timps_folder = os.path.join(os.getcwd(), 'TIMPS_data')\n",
    "unique_timps_id = '227341'   #same TIMPS ID for Case 10 and Case 12 (see Dropsonde_Metric_Calculations_CPEXCV.csv)\n",
    "\n",
    "for filename in os.listdir(timps_folder):\n",
    "    if unique_timps_id in filename:\n",
    "        timps_filepath = os.path.join(timps_folder, filename)\n",
    "\n",
    "case_date1 = case_dict_stream[0][:-2]\n",
    "case_date2 = case_dict_stream[1][:-2]\n",
    "\n",
    "timps_ds0 = xr.open_dataset(timps_filepath)\n",
    "#timps_ds0 = timps_ds0.sel(time = case_date1)\n",
    "vrr_max = timps_ds0.maxrr.values     #mm/hr\n",
    "timps_time = timps_ds0.time.values   #datetime\n",
    "\n",
    "#using scipy.stats.norm for standardized Gaussian weights (mean=0, std=1) to do a 13-point (6-hourly because TIMPS data is 0.5-hourly) moving average:\n",
    "x_values = np.linspace(-3, 3, 13)    #make sure however many points are used (13), the range is centered on zero for a true Gaussian/bell curve (e.g., see plt.plot(x_values, weights))\n",
    "                                     #NOTE: to increase the amount of points used in the filtering, just change the last number (13)\n",
    "                                     #      the first 2 numbers make the plot look most like a Gaussian bell curve, no matter what the last number is\n",
    "                                     #      (e.g., test other numbers and use plt.plot(x_values, weights))\n",
    "weights = norm.pdf(x_values, loc = 0, scale = 1)   #mean = 0, standard deviation = 1\n",
    "#boxcar_weights = np.ones(7)\n",
    "#plt.plot(x_values, weights)\n",
    "\n",
    "#using a non-recursive filter for moving average; a recursive filter would use both the raw data and the previously/already filtered data\n",
    "   #see Coursework/2nd Year/Spring 2022/AOS 575/aos575_applab5_synthetic_data_with_filters.ipynb for more info/examples of moving averages and filtering\n",
    "vrr_max_gfilt = sig.filtfilt(weights, np.sum(weights), vrr_max)   #applying the moving average to the data (numerator, denominator, data)\n",
    "\n",
    "ax = group_fig.add_subplot(gs[2, :])\n",
    "#ax.plot(timps_time, vrr_max, color = 'r', linestyle = '-', linewidth = 2, label = 'Unsmoothed')\n",
    "ax.plot(timps_time, vrr_max_gfilt, color = 'b', linestyle = '-', linewidth = 2, label = 'Smoothed')\n",
    "        \n",
    "ax.axvspan(datetime.strptime(case_date1 + ',161000', '%Y%m%d,%H%M%S'), datetime.strptime(case_date1 + ',180000', '%Y%m%d,%H%M%S'), color = 'orange', alpha = 0.2)   #denotes the sampling time range for Case 10\n",
    "ax.axvspan(datetime.strptime(case_date2 + ',134500', '%Y%m%d,%H%M%S'), datetime.strptime(case_date2 + ',151300', '%Y%m%d,%H%M%S'), color = 'green', alpha = 0.2)    #denotes the sampling time range for Case 12 (flight leg 1)\n",
    "ax.axvspan(datetime.strptime(case_date2 + ',161800', '%Y%m%d,%H%M%S'), datetime.strptime(case_date2 + ',174500', '%Y%m%d,%H%M%S'), color = 'green', alpha = 0.2)    #denotes the sampling time range for Case 12 (flight leg 2)\n",
    "\n",
    "range_start = datetime.strptime(case_date1 + ',133000', '%Y%m%d,%H%M%S')\n",
    "range_end = datetime.strptime('20220908,003000', '%Y%m%d,%H%M%S')\n",
    "#range_end = datetime.strptime(case_date2 + ',233000', '%Y%m%d,%H%M%S')\n",
    "\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%H\"))\n",
    "#ax.xaxis.set_major_locator(ticker.MaxNLocator(18))      #sets number of ticks\n",
    "ax.set_xlim([np.datetime64(range_start),np.datetime64(range_end)])    \n",
    "    #use the above line to narrow the plot's time range (even within a file!!)\n",
    "        #range_start and range_end must be a datetime object or a string with the \n",
    "            #format: 'YYYY-MM-DD HH:MM:SS' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "blah = range_start + timedelta(minutes = 30)\n",
    "ax.set_xticks([blah + timedelta(hours = 2 * xx) for xx in range(18)])\n",
    "\n",
    "ax.set_ylim([15,45])\n",
    "ax.set_yticks(np.arange(15, 45.1, 5))\n",
    "\n",
    "ax.set_xlabel('Time [UTC]', labelpad = 15)\n",
    "ax.set_ylabel('Maximum Rain Rate [mm hr$^{-1}$]', labelpad = 15)\n",
    "ax.set_title('e) TIMPS 0.5-hourly Maximum Rain Rate (6-hourly Smoothed)')\n",
    "\n",
    "#ax.legend(loc = 'upper right')\n",
    "\n",
    "#just some tick size settings\n",
    "ax.tick_params(axis = 'x', rotation = 50)\n",
    "ax.tick_params(length = 15, width = 5)\n",
    "\n",
    "#MIMIC TPW colorbar\n",
    "ticks_tpw = np.arange(0, 70.5, 10, dtype = int)\n",
    "#cax = group_fig.add_axes([ax.get_position().x1+0.08, ax.get_position().y0 + 0.007, 0.03, ax.get_position().height])\n",
    "cax0 = group_fig.add_axes([ax.get_position().x1 + 0.08, ax.get_position().y1 + 0.045, 0.03, 0.498])\n",
    "cbar0 = group_fig.colorbar(pm0, cax = cax0, ticks = ticks_tpw)\n",
    "cbar0.set_label('TPW [mm]')\n",
    "cbar0.ax.set_yticklabels(list(map(str, list(ticks_tpw))))  #labels automatically default to tick values given to ticks parameter in fig.colorbar(), unless you're using a log scale I guess\n",
    "cbar0.ax.yaxis.set_ticks_position('left')\n",
    "cbar0.ax.yaxis.set_label_position('left')\n",
    "\n",
    "# #ERA5 RH colorbar\n",
    "# ticks_rh = np.arange(0, 100.5, 10, dtype = int)\n",
    "# #cax = group_fig.add_axes([ax.get_position().x1+0.08, ax.get_position().y0 + 0.007, 0.03, ax.get_position().height])\n",
    "# cax0 = group_fig.add_axes([ax.get_position().x1 + 0.08, ax.get_position().y1 + 0.045, 0.03, 0.498])\n",
    "# cbar0 = group_fig.colorbar(pm0, cax = cax0, ticks = ticks_rh)\n",
    "# cbar0.set_label('Relative Humidity [%]')\n",
    "# cbar0.ax.set_yticklabels(list(map(str, list(ticks_rh))))  #labels automatically default to tick values given to ticks parameter in fig.colorbar(), unless you're using a log scale I guess\n",
    "# cbar0.ax.yaxis.set_ticks_position('left')\n",
    "# cbar0.ax.yaxis.set_label_position('left')\n",
    "\n",
    "#IMERG colorbar\n",
    "ticks_imerg = np.array([0.1, 1, 5, 10, 20, 40], dtype = float)\n",
    "cax1 = group_fig.add_axes([ax.get_position().x1 + 0.08, ax.get_position().y1 + 0.045, 0.03, 0.498])\n",
    "cbar1 = group_fig.colorbar(pm1, cax = cax1, ticks = ticks_imerg)\n",
    "cbar1.set_label('IMERG [mm hr$\\\\bf{^{-1}}$]')\n",
    "cbar1.ax.set_yticklabels(list(map(str, list(ticks_imerg))))  #labels automatically default to tick values given to ticks parameter in fig.colorbar(), unless you're using a log scale I guess\n",
    "cbar1.ax.yaxis.set_ticks_position('right')\n",
    "cbar1.ax.yaxis.set_label_position('right')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.subplots_adjust(wspace = 0.1)\n",
    "\n",
    "#save the figure\n",
    "plt.subplots_adjust(hspace = 0.2, wspace = 0.25)\n",
    "plt.savefig('/Users/ben/Desktop/Figure10.png', bbox_inches = 'tight')\n",
    "#plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "#plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "plt.close()\n",
    "plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "im = Image.open('/Users/ben/Desktop/Figure10.png')\n",
    "\n",
    "try:\n",
    "    im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "except:\n",
    "    #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "    #though this line will have isolated, extremely minor image effects due to \n",
    "    #only using 256 colors instead of the 3-element RGB scale\n",
    "    im2 = im.convert('P')\n",
    "\n",
    "im2.save('/Users/ben/Desktop/Figure10.png')\n",
    "im.close()\n",
    "im2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b2205-bfe2-4dec-bc89-a48148bd5f55",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figure 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ffadd-55b3-4976-b9af-63f3b4e959f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case 10 and 12 convection-relative convergence PDFs\n",
    "#adapted from CPEXCV_ConvergencePDFs_TIMPS.ipynb\n",
    "\n",
    "pressures_to_plot_conv = [975, 700]    #desired pressure levels for convergence\n",
    "\n",
    "#CPEX-CV convective cases to compare against one another for AGU paper\n",
    "case_dict_conv_aew = {10: ['20220906', [16,17,18]],\n",
    "                      12: ['20220907', [14,15,16,17,18]]}\n",
    "\n",
    "#THIS CELL PLOTS A 3-PANEL PLOT OF CONVECTION-RELATIVE CONVERGENCE PDFs COMPARING 2 CASES (1 PANEL PER LIFECYCLE STAGE)\n",
    "\n",
    "#For each CPEX-CV case, calculate and plot domain-PDFs of low- (975 hPa) and mid- (700 hPa) level convergence \n",
    "#(display median, mean, standard deviation, skewness, and kurtosis as well), with the domain being an \n",
    "#2-by-2 degree box around TIMPS MCS centroids (e.g., Galarneau et al., 2023)\n",
    "    #partition the data by lifecycle of the convection (use TIMPS \"gmd\" variable)\n",
    "    #for calculating domain-mean convergence, cosine-weight each grid box value (see AOS 573 material)\n",
    "    #KEEP TRACK OF WHICH GRID CELLS YOU HAVE ALREADY ADDED CONVERGENCE FOR, SO THAT YOU DON'T\n",
    "        #DOUBLE COUNT GRID CELLS WHEN YOU HAVE A CASE WITH MULTIPLE TIMPS IDS WHOSE BOXES MAY OVERLAP\n",
    "\n",
    "#CPEX-CV convective cases (average time for dropsondes for a given convective case, per CPEX-CV Well Documented Convection.docx)\n",
    "case_dict_comp = {10: ['20220906', [16,17,18]],\n",
    "                  12: ['20220907', [14,15,16,17,18]]}\n",
    "\n",
    "# case_dict_comp = {8: ['20220906', [11,12]],\n",
    "#                   9: ['20220906', [13,14,15,16]],\n",
    "#                   10: ['20220906', [16,17,18]],\n",
    "#                   11: ['20220907', [13,14]],\n",
    "#                   12: ['20220907', [14,15,16,17,18]],\n",
    "#                   13: ['20220907', [15,16]],\n",
    "#                   14: ['20220914', [10,11,12,14,15]],\n",
    "#                   15: ['20220914', [12,13,14,15,16,17]],\n",
    "#                   16: ['20220916', [14,15,16]],\n",
    "#                   17: ['20220916', [17,18,19]],\n",
    "#                   18: ['20220922', [6,7,8,9]],\n",
    "#                   19: ['20220923', [9,10,11,12,13,14,15]],\n",
    "#                   20: ['20220926', [7,8,9,10,11]],\n",
    "#                   21: ['20220929', [11,12,13,14]],\n",
    "#                   22: ['20220930', [14,15]]}        \n",
    "        \n",
    "#set some baseline plot displays\n",
    "\n",
    "#matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 21\n",
    "matplotlib.rcParams['axes.titlesize'] = 21\n",
    "matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "matplotlib.rcParams['xtick.labelsize'] = 21\n",
    "matplotlib.rcParams['ytick.labelsize'] = 21\n",
    "matplotlib.rcParams['legend.fontsize'] = 20\n",
    "#matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "#matplotlib.rcParams['axes.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "        \n",
    "#Dropsonde data\n",
    "drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "df_drop = pd.read_csv(drop_metric_filepath)\n",
    "\n",
    "#ERA5 data\n",
    "era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "era5_path = os.path.join(era5_folder, 'CPEXCV_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "ds_era5 = xr.open_dataset(era5_path)\n",
    "\n",
    "#TIMPS data\n",
    "timps_folder = os.path.join(os.getcwd(), 'TIMPS_data')\n",
    "\n",
    "conv_bins = np.arange(-15,15.1,0.5)\n",
    "\n",
    "for pres_lev in pressures_to_plot_conv:          #plot convergence PDFs at low- and mid-levels\n",
    "\n",
    "    group_fig = plt.figure(figsize = (36, 12))   #initialize the convergence PDF figure for the given case comparison\n",
    "\n",
    "    first_case_no_lifecycle_data = [False, False, False]\n",
    "    \n",
    "    for case_num in case_dict_comp.keys():\n",
    "\n",
    "        #print (f'Case {case_num} convergence PDF plots in progress...')\n",
    "\n",
    "        df_drop_case = df_drop[df_drop['Case'] == case_num].copy()\n",
    "\n",
    "        case_date = case_dict_comp[case_num][0]\n",
    "        case_hours = case_dict_comp[case_num][1]\n",
    "        case_timps_ids = df_drop_case['TIMPS ID'].unique()\n",
    "\n",
    "        for lifecycle in [1,2,3]:                    #partition convergence PDFs by TIMPS convective lifecycle stage\n",
    "\n",
    "            conv_df = pd.DataFrame()\n",
    "            conv_lats_df = pd.DataFrame()\n",
    "            #conv_lons_df = pd.DataFrame()\n",
    "\n",
    "            for hr in case_hours:\n",
    "                hr2 = str(hr).zfill(2)\n",
    "\n",
    "                coords_check_list = []\n",
    "\n",
    "                for ii, unique_timps_id in enumerate(case_timps_ids):\n",
    "\n",
    "                    timps_filepath = None\n",
    "                    if pd.isnull(unique_timps_id):\n",
    "                        continue\n",
    "                    else:\n",
    "                        unique_timps_id = str(int(unique_timps_id))\n",
    "                        for filename in os.listdir(timps_folder):\n",
    "                            if unique_timps_id in filename:\n",
    "                                timps_filepath = os.path.join(timps_folder, filename)\n",
    "                                break\n",
    "\n",
    "                    if timps_filepath == None:\n",
    "                        sys.exit(f'Could not find TIMPS file for TIMP ID {unique_timps_id}')\n",
    "                    else:\n",
    "                        timps_ds0 = xr.open_dataset(timps_filepath)\n",
    "                        timps_ds0 = timps_ds0.sel(time = case_date)\n",
    "                        timps_ds = timps_ds0.sel(time = timps_ds0.time.dt.hour.isin(hr))   #gives 2 times: minute = 0 and minute = 30\n",
    "                        timps_ds = timps_ds.sel(time = timps_ds.time.dt.minute.isin(0))    #grab the time on the hour to match with ERA5\n",
    "\n",
    "                        if len(timps_ds.gmd) == 0:\n",
    "                            print (f'{hr2} UTC is out of range of the TIMPS ID range ({timps_ds0.time[0].values.astype(str)[:-10]} - {timps_ds0.time[-1].values.astype(str)[:-10]})')\n",
    "                            timps_ds0.close()\n",
    "                            continue\n",
    "\n",
    "                        #TIMPS has an \"unidentifiable\" lifecycle phase for the collection range for Case 10, so \n",
    "                            #we're using our manual characterization of \"mature\" instead for Case 10\n",
    "                        if case_num == 10:\n",
    "                            timps_gmd = 2\n",
    "                        else:\n",
    "                            timps_gmd = timps_ds.gmd.item()\n",
    "\n",
    "                        #only add ERA5 convergence data for the given lifecycle stage\n",
    "                        if timps_gmd != lifecycle:\n",
    "                            timps_ds0.close()\n",
    "                            continue\n",
    "\n",
    "                        timps_weighted_lat = timps_ds.centlatwgt.item()\n",
    "                        timps_weighted_lon = timps_ds.centlonwgt.item()\n",
    "\n",
    "                        #create 5-by-5 degree box around weighted centroid for the given TIMPS ID at the given hour\n",
    "                        timps_lat_range = slice(timps_weighted_lat + 2.5, timps_weighted_lat - 2.5)\n",
    "                        timps_lon_range = slice(timps_weighted_lon - 2.5, timps_weighted_lon + 2.5)\n",
    "\n",
    "                        #grab all the ERA5 low-/mid-level convergence values and corresponding lats/lons within the given convective box at the given hour\n",
    "                        v700 = ds_era5.v.sel(time = case_date).sel(level = 700)\n",
    "                        v700 = v700.sel(time = v700.time.dt.hour.isin(hr))\n",
    "                        v700 = mpcalc.smooth_gaussian(v700, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "                        v700 = v700.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                        \n",
    "                        #manually calculating convergence from ERA5 u and v winds (recommended by Brandon Wolding via George Kiladis)\n",
    "                        u = ds_era5.u.sel(time = case_date).sel(level = pres_lev)\n",
    "                        u = u.sel(time = u.time.dt.hour.isin(hr))\n",
    "                        u = mpcalc.smooth_gaussian(u, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "                        #u = u.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                \n",
    "                        v = ds_era5.v.sel(time = case_date).sel(level = pres_lev)\n",
    "                        v = v.sel(time = v.time.dt.hour.isin(hr))\n",
    "                        v = mpcalc.smooth_gaussian(v, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "                        #v = v.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                        \n",
    "                        delta_lons = 0.25   #ERA5 lat/lon resolution is 0.25 degrees\n",
    "                        delta_lons_meters = (111.3195 * 1000 * delta_lons) * np.cos(u.latitude.values * np.pi/180)  #distance between longitude lines at equator is 111.3195 km and cosine weighting this distance by latitude\n",
    "                        dudx = (u[:,:,1:].values - u[:,:,:-1].values).squeeze() / np.expand_dims(np.abs(delta_lons_meters), axis=1)  #squeeze() removes dimensions of size 1 from an array, and expand_dims() inserts a new axis that will appear at the axis position\n",
    "                        dudx = np.column_stack((dudx, dudx[:,-1]))  #duplicate the last column of dudx to match original grid shape (and shape of dvdy)\n",
    "            \n",
    "                        delta_lats = 0.25\n",
    "                        delta_lats_meters = 110.5744 * 1000 * delta_lats  #distance between latitude lines everywhere\n",
    "                        dvdy = (v[:,:-1,:].values - v[:,1:,:].values).squeeze() / delta_lats_meters  #squeeze() removes dimensions of size 1 from an array\n",
    "                        dvdy = np.vstack((dvdy, dvdy[-1,:]))  #duplicate the last row of dvdy to match original grid shape (and shape of dudx)\n",
    "            \n",
    "                        conv_old = (dudx + dvdy) * -1 * 10**5  #manually calculated convergence from ERA5 u and v winds (times 10**5 1/s)\n",
    "                        ds_conv = xr.Dataset(data_vars = dict(convergence = ([\"latitude\", \"longitude\"], conv_old)),\n",
    "                                             coords = dict(latitude = (\"latitude\", u.latitude.values), \n",
    "                                                           longitude = (\"longitude\", u.longitude.values)),\n",
    "                                             attrs = dict(description = \"Manually calculated ERA5 convergence data\"))\n",
    "                        \n",
    "                        conv = ds_conv.convergence.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "\n",
    "                        # #grab all the ERA5 low-level convergence values and corresponding lats/lons within the given TIMPS ID box at the given hour\n",
    "                        # conv = ds_era5.d.sel(time = case_date).sel(level = pres_lev) * -1    #convergence of the wind (1/s)\n",
    "                        # conv = conv.sel(time = conv.time.dt.hour.isin(hr)) * 10**5  #convergence of the wind (times 10**5 1/s)\n",
    "                        # conv = conv.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "\n",
    "                        lon, lat = np.meshgrid(conv.longitude, conv.latitude)\n",
    "                        lons = lon.reshape(-1)\n",
    "                        lats = lat.reshape(-1)\n",
    "                        conv_values = conv.values.reshape(-1)\n",
    "\n",
    "                        #add data from each hour as COLUMNS to corresponding df\n",
    "                        #only add values to conv_df that haven't been already been added from a prior TIMPS ID for the given hour (so no duplicates!)\n",
    "                        if ii != 0:  #if not the first TIMPS ID for the given hour\n",
    "                            for xx, coord in enumerate(zip(lats,lons)):\n",
    "                                if coord in coords_check_list:\n",
    "                                    print (f'Duplicate coordinate for Case {case_num}, replacing with NaN')\n",
    "                                    lats[xx] = np.nan\n",
    "                                    lons[xx] = np.nan\n",
    "                                    conv_values[xx] = np.nan\n",
    "\n",
    "                        coords_check_list += list(zip(lats, lons))  #append the lats/lons to the list as tuples (coordinate pairs)\n",
    "\n",
    "                        conv_df = pd.concat((conv_df, pd.Series(conv_values)), axis = 1, ignore_index = True)\n",
    "                        conv_lats_df = pd.concat((conv_lats_df, pd.Series(lats)), axis = 1, ignore_index = True)\n",
    "                        #conv_lons_df = pd.concat((conv_lons_df, pd.Series(lons)), axis = 1, ignore_index = True)\n",
    "\n",
    "                        timps_ds0.close()\n",
    "            \n",
    "            #create or grab the axis of interest\n",
    "            if case_num == list(case_dict_comp.keys())[0]:\n",
    "                ax = group_fig.add_subplot(1, 3, lifecycle)\n",
    "                text_denom = 0\n",
    "                \n",
    "                if lifecycle == 1:\n",
    "                    clc = 'Growth'\n",
    "                    color = 'limegreen'\n",
    "                elif lifecycle == 2:\n",
    "                    clc = 'Mature'\n",
    "                    color = 'darkred'\n",
    "                elif lifecycle == 3:\n",
    "                    clc = 'Decay'\n",
    "                    color = 'cornflowerblue'\n",
    "            else:  #case_num == list(case_dict_comp.keys())[-1]\n",
    "                ax = group_fig.get_axes()[lifecycle - 1]\n",
    "                text_denom = 0.2\n",
    "\n",
    "                if lifecycle == 1:\n",
    "                    clc = 'Growth'\n",
    "                    color = 'darkgreen'\n",
    "                elif lifecycle == 2:\n",
    "                    clc = 'Mature'\n",
    "                    color = 'navy'\n",
    "                elif lifecycle == 3:\n",
    "                    clc = 'Decay'\n",
    "                    color = 'darkblue'\n",
    "            \n",
    "                #ax.set_title('ERA5 %i hPa Convergence PDFs\\n(Convection-Relative, %s Stage, 5x5 Degree Box)' % (pres_lev, clc))\n",
    "                ax.set_title('a) ERA5 %i hPa Convergence PDFs (Convection-Relative)' % (pres_lev))\n",
    "            \n",
    "            if (len(conv_df) == 0) and (case_num == list(case_dict_comp.keys())[0]):     #no data for the given lifecycle for the given case for the given hours\n",
    "                first_case_no_lifecycle_data[lifecycle - 1] = True\n",
    "                continue\n",
    "            elif (len(conv_df) == 0) and (case_num == list(case_dict_comp.keys())[-1]):  #no data for the given lifecycle for the given case for the given hours\n",
    "                if first_case_no_lifecycle_data[lifecycle - 1]:  #no data for the given lifecycle for either case\n",
    "                    ax.text(0.5, 0.5, f'{clc} stage was not sampled for\\nCase {list(case_dict_comp.keys())[0]} nor Case {case_num} during CPEX-CV', \n",
    "                            horizontalalignment = 'center', verticalalignment = 'center', \n",
    "                            fontsize = 30, bbox = {'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "                    ax.set_xticklabels([])\n",
    "                    ax.set_yticklabels([])\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            conv_df = conv_df.values  #convert Pandas DataFrame to NumPy array\n",
    "            conv_lats_df = conv_lats_df.values  #convert Pandas DataFrame to NumPy array\n",
    "\n",
    "            #mask NaN values in the Dataframes so that the numpy stat calculations work below\n",
    "            conv_df_masked = np.ma.masked_where(np.isnan(conv_df), conv_df)\n",
    "            conv_lats_df_masked = np.ma.masked_where(np.isnan(conv_lats_df), conv_lats_df)\n",
    "\n",
    "            #line plot histogram (clearer to interpret than \"step\" histogram below)\n",
    "            hist, bins = np.histogram(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None)\n",
    "            bin_centers = (bins[:-1] + bins[1:]) / 2  # Midpoints of the bins\n",
    "            ax.plot(bin_centers, hist, linewidth = 2, linestyle = '-', color = color, label = f'Case {case_num}')\n",
    "\n",
    "            # #normal \"step\" histogram\n",
    "            # ax.hist(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None,\n",
    "            #         histtype = 'step', align = 'mid', orientation = 'vertical', color = color, \n",
    "            #         linewidth = 2, label = f'Case {case_num}')\n",
    "            #     #density = True returns a probability density: each bin will display the bin's raw count \n",
    "            #         #divided by the total number of counts and the bin width\n",
    "            #         #(density = counts / (sum(counts) * np.diff(bins))), so that the area under the \n",
    "            #         #histogram integrates to 1 (np.sum(density * np.diff(bins)) == 1)\n",
    "\n",
    "            cos_weights = np.sqrt(np.cos(conv_lats_df_masked * np.pi/180))   #cosine weights to apply to conv_df\n",
    "\n",
    "            conv_count = np.count_nonzero(~np.isnan(conv_df))\n",
    "            conv_median = np.round(np.nanmedian(conv_df, axis = None), 2)\n",
    "            conv_mean = np.round(np.nanmean(conv_df, axis = None), 2)                                        #non-weighted mean (1st moment)\n",
    "            conv_wgt_mean = np.round(np.average(conv_df_masked, axis = None, weights = cos_weights), 2)      #cosine-weighted mean (1st moment)\n",
    "            conv_std = np.round(np.std(conv_df_masked, axis = None), 2)                                      #standard deviation (2nd moment)\n",
    "            conv_skew = np.round(scipy.stats.skew(conv_df_masked, axis = None, nan_policy = 'omit'), 4)      #skewness (3rd moment)\n",
    "            conv_kurt = np.round(scipy.stats.kurtosis(conv_df_masked, axis = None, nan_policy = 'omit'), 4)  #kurtosis (4th moment)\n",
    "\n",
    "            ax.axvline(x = 0, color = 'k', linestyle = '--', alpha = 0.5)\n",
    "            \n",
    "            # ax.text(0.98, 0.89 - text_denom, \n",
    "            #         f'Count: {conv_count}\\nMedian: {conv_median}\\nMean: {conv_mean}\\nWeighted Mean: {conv_wgt_mean}\\nStandard Deviation: {conv_std}\\nSkewness: {conv_skew}\\nKurtosis: {conv_kurt}\\n', \n",
    "            #         transform = ax.transAxes, horizontalalignment = 'right', verticalalignment = 'center', \n",
    "            #         fontsize = 16, fontweight = 'bold', color = color)\n",
    "                    \n",
    "        #print (f'Case {case_num} convergence PDF plots complete!\\n')\n",
    "                    \n",
    "    for ax in group_fig.get_axes():\n",
    "        ax.set_xlabel('Convergence [10$^{-5}$ s$^{-1}$]')\n",
    "        ax.set_ylabel('Prob(Convergence)')\n",
    "        ax.set_xlim([-13,13])\n",
    "        ax.set_xticks(np.arange(-13, 13.1, 2))\n",
    "        ax.set_ylim(bottom = 0)\n",
    "        #ax.set_yticks(np.arange(0, 0.401, 0.05))\n",
    "        ax.grid(axis = 'y')\n",
    "        ax.legend(loc = 'upper left')\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    #plt.subplots_adjust(wspace = 0.1)\n",
    "    \n",
    "    #save the figure\n",
    "    case_nums = '-'.join(map(str, case_dict_comp.keys()))\n",
    "    plot_save_name = f'Case{case_nums}_{pres_lev}hPa_convergence_PDFs_convection_relative_separate.png'\n",
    "    plt.savefig(os.path.join('/Users/ben/Desktop', plot_save_name), bbox_inches = 'tight')\n",
    "    #plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "    #plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "    plt.close()\n",
    "    plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "    ##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "    ##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "    im = Image.open(os.path.join('/Users/ben/Desktop', plot_save_name))\n",
    "\n",
    "    try:\n",
    "        im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "    except:\n",
    "        #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "        #though this line will have isolated, extremely minor image effects due to \n",
    "        #only using 256 colors instead of the 3-element RGB scale\n",
    "        im2 = im.convert('P')\n",
    "\n",
    "    im2.save(os.path.join('/Users/ben/Desktop', plot_save_name))\n",
    "    im.close()\n",
    "    im2.close()\n",
    "\n",
    "ds_era5.close()\n",
    "\n",
    "#tend = time.time()\n",
    "#print (f'This script took {np.round((tend - tstart) / 60, 1)} minutes to complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a395eca0-8e05-4668-be65-32646c17228c",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figure 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710137f7-e3bc-4699-b7f9-81311017e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case 10 and 12 AEW-relative convergence PDFs\n",
    "#adapted from CPEXCV_ConvergencePDFs_TIMPS.ipynb\n",
    "\n",
    "pressures_to_plot_conv = [975, 700]    #desired pressure levels for convergence\n",
    "\n",
    "#CPEX-CV convective cases to compare against one another for AGU paper\n",
    "case_dict_conv_aew = {10: ['20220906', [16,17,18]],\n",
    "                      12: ['20220907', [14,15,16,17,18]]}\n",
    "\n",
    "#THIS CELL PLOTS A 1-PANEL PLOT OF AEW-RELATIVE CONVERGENCE PDFs COMPARING 2 CASES (1 PANEL PER AEW SECTOR)\n",
    "\n",
    "#For each CPEX-CV case, calculate and plot domain-PDFs of low- (975 hPa) and mid- (700 hPa) level convergence \n",
    "#(display median, mean, standard deviation, skewness, and kurtosis as well), with the domain being a \n",
    "#10-by-10 degree box around the AEW center for the given case (get from Quintons AEW tracker: https://osf.io/jnv5u, https://zenodo.org/records/13350860)\n",
    "    #partition the data into the 2 sectors of the AEW for the given case (ahead/behind the AEW)\n",
    "    #for calculating domain-mean convergence, cosine-weight each grid box value (see AOS 573 material)\n",
    "    \n",
    "    #WON'T NEED TO KEEP TRACK OF WHICH GRID CELLS YOU HAVE ALREADY ADDED CONVERGENCE FOR, SINCE YOU'RE NOT\n",
    "        #WORKING WITH MULTIPLE AEWs AT A GIVEN HOUR (LIKE YOU WERE WITH MULTIPLE TIMPS IDs PER HOUR)\n",
    "        \n",
    "    #Also don't need to split up convergence PDFs by convective lifecycle (just ahead/behind the AEW), because\n",
    "        #it would be difficult to relate convergence of an AEW region to convective lifecycle, since one\n",
    "        #AEW region could (and likely often does) have convective systems that are in different lifecycle stages\n",
    "\n",
    "#set some baseline plot displays\n",
    "\n",
    "#matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 21\n",
    "matplotlib.rcParams['axes.titlesize'] = 21\n",
    "matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "matplotlib.rcParams['xtick.labelsize'] = 21\n",
    "matplotlib.rcParams['ytick.labelsize'] = 21\n",
    "matplotlib.rcParams['legend.fontsize'] = 20\n",
    "#matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "#matplotlib.rcParams['axes.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "        \n",
    "#Dropsonde data\n",
    "drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "df_drop = pd.read_csv(drop_metric_filepath)\n",
    "\n",
    "#ERA5 data\n",
    "era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "era5_path = os.path.join(era5_folder, 'CPEXCV_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "ds_era5 = xr.open_dataset(era5_path)\n",
    "\n",
    "#AEW tracker data, 6-hourly (Quintons AEW tracker: https://osf.io/jnv5u, https://zenodo.org/records/13350860)\n",
    "aew_folder = os.path.join(os.getcwd(), 'AEW_Tracker_Data')\n",
    "aew_path = os.path.join(aew_folder, 'AEW_tracks_post_processed_year_2022.nc')\n",
    "ds_aew = xr.open_dataset(aew_path)\n",
    "\n",
    "#TIMPS data\n",
    "timps_folder = os.path.join(os.getcwd(), 'TIMPS_data')\n",
    "\n",
    "conv_bins = np.arange(-15,15.1,0.5)\n",
    "\n",
    "#plot convergence PDFs at low- and mid-levels for each AEW sector for each case\n",
    "for pres_lev in pressures_to_plot_conv:\n",
    "    for sector in ['full', 'south_ahead', 'ahead', 'north_ahead', 'south_behind', 'behind', 'north_behind', 'north']:\n",
    "    \n",
    "        group_fig = plt.figure(figsize = (12, 12))   #initialize the convergence PDF figure for the given case\n",
    "        ax = group_fig.add_subplot(1, 1, 1)\n",
    "        \n",
    "        for case_num in case_dict_conv_aew.keys():\n",
    "        \n",
    "            #print (f'Case {case_num} convergence PDF plots in progress...')\n",
    "            \n",
    "            aew_system_index_use = None\n",
    "            \n",
    "            df_drop_case = df_drop[df_drop['Case'] == case_num].copy()\n",
    "            \n",
    "            case_date = case_dict_conv_aew[case_num][0]\n",
    "            case_hours = case_dict_conv_aew[case_num][1]\n",
    "            case_timps_ids = df_drop_case['TIMPS ID'].unique()\n",
    "            \n",
    "            if len(case_timps_ids) == 1 and pd.isnull(case_timps_ids[0]):  #no TIMPS IDs for the given case\n",
    "                continue\n",
    "            \n",
    "            aews_at_given_date = ds_aew.sel(time = case_date)\n",
    "            day_after = datetime.strptime(case_date, '%Y%m%d') + timedelta(days = 1)  #day after case_date\n",
    "            aews_at_given_date_plus1 = ds_aew.sel(time = datetime.strftime(day_after, '%Y%m%d'))\n",
    "            \n",
    "            #match the convective case to the nearest AEW (longitudinally) from Quinton's tracker\n",
    "                #and confirm that each TIMPS ID for the given case matches to the same AEW (sanity check)\n",
    "            case_hour_nearest_multiple6 = case_hours[np.nanargmin(np.abs(np.array(case_hours) % 6))]  #find the hour closest to a multiple of 6 (AEW tracker is only in 6-hourly intervals)\n",
    "            nearest_6_to_case_hour = 6 * round(case_hour_nearest_multiple6 / 6)  #find the multiple of 6 closest to case_hour_nearest_multiple6\n",
    "        \n",
    "            for ii, unique_timps_id in enumerate(case_timps_ids):\n",
    "                \n",
    "                unique_timps_id = str(int(unique_timps_id))\n",
    "                \n",
    "                timps_filepath = None\n",
    "                for filename in os.listdir(timps_folder):\n",
    "                    if unique_timps_id in filename:\n",
    "                        timps_filepath = os.path.join(timps_folder, filename)\n",
    "                        break\n",
    "        \n",
    "                if timps_filepath == None:\n",
    "                    sys.exit(f'Could not find TIMPS file for TIMP ID {unique_timps_id}')\n",
    "                else:\n",
    "                    timps_ds0 = xr.open_dataset(timps_filepath)\n",
    "                    timps_ds0 = timps_ds0.sel(time = case_date)\n",
    "                    timps_ds = timps_ds0.sel(time = timps_ds0.time.dt.hour.isin(case_hour_nearest_multiple6))   #gives 2 times: minute = 0 and minute = 30\n",
    "                    timps_ds = timps_ds.sel(time = timps_ds.time.dt.minute.isin(0))    #grab the time on the hour to match with ERA5 and AEW tracker\n",
    "        \n",
    "                    if len(timps_ds.gmd) == 0:\n",
    "                        print (f'{str(case_hour_nearest_multiple6).zfill(2)} UTC is out of range of the TIMPS ID range ({timps_ds0.time[0].values.astype(str)[:-10]} - {timps_ds0.time[-1].values.astype(str)[:-10]})')\n",
    "                        timps_ds0.close()\n",
    "                        continue\n",
    "        \n",
    "                    #timps_weighted_lat = timps_ds.centlatwgt.item()\n",
    "                    timps_weighted_lon = timps_ds.centlonwgt.item()\n",
    "                    \n",
    "                    aews_at_given_hour = aews_at_given_date.sel(time = aews_at_given_date.time.dt.hour.isin(nearest_6_to_case_hour))\n",
    "                \n",
    "                    lon_difs = aews_at_given_hour['AEW_lon_smooth'][:,0] - timps_weighted_lon\n",
    "                    aew_system_index = np.nanargmin(np.abs(lon_difs).values)   #np.argmin() would grab a NaN value!\n",
    "                    \n",
    "                    if aew_system_index_use != None:\n",
    "                        if case_num != 22:\n",
    "                            assert aew_system_index == aew_system_index_use, 'TIMPS IDs for the given case are not matching to the same AEW'\n",
    "                        else:  #Case 22 has 2 TIMPS IDs which incorrectly match to different AEWs;\n",
    "                               #the 2nd TIMPS ID (304352) matches to the correct AEW, so we're forcing the code to choose that AEW here (not ideal, but just one case we need to do this for)\n",
    "                            aew_system_index_use = aew_system_index * 1   #the number of the AEW in the AEW tracker file (using * 1 so that aew_system_index_use variable doesn't point (i.e., isn't tied to) to the same reference as aew_system_index)\n",
    "                            print (f\"Actual AEW system: {aew_system_index_use + 1}, AEW central longitude and strength at {case_date} {nearest_6_to_case_hour} UTC: {np.round(aews_at_given_hour['AEW_lon_smooth'][aew_system_index_use,0].item(), 2)}, {aews_at_given_hour['AEW_strength'][aew_system_index_use,0].item()} s-1\")\n",
    "                    else:\n",
    "                        aew_system_index_use = aew_system_index * 1   #the number of the AEW in the AEW tracker file (using * 1 so that aew_system_index_use variable doesn't point (i.e., isn't tied to) to the same reference as aew_system_index)\n",
    "                        print (f\"AEW system: {aew_system_index_use + 1}, AEW central longitude and strength at {case_date} {nearest_6_to_case_hour} UTC: {np.round(aews_at_given_hour['AEW_lon_smooth'][aew_system_index_use,0].item(), 2)}, {aews_at_given_hour['AEW_strength'][aew_system_index_use,0].item()} s-1\")\n",
    "                        \n",
    "                    timps_ds0.close()\n",
    "                \n",
    "            matched_aew_ds = aews_at_given_date.isel(system = aew_system_index_use)\n",
    "            matched_aew_ds_dayafter = aews_at_given_date_plus1.isel(system = aew_system_index_use)\n",
    "    \n",
    "            conv_df = pd.DataFrame()\n",
    "            conv_lats_df = pd.DataFrame()\n",
    "            #conv_lons_df = pd.DataFrame()\n",
    "    \n",
    "            for hr in case_hours:\n",
    "    \n",
    "                #grab/calculate the AEW centroid coordinates for the given hr\n",
    "                if hr % 6 == 0:  #if hr is a multiple of 6, then don't need to interpolate the AEW centroid at all\n",
    "                    matched_aew_smoothed_lat = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(hr)).AEW_lat_smooth.item()\n",
    "                    matched_aew_smoothed_lon = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(hr)).AEW_lon_smooth.item()\n",
    "    \n",
    "                else:  #interpolate the centroid of the matched AEW to the given hr\n",
    "    \n",
    "                    #find the multiple of 6 directly below/equal to hr; this will be your starting hour to interpolate the AEW centroid to hr\n",
    "                    nearest_6_below_hr = 6 * (hr // 6)\n",
    "                    nearest_6_above_hr = nearest_6_below_hr + 6  #this will be your ending hour to interpolate the AEW centroid to hr\n",
    "    \n",
    "                    matched_aew_start_hr_ds = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(nearest_6_below_hr))\n",
    "    \n",
    "                    if nearest_6_above_hr == 24:  #grab 00 UTC from the next day\n",
    "                        matched_aew_end_hr_ds = matched_aew_ds_dayafter.sel(time = matched_aew_ds_dayafter.time.dt.hour.isin(0))\n",
    "                    else:\n",
    "                        matched_aew_end_hr_ds = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(nearest_6_above_hr))\n",
    "    \n",
    "                    matched_aew_start_lat = matched_aew_start_hr_ds.AEW_lat_smooth.item()\n",
    "                    matched_aew_start_lon = matched_aew_start_hr_ds.AEW_lon_smooth.item()\n",
    "                    matched_aew_end_lat = matched_aew_end_hr_ds.AEW_lat_smooth.item()\n",
    "                    matched_aew_end_lon = matched_aew_end_hr_ds.AEW_lon_smooth.item()\n",
    "    \n",
    "                    #AEW centroid moves XX degrees per hour\n",
    "                    lat_per_hour = (matched_aew_end_lat - matched_aew_start_lat) / 6\n",
    "                    lon_per_hour = (matched_aew_end_lon - matched_aew_start_lon) / 6\n",
    "    \n",
    "                    #interpolated AEW centroid at the given hr\n",
    "                    matched_aew_smoothed_lat = matched_aew_start_lat + (lat_per_hour * (hr % 6))\n",
    "                    matched_aew_smoothed_lon = matched_aew_start_lon + (lon_per_hour * (hr % 6))\n",
    "    \n",
    "                #create 10-by-10 degree box around the (interpolated) AEW centroid at the given hr\n",
    "                aew_lat_range = slice(matched_aew_smoothed_lat + 5, matched_aew_smoothed_lat - 5)\n",
    "                aew_lon_range = slice(matched_aew_smoothed_lon - 5, matched_aew_smoothed_lon + 5)\n",
    "    \n",
    "                #grab all the ERA5 low-/mid-level convergence values and corresponding lats/lons within the given AEW box at the given hour\n",
    "                    #and separate the data into the 2 sectors (ahead/behind) of the AEW\n",
    "                v700 = ds_era5.v.sel(time = case_date).sel(level = 700)\n",
    "                v700 = v700.sel(time = v700.time.dt.hour.isin(hr))\n",
    "                v700 = mpcalc.smooth_gaussian(v700, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "                v700 = v700.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "            \n",
    "                #manually calculating convergence from ERA5 u and v winds (recommended by Brandon Wolding via George Kiladis)\n",
    "                u = ds_era5.u.sel(time = case_date).sel(level = pres_lev)\n",
    "                u = u.sel(time = u.time.dt.hour.isin(hr))\n",
    "                u = mpcalc.smooth_gaussian(u, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "                #u = u.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "                v = ds_era5.v.sel(time = case_date).sel(level = pres_lev)\n",
    "                v = v.sel(time = v.time.dt.hour.isin(hr))\n",
    "                v = mpcalc.smooth_gaussian(v, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "                #v = v.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "                delta_lons = 0.25   #ERA5 lat/lon resolution is 0.25 degrees\n",
    "                delta_lons_meters = (111.3195 * 1000 * delta_lons) * np.cos(u.latitude.values * np.pi/180)  #distance between longitude lines at equator is 111.3195 km and cosine weighting this distance by latitude\n",
    "                dudx = (u[:,:,1:].values - u[:,:,:-1].values).squeeze() / np.expand_dims(np.abs(delta_lons_meters), axis=1)  #squeeze() removes dimensions of size 1 from an array, and expand_dims() inserts a new axis that will appear at the axis position\n",
    "                dudx = np.column_stack((dudx, dudx[:,-1]))  #duplicate the last column of dudx to match original grid shape (and shape of dvdy)\n",
    "    \n",
    "                delta_lats = 0.25\n",
    "                delta_lats_meters = 110.5744 * 1000 * delta_lats  #distance between latitude lines everywhere\n",
    "                dvdy = (v[:,:-1,:].values - v[:,1:,:].values).squeeze() / delta_lats_meters  #squeeze() removes dimensions of size 1 from an array\n",
    "                dvdy = np.vstack((dvdy, dvdy[-1,:]))  #duplicate the last row of dvdy to match original grid shape (and shape of dudx)\n",
    "    \n",
    "                conv_old = (dudx + dvdy) * -1 * 10**5  #manually calculated convergence from ERA5 u and v winds (times 10**5 1/s)\n",
    "                ds_conv = xr.Dataset(data_vars = dict(convergence = ([\"latitude\", \"longitude\"], conv_old)),\n",
    "                                     coords = dict(latitude = (\"latitude\", u.latitude.values), \n",
    "                                                   longitude = (\"longitude\", u.longitude.values)),\n",
    "                                     attrs = dict(description = \"Manually calculated ERA5 convergence data\"))\n",
    "                \n",
    "                conv = ds_conv.convergence.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "            \n",
    "                # #using convergence variable from ERA5\n",
    "                # conv = ds_era5.d.sel(time = case_date).sel(level = pres_lev) * -1    #convergence of the wind (1/s)\n",
    "                # conv = conv.sel(time = conv.time.dt.hour.isin(hr)) * 10**5  #convergence of the wind (times 10**5 1/s)\n",
    "                # conv = conv.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "                #filter the convergence data by the 700-hPa v-component of the wind  \n",
    "                    #v <= 0: the grid point is ahead of the AEW center \n",
    "                    #v > 0: the grid point is behind the AEW center\n",
    "                        #This dynamically defines ahead/behind AEW centers, which is especially practical for asymmetric AEWs!\n",
    "                if sector == 'full':\n",
    "                    conv = conv * 1\n",
    "                    clc = 'Full'\n",
    "                elif sector == 'south_ahead':\n",
    "                    conv = conv.where(conv.latitude < matched_aew_smoothed_lat).where(v700 <= 0)  #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Ahead (South)'\n",
    "                elif sector == 'ahead':\n",
    "                    conv = conv.where(v700 <= 0)  #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Ahead'\n",
    "                elif sector == 'north_ahead':\n",
    "                    conv = conv.where(conv.latitude >= matched_aew_smoothed_lat).where(v700 <= 0)  #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Ahead (North)'\n",
    "                elif sector == 'south_behind':\n",
    "                    conv = conv.where(conv.latitude < matched_aew_smoothed_lat).where(v700 > 0)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Behind (South)'\n",
    "                elif sector == 'behind':\n",
    "                    conv = conv.where(v700 > 0)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Behind'\n",
    "                elif sector == 'north_behind':\n",
    "                    conv = conv.where(conv.latitude >= matched_aew_smoothed_lat).where(v700 > 0)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Behind (North)'\n",
    "                elif sector == 'north':\n",
    "                    conv = conv.where(conv.latitude >= matched_aew_smoothed_lat)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'North'\n",
    "    \n",
    "                lon, lat = np.meshgrid(conv.longitude, conv.latitude)\n",
    "                lats = lat.reshape(-1)\n",
    "                conv_values = conv.values.reshape(-1)\n",
    "    \n",
    "                #add data from each hour as COLUMNS to corresponding df\n",
    "                conv_df = pd.concat((conv_df, pd.Series(conv_values)), axis = 1, ignore_index = True)\n",
    "                conv_lats_df = pd.concat((conv_lats_df, pd.Series(lats)), axis = 1, ignore_index = True)\n",
    "                #conv_lons_df = pd.concat((conv_lons_df, pd.Series(lons)), axis = 1, ignore_index = True)         \n",
    "                    \n",
    "            if case_num == list(case_dict_conv_aew.keys())[0]:\n",
    "                color = 'darkred'\n",
    "                text_denom = 0\n",
    "            elif case_num == list(case_dict_conv_aew.keys())[1]:\n",
    "                color = 'navy'\n",
    "                text_denom = 0.2\n",
    "    \n",
    "            conv_df = conv_df.values  #convert Pandas DataFrame to NumPy array\n",
    "            conv_lats_df = conv_lats_df.values  #convert Pandas DataFrame to NumPy array\n",
    "    \n",
    "            #mask NaN values in the Dataframes so that the numpy stat calculations work below\n",
    "            conv_df_masked = np.ma.masked_where(np.isnan(conv_df), conv_df)\n",
    "            conv_lats_df_masked = np.ma.masked_where(np.isnan(conv_lats_df), conv_lats_df)\n",
    "    \n",
    "            #line plot histogram (clearer to interpret than \"step\" histogram below)\n",
    "            hist, bins = np.histogram(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None)\n",
    "            bin_centers = (bins[:-1] + bins[1:]) / 2  # Midpoints of the bins\n",
    "            ax.plot(bin_centers, hist, linewidth = 2, linestyle = '-', color = color, label = f'Case {case_num}')\n",
    "    \n",
    "            # #normal \"step\" histogram\n",
    "            # ax.hist(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None,\n",
    "            #         histtype = 'step', align = 'mid', orientation = 'vertical', color = color,\n",
    "            #         linewidth = 2, label = f'Case {case_num}')\n",
    "            #     #density = True returns a probability density: each bin will display the bin's raw count \n",
    "            #         #divided by the total number of counts times the bin width\n",
    "            #         #(density = counts / (sum(counts) * np.diff(bins))), so that the area under the \n",
    "            #         #histogram integrates to 1 (np.sum(density * np.diff(bins)) == 1)\n",
    "    \n",
    "            cos_weights = np.sqrt(np.cos(conv_lats_df_masked * np.pi/180))   #cosine weights to apply to conv_df_masked\n",
    "    \n",
    "            conv_count = np.count_nonzero(~np.isnan(conv_df))\n",
    "            conv_median = np.round(np.nanmedian(conv_df, axis = None), 2)\n",
    "            conv_mean = np.round(np.nanmean(conv_df, axis = None), 2)                                        #non-weighted mean (1st moment)\n",
    "            conv_wgt_mean = np.round(np.average(conv_df_masked, axis = None, weights = cos_weights), 2)      #cosine-weighted mean (1st moment)\n",
    "            conv_std = np.round(np.std(conv_df_masked, axis = None), 2)                                      #standard deviation (2nd moment)\n",
    "            conv_skew = np.round(scipy.stats.skew(conv_df_masked, axis = None, nan_policy = 'omit'), 4)      #skewness (3rd moment)\n",
    "            conv_kurt = np.round(scipy.stats.kurtosis(conv_df_masked, axis = None, nan_policy = 'omit'), 4)  #kurtosis (4th moment)\n",
    "    \n",
    "            # ax.text(0.98, 0.89 - text_denom, \n",
    "            #         f'Count: {conv_count}\\nMedian: {conv_median}\\nMean: {conv_mean}\\nWeighted Mean: {conv_wgt_mean}\\nStandard Deviation: {conv_std}\\nSkewness: {conv_skew}\\nKurtosis: {conv_kurt}\\n', \n",
    "            #         transform = ax.transAxes, horizontalalignment = 'right', verticalalignment = 'center', \n",
    "            #         fontsize = 16, fontweight = 'bold', color = color)\n",
    "        \n",
    "            if case_num == 20:\n",
    "                ax.text(0.5, 0.5, f'Case {case_num} not being correctly\\nmatched to the appropriate AEW', \n",
    "                transform = ax.transAxes, horizontalalignment = 'center', verticalalignment = 'center', \n",
    "                fontsize = 30, bbox = {'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "\n",
    "        ax.set_title('a) ERA5 %i hPa Convergence PDFs (AEW-Relative, %s)' % (pres_lev, clc))\n",
    "        ax.axvline(x = 0, color = 'k', linestyle = '--', alpha = 0.5)\n",
    "        ax.set_xlabel('Convergence [10$^{-5}$ s$^{-1}$]')\n",
    "        ax.set_ylabel('Prob(Convergence)')\n",
    "        ax.set_xlim([-11,11])\n",
    "        ax.set_xticks(np.arange(-11, 11.1, 2))\n",
    "        ax.set_ylim(bottom = 0)\n",
    "        #ax.set_yticks(np.arange(0, 0.601, 0.05))\n",
    "        ax.grid(axis = 'y')\n",
    "        ax.legend(loc = 'upper left')\n",
    "                        \n",
    "        #plt.tight_layout()\n",
    "        #plt.subplots_adjust(wspace = 0.1)\n",
    "\n",
    "        #save the figure\n",
    "        plot_save_name = f'Case{list(case_dict_conv_aew.keys())[0]}-{list(case_dict_conv_aew.keys())[1]}_{pres_lev}hPa_convergence_PDFs_AEW_relative_{sector}.png'\n",
    "        plt.savefig(os.path.join('/Users/ben/Desktop', plot_save_name), bbox_inches = 'tight')\n",
    "        #plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "        #plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "        plt.close()\n",
    "        plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "        ##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "        ##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "        im = Image.open(os.path.join('/Users/ben/Desktop', plot_save_name))\n",
    "\n",
    "        try:\n",
    "            im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "        except:\n",
    "            #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "            #though this line will have isolated, extremely minor image effects due to \n",
    "            #only using 256 colors instead of the 3-element RGB scale\n",
    "            im2 = im.convert('P')\n",
    "    \n",
    "        im2.save(os.path.join('/Users/ben/Desktop', plot_save_name))\n",
    "        im.close()\n",
    "        im2.close()\n",
    "        \n",
    "        #print (f'Case {case_num} convergence PDF plots complete!\\n')\n",
    "    \n",
    "ds_era5.close()\n",
    "ds_aew.close()\n",
    "\n",
    "#tend = time.time()\n",
    "#print (f'This script took {np.round((tend - tstart) / 60, 1)} minutes to complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07502bc-74b7-46d9-9c4f-56a52059ebf5",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figure 13 and 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7b7d3-e81b-4801-8c78-ef4cf4311e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss regional variability in this section w/regard to CPEX and CPEX-AW\n",
    "#     (create figure similar to Figure 18b in your thesis comparing bulk metric spreads for each field campaign)\n",
    "# adapted from metric_boxwhisker_4panel_CPEXCV.py (see \"For CPEX-CV AGU Paper (Figures 4, 6, and 8)\" above) and metric_boxwhisker_4panel.py \n",
    "\n",
    "# matplotlib.rcParams['axes.labelsize'] = 14\n",
    "# matplotlib.rcParams['axes.titlesize'] = 14\n",
    "# matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "# matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "# matplotlib.rcParams['legend.fontsize'] = 12\n",
    "# matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "\n",
    "TC_days = [20170619, 20170620, 20210826, 20210828, 20210901, 20210904, 20220923]  #including TDs as TCs (from both CPEX and CPEX-AW and CPEX-CV)\n",
    "\n",
    "###CPEX-CV files and dataframes\n",
    "drop_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "#drop_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations.csv')\n",
    "df = pd.read_csv(drop_filepath)\n",
    "df_noTC = df[~df['Date'].isin(TC_days)].copy()  #filter out cases associated with a TD/TC\n",
    "\n",
    "dawn_filepath = os.path.join(os.getcwd(), 'DAWN_Shear_Calculations_CPEXCV.csv')\n",
    "#dawn_filepath = os.path.join(os.getcwd(), 'DAWN_Shear_Calculations.csv')\n",
    "df_dawn = pd.read_csv(dawn_filepath)\n",
    "df1 = pd.concat([df, df_dawn], ignore_index = True)  #concatenates fields with same heading\n",
    "df1_noTC = df1[~df1['Date'].isin(TC_days)].copy()  #filter out cases associated with a TD/TC\n",
    "\n",
    "###CPEX(-AW) files and dataframes (W. Atlantic --> _watl)\n",
    "drop_filepath_watl = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations.csv')\n",
    "#drop_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "df_watl = pd.read_csv(drop_filepath_watl)\n",
    "df_noTC_watl = df_watl[~df_watl['Date'].isin(TC_days)].copy()  #filter out cases associated with a TD/TC\n",
    "\n",
    "dawn_filepath_watl = os.path.join(os.getcwd(), 'DAWN_Shear_Calculations.csv')\n",
    "#dawn_filepath = os.path.join(os.getcwd(), 'DAWN_Shear_Calculations_CPEXCV.csv')\n",
    "df_dawn_watl = pd.read_csv(dawn_filepath_watl)\n",
    "df1_watl = pd.concat([df_watl, df_dawn_watl], ignore_index = True)  #concatenates fields with same heading\n",
    "df1_noTC_watl = df1_watl[~df1_watl['Date'].isin(TC_days)].copy()  #filter out cases associated with a TD/TC\n",
    "\n",
    "\n",
    "rh_layers = ['Deep Layer RH [%]', 'PBL RH [%]', 'Mid Layer RH [%]', 'Upper Layer RH [%]']\n",
    "# speed_shear_layers = ['SHARPpy Direct Method Deep Layer Speed Shear [kts]', 'SHARPpy Direct Method PBL Speed Shear [kts]', \n",
    "#                       'SHARPpy Direct Method Mid Layer Speed Shear [kts]', 'SHARPpy Direct Method Upper Layer Speed Shear [kts]']\n",
    "speed_shear_layers = ['500m Bottom Cap Deep Layer Speed Shear [kts]', 'SHARPpy Direct Method PBL Speed Shear [kts]', \n",
    "                      'SHARPpy Direct Method Mid Layer Speed Shear [kts]', 'SHARPpy Direct Method Upper Layer Speed Shear [kts]']\n",
    "cape_layers = ['Deep Layer MUCAPE [J/kg]', 'Deep Layer MLCAPE [J/kg]', 'Above FZL MUCAPE [J/kg]', 'Above FZL MLCAPE [J/kg]']\n",
    "\n",
    "use_alpha = 0.25  #alpha for box plots with all sondes, not just inflow sondes\n",
    "\n",
    "#for convective type comparison of All Lifecycles dropsondes\n",
    "def box_plot_onlyIsoOrg_RH(plotting_metric_name, ax):  #parameter will be a data column from df\n",
    "    \n",
    "    if plotting_metric_name == 'Upper Layer RH [%]':  #include \"In Precip\" sondes if \"Partially In Precip\" == Yes\n",
    "        df_use0 = df_noTC[df_noTC['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "        df_use1 = df_use0[(df_use0['Environment Falling In'] != 'In Precip') | (df_use0['Partially In Precip'] == 'Yes')].copy()  #filter out In Precip, unless \"Partially In Precip\" == Yes\n",
    "        df_use = df_use1[(df_use1['Low-level Inflow Sonde'] == 'Yes') | (df_use1['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "        #df_use = df_noTC[(df_noTC['Environment Falling In'] != 'In Precip') & (df_noTC['Environment Falling In'] != 'Clear Far')].copy()  #filter out In Precip and Clear Far dropsondes\n",
    "    else:\n",
    "        df_use1 = df_noTC[(df_noTC['Environment Falling In'] != 'In Precip') & (df_noTC['Environment Falling In'] != 'Clear Far')].copy()  #filter out In Precip and Clear Far dropsondes\n",
    "        df_use = df_use1[(df_use1['Low-level Inflow Sonde'] == 'Yes') | (df_use1['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "\n",
    "    #all sondes\n",
    "    df_iso_all = df_use1[df_use1['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all = df_use1[df_use1['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all = df_use1[df_use1['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso = df_use[df_use['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org = df_use[df_use['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat = df_use[df_use['Primary Convective Type'] == 'Scattered'].copy()\n",
    "\n",
    "######################\n",
    "    if plotting_metric_name == 'Upper Layer RH [%]':  #include \"In Precip\" sondes if \"Partially In Precip\" == Yes\n",
    "        df_use0_watl = df_noTC_watl[df_noTC_watl['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "        df_use1_watl = df_use0_watl[(df_use0_watl['Environment Falling In'] != 'In Precip') | (df_use0_watl['Partially In Precip'] == 'Yes')].copy()  #filter out In Precip, unless \"Partially In Precip\" == Yes\n",
    "        df_use_watl = df_use1_watl[(df_use1_watl['Low-level Inflow Sonde'] == 'Yes') | (df_use1_watl['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "        #df_use_watl = df_noTC_watl[(df_noTC_watl['Environment Falling In'] != 'In Precip') & (df_noTC_watl['Environment Falling In'] != 'Clear Far')].copy()  #filter out In Precip and Clear Far dropsondes\n",
    "    else:\n",
    "        df_use1_watl = df_noTC_watl[(df_noTC_watl['Environment Falling In'] != 'In Precip') & (df_noTC_watl['Environment Falling In'] != 'Clear Far')].copy()  #filter out In Precip and Clear Far dropsondes\n",
    "        df_use_watl = df_use1_watl[(df_use1_watl['Low-level Inflow Sonde'] == 'Yes') | (df_use1_watl['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "\n",
    "    #all sondes\n",
    "    df_iso_all_watl = df_use1_watl[df_use1_watl['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all_watl = df_use1_watl[df_use1_watl['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all_watl = df_use1_watl[df_use1_watl['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso_watl = df_use_watl[df_use_watl['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_watl = df_use_watl[df_use_watl['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_watl = df_use_watl[df_use_watl['Primary Convective Type'] == 'Scattered'].copy()\n",
    "    \n",
    "    #need to filter out NaN values (using dropna()), otherwise the boxplot() won't create anything\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_scat[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                 patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated', 'Organized', 'Scattered'])\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_scat[plotting_metric_name].dropna().values, df_scat_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                  patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized', 'Scattered\\n(Inflow)', 'Scattered'])\n",
    "    bp = ax.boxplot([df_iso_watl[plotting_metric_name].dropna().values, df_iso[plotting_metric_name].dropna().values, df_iso_all_watl[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "                     df_org_watl[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_org_all_watl[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "                     patch_artist = True, vert = True, widths = 0.65, tick_labels = ['Isolated\\n(Inflow)\\n(CPEX(-AW))', 'Isolated\\n(Inflow)\\n(CPEX-CV)', 'Isolated\\n(CPEX(-AW))', 'Isolated\\n(CPEX-CV)', 'Organized\\n(Inflow)\\n(CPEX(-AW))', 'Organized\\n(Inflow)\\n(CPEX-CV)', 'Organized\\n(CPEX(-AW))', 'Organized\\n(CPEX-CV)'])\n",
    "    \n",
    "    print (f'Isolated {plotting_metric_name} median (CPEX(-AW)):', df_iso_watl[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median (CPEX(-AW)):', df_org_watl[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median (CPEX(-AW)):', df_scat_watl[plotting_metric_name].median(skipna = True))\n",
    "    \n",
    "    print (f'Isolated {plotting_metric_name} median (CPEX-CV):', df_iso[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median (CPEX-CV):', df_org[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median (CPEX-CV):', df_scat[plotting_metric_name].median(skipna = True))\n",
    "\n",
    "    colors = ['red', 'red', 'red', 'red', 'blue', 'blue', 'blue', 'blue']\n",
    "    nums = list(range(len(colors)))\n",
    "     \n",
    "    for ii, patch, color in zip(nums, bp['boxes'], colors):\n",
    "        if ii in [0,1,4,5]:\n",
    "            patch.set_facecolor(color)\n",
    "        else:\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(use_alpha)\n",
    "\n",
    "    #changing color and linewidth of medians\n",
    "    for ii, median in enumerate(bp['medians']):\n",
    "        if ii in [0,1,4,5]:\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "        else:\n",
    "            #median.set(color = 'white', alpha = use_alpha, linewidth = 3)\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "\n",
    "    #changing color and linewidth of whiskers\n",
    "    for ii, whisker in enumerate(bp['whiskers']):\n",
    "        if ii in [0, 1, 2, 3, 8, 9, 10, 11]:  #16 whiskers, not 8\n",
    "            whisker.set(color = 'k', linewidth = 2, linestyle = \"-\")\n",
    "        else:\n",
    "            whisker.set(color = 'k', alpha = use_alpha, linewidth = 2, linestyle = \"-\")\n",
    "     \n",
    "    #changing color and linewidth of caps\n",
    "    for ii, cap in enumerate(bp['caps']):\n",
    "        if ii in [0, 1, 2, 3, 8, 9, 10, 11]:  #16 caps, not 8\n",
    "            cap.set(color = 'k', linewidth = 2)\n",
    "        else:\n",
    "            cap.set(color = 'k', alpha = use_alpha, linewidth = 2)\n",
    "     \n",
    "    #changing style of fliers\n",
    "    for ii, flier, color in zip(nums, bp['fliers'], colors):\n",
    "        if ii in [0,1,4,5]:\n",
    "            flier.set(marker = 'o', color = 'k', markersize = 13, markerfacecolor = color)\n",
    "        else:\n",
    "            flier.set(marker = 'o', color = 'k', alpha = use_alpha, markersize = 13, markerfacecolor = color)\n",
    "            \n",
    "    ax.grid(True, axis = 'y')\n",
    "    ax.set_title(plotting_metric_name[:-4], fontsize = 33, fontweight = 'bold')\n",
    "    #ax.set_xlabel('Convective Type', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[%]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([15,100])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    " \n",
    "    \n",
    "group_fig = plt.figure(figsize=(48,24))\n",
    "#group_fig = plt.figure(figsize=(22,24))\n",
    "\n",
    "for i, layer in enumerate(rh_layers):\n",
    "    ax = group_fig.add_subplot(2,2,i+1)\n",
    "    box_plot_onlyIsoOrg_RH(layer, ax)\n",
    "\n",
    "#custom legend\n",
    "# legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 's', markersize = 13, label='Isolated (In Precip Profiles Excluded)'),\n",
    "#                    Line2D([], [], color='blue', linewidth = 0, marker = 's', markersize = 13, label='Organized (In Precip Profiles Excluded)')]\n",
    "# group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 25)\n",
    "\n",
    "group_fig.text(0.5, 0.473, 'In Precip Profiles Excluded', horizontalalignment='center', verticalalignment='center', \n",
    "               fontsize = 40, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "plt.subplots_adjust(hspace = 0.45, wspace = 0.15)\n",
    "plt.savefig('/Users/ben/Desktop/RH_box_4panel_allCPEX.png', bbox_inches = 'tight')\n",
    "plt.close()\n",
    "print ('')\n",
    "\n",
    "\n",
    "#for convective type comparison of All Lifecycles dropsondes\n",
    "def box_plot_onlyIsoOrg_SS(plotting_metric_name, ax):  #parameter will be a data column from df\n",
    "\n",
    "    if 'Deep Layer Speed Shear' in plotting_metric_name:  #include DAWN speed shear in the deep layer speed shear plot\n",
    "        df_use1 = df1_noTC[df1_noTC['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "        df_use = df_use1[(df_use1['Low-level Inflow Sonde'] != 'No') | (df_use1['Mid-level Inflow Sonde'] != 'No')].copy()\n",
    "    \n",
    "        ###############\n",
    "        df_use1_watl = df1_noTC_watl[df1_noTC_watl['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "        df_use_watl = df_use1_watl[(df_use1_watl['Low-level Inflow Sonde'] != 'No') | (df_use1_watl['Mid-level Inflow Sonde'] != 'No')].copy()\n",
    "            \n",
    "        ax.set_title('Deep Layer Shear (Dropsonde & DAWN, 0.5km - 7.6km)', fontsize = 30, fontweight = 'bold')\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        df_use1 = df_noTC[df_noTC['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "        df_use = df_use1[(df_use1['Low-level Inflow Sonde'] == 'Yes') | (df_use1['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "\n",
    "        ######################\n",
    "        \n",
    "        df_use1_watl = df_noTC_watl[df_noTC_watl['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "        df_use_watl = df_use1_watl[(df_use1_watl['Low-level Inflow Sonde'] == 'Yes') | (df_use1_watl['Mid-level Inflow Sonde'] == 'Yes')].copy()\n",
    "\n",
    "        ax.set_title(plotting_metric_name[22:-18] + ' Shear', fontsize = 33, fontweight = 'bold')\n",
    "        \n",
    "    #all sondes\n",
    "    df_iso_all = df_use1[df_use1['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all = df_use1[df_use1['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all = df_use1[df_use1['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso = df_use[df_use['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org = df_use[df_use['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat = df_use[df_use['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #all sondes\n",
    "    df_iso_all_watl = df_use1_watl[df_use1_watl['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all_watl = df_use1_watl[df_use1_watl['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all_watl = df_use1_watl[df_use1_watl['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso_watl = df_use_watl[df_use_watl['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_watl = df_use_watl[df_use_watl['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_watl = df_use_watl[df_use_watl['Primary Convective Type'] == 'Scattered'].copy()\n",
    "    \n",
    "    #need to filter out NaN values (using dropna()), otherwise the boxplot() won't create anything\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_scat[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                 patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated', 'Organized', 'Scattered'])\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_scat[plotting_metric_name].dropna().values, df_scat_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                  patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized', 'Scattered\\n(Inflow)', 'Scattered'])\n",
    "    bp = ax.boxplot([df_iso_watl[plotting_metric_name].dropna().values, df_iso[plotting_metric_name].dropna().values, df_iso_all_watl[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "                     df_org_watl[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_org_all_watl[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "                     patch_artist = True, vert = True, widths = 0.65, tick_labels = ['Isolated\\n(Inflow)\\n(CPEX(-AW))', 'Isolated\\n(Inflow)\\n(CPEX-CV)', 'Isolated\\n(CPEX(-AW))', 'Isolated\\n(CPEX-CV)', 'Organized\\n(Inflow)\\n(CPEX(-AW))', 'Organized\\n(Inflow)\\n(CPEX-CV)', 'Organized\\n(CPEX(-AW))', 'Organized\\n(CPEX-CV)'])\n",
    "    \n",
    "    print (f'Isolated {plotting_metric_name} median (CPEX(-AW)):', df_iso_watl[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median (CPEX(-AW)):', df_org_watl[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median (CPEX(-AW)):', df_scat_watl[plotting_metric_name].median(skipna = True))\n",
    "    \n",
    "    print (f'Isolated {plotting_metric_name} median (CPEX-CV):', df_iso[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median (CPEX-CV):', df_org[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median (CPEX-CV):', df_scat[plotting_metric_name].median(skipna = True))\n",
    "\n",
    "    colors = ['red', 'red', 'red', 'red', 'blue', 'blue', 'blue', 'blue']\n",
    "    nums = list(range(len(colors)))\n",
    "     \n",
    "    for ii, patch, color in zip(nums, bp['boxes'], colors):\n",
    "        if ii in [0,1,4,5]:\n",
    "            patch.set_facecolor(color)\n",
    "        else:\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(use_alpha)\n",
    "\n",
    "    #changing color and linewidth of medians\n",
    "    for ii, median in enumerate(bp['medians']):\n",
    "        if ii in [0,1,4,5]:\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "        else:\n",
    "            #median.set(color = 'white', alpha = use_alpha, linewidth = 3)\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "\n",
    "    #changing color and linewidth of whiskers\n",
    "    for ii, whisker in enumerate(bp['whiskers']):\n",
    "        if ii in [0, 1, 2, 3, 8, 9, 10, 11]:  #16 whiskers, not 8\n",
    "            whisker.set(color = 'k', linewidth = 2, linestyle = \"-\")\n",
    "        else:\n",
    "            whisker.set(color = 'k', alpha = use_alpha, linewidth = 2, linestyle = \"-\")\n",
    "     \n",
    "    #changing color and linewidth of caps\n",
    "    for ii, cap in enumerate(bp['caps']):\n",
    "        if ii in [0, 1, 2, 3, 8, 9, 10, 11]:  #16 caps, not 8\n",
    "            cap.set(color = 'k', linewidth = 2)\n",
    "        else:\n",
    "            cap.set(color = 'k', alpha = use_alpha, linewidth = 2)\n",
    "     \n",
    "    #changing style of fliers\n",
    "    for ii, flier, color in zip(nums, bp['fliers'], colors):\n",
    "        if ii in [0,1,4,5]:\n",
    "            flier.set(marker = 'o', color = 'k', markersize = 13, markerfacecolor = color)\n",
    "        else:\n",
    "            flier.set(marker = 'o', color = 'k', alpha = use_alpha, markersize = 13, markerfacecolor = color)\n",
    "\n",
    "    ax.grid(True, axis = 'y')\n",
    "    #ax.set_xlabel('Convective Type', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[kts]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([0,50])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    \n",
    " \n",
    "group_fig = plt.figure(figsize=(48,24))\n",
    "#group_fig = plt.figure(figsize=(22,24))\n",
    "\n",
    "for i, layer in enumerate(speed_shear_layers):\n",
    "    ax = group_fig.add_subplot(2,2,i+1)\n",
    "    box_plot_onlyIsoOrg_SS(layer, ax)\n",
    "\n",
    "#custom legend\n",
    "# legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 's', markersize = 13, label='Isolated'),\n",
    "#                    Line2D([], [], color='blue', linewidth = 0, marker = 's', markersize = 13, label='Organized')]\n",
    "# group_fig.legend(handles = legend_elements, loc = 'center', fontsize = 25)\n",
    "\n",
    "group_fig.text(0.5, 0.473, 'In Precip Profiles Included', horizontalalignment='center', verticalalignment='center', \n",
    "               fontsize = 40, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "plt.subplots_adjust(hspace = 0.45, wspace = 0.15)\n",
    "plt.savefig('/Users/ben/Desktop/SS_box_4panel_allCPEX.png', bbox_inches = 'tight')\n",
    "plt.close()\n",
    "print ('')\n",
    "\n",
    "#for convective type comparison of All Lifecycles dropsondes\n",
    "def box_plot_onlyIsoOrg_SSwDAWN(plotting_metric_name):  #parameter will be a data column from df1\n",
    "\n",
    "    group_fig = plt.figure(figsize=(48,24))\n",
    "    #group_fig = plt.figure(figsize=(32,21))\n",
    "    ax = group_fig.add_subplot(1,2,1)\n",
    "    \n",
    "    df_use1 = df1_noTC[df1_noTC['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "    df_use = df_use1[(df_use1['Low-level Inflow Sonde'] != 'No') | (df_use1['Mid-level Inflow Sonde'] != 'No')].copy()\n",
    "    \n",
    "    #all sondes\n",
    "    df_iso_all = df_use1[df_use1['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all = df_use1[df_use1['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all = df_use1[df_use1['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso = df_use[df_use['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org = df_use[df_use['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat = df_use[df_use['Primary Convective Type'] == 'Scattered'].copy()\n",
    "\n",
    "###############\n",
    "    df_use1_watl = df1_noTC_watl[df1_noTC_watl['Environment Falling In'] != 'Clear Far'].copy()  #filter out Clear Far dropsondes\n",
    "    df_use_watl = df_use1_watl[(df_use1_watl['Low-level Inflow Sonde'] != 'No') | (df_use1_watl['Mid-level Inflow Sonde'] != 'No')].copy()\n",
    "    \n",
    "    #all sondes\n",
    "    df_iso_all_watl = df_use1_watl[df_use1_watl['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_all_watl = df_use1_watl[df_use1_watl['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_all_watl = df_use1_watl[df_use1_watl['Primary Convective Type'] == 'Scattered'].copy()\n",
    "        \n",
    "    #just inflow sondes\n",
    "    df_iso_watl = df_use_watl[df_use_watl['Primary Convective Type'] == 'Isolated'].copy()\n",
    "    df_org_watl = df_use_watl[df_use_watl['Primary Convective Type'] == 'Organized'].copy()\n",
    "    #df_scat_watl = df_use_watl[df_use_watl['Primary Convective Type'] == 'Scattered'].copy()\n",
    "    \n",
    "    #need to filter out NaN values (using dropna()), otherwise the boxplot() won't create anything\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_scat[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                 patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated', 'Organized', 'Scattered'])\n",
    "    # bp = ax.boxplot([df_iso[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_org[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values, \n",
    "    #                  df_scat[plotting_metric_name].dropna().values, df_scat_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "    #                  patch_artist = True, vert = True, widths = 0.65, labels = ['Isolated\\n(Inflow)', 'Isolated', 'Organized\\n(Inflow)', 'Organized', 'Scattered\\n(Inflow)', 'Scattered'])\n",
    "    bp = ax.boxplot([df_iso_watl[plotting_metric_name].dropna().values, df_iso[plotting_metric_name].dropna().values, df_iso_all_watl[plotting_metric_name].dropna().values, df_iso_all[plotting_metric_name].dropna().values, \n",
    "                     df_org_watl[plotting_metric_name].dropna().values, df_org[plotting_metric_name].dropna().values, df_org_all_watl[plotting_metric_name].dropna().values, df_org_all[plotting_metric_name].dropna().values], notch = True, bootstrap = 10000,\n",
    "                     patch_artist = True, vert = True, widths = 0.65, tick_labels = ['Isolated\\n(Inflow)\\n(CPEX(-AW))', 'Isolated\\n(Inflow)\\n(CPEX-CV)', 'Isolated\\n(CPEX(-AW))', 'Isolated\\n(CPEX-CV)', 'Organized\\n(Inflow)\\n(CPEX(-AW))', 'Organized\\n(Inflow)\\n(CPEX-CV)', 'Organized\\n(CPEX(-AW))', 'Organized\\n(CPEX-CV)'])\n",
    "        \n",
    "    print (f'Isolated {plotting_metric_name} median (CPEX(-AW)):', df_iso_watl[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median (CPEX(-AW)):', df_org_watl[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median (CPEX(-AW)):', df_scat_watl[plotting_metric_name].median(skipna = True))\n",
    "    \n",
    "    print (f'Isolated {plotting_metric_name} median (CPEX-CV):', df_iso[plotting_metric_name].median(skipna = True))\n",
    "    print (f'Organized {plotting_metric_name} median (CPEX-CV):', df_org[plotting_metric_name].median(skipna = True))\n",
    "    #print (f'Scattered {plotting_metric_name} median (CPEX-CV):', df_scat[plotting_metric_name].median(skipna = True))\n",
    "\n",
    "    colors = ['red', 'red', 'red', 'red', 'blue', 'blue', 'blue', 'blue']\n",
    "    nums = list(range(len(colors)))\n",
    "     \n",
    "    for ii, patch, color in zip(nums, bp['boxes'], colors):\n",
    "        if ii in [0,1,4,5]:\n",
    "            patch.set_facecolor(color)\n",
    "        else:\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(use_alpha)\n",
    "\n",
    "    #changing color and linewidth of medians\n",
    "    for ii, median in enumerate(bp['medians']):\n",
    "        if ii in [0,1,4,5]:\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "        else:\n",
    "            #median.set(color = 'white', alpha = use_alpha, linewidth = 3)\n",
    "            median.set(color = 'white', linewidth = 3)\n",
    "\n",
    "    #changing color and linewidth of whiskers\n",
    "    for ii, whisker in enumerate(bp['whiskers']):\n",
    "        if ii in [0, 1, 2, 3, 8, 9, 10, 11]:  #16 whiskers, not 8\n",
    "            whisker.set(color = 'k', linewidth = 2, linestyle = \"-\")\n",
    "        else:\n",
    "            whisker.set(color = 'k', alpha = use_alpha, linewidth = 2, linestyle = \"-\")\n",
    "     \n",
    "    #changing color and linewidth of caps\n",
    "    for ii, cap in enumerate(bp['caps']):\n",
    "        if ii in [0, 1, 2, 3, 8, 9, 10, 11]:  #16 caps, not 8\n",
    "            cap.set(color = 'k', linewidth = 2)\n",
    "        else:\n",
    "            cap.set(color = 'k', alpha = use_alpha, linewidth = 2)\n",
    "     \n",
    "    #changing style of fliers\n",
    "    for ii, flier, color in zip(nums, bp['fliers'], colors):\n",
    "        if ii in [0,1,4,5]:\n",
    "            flier.set(marker = 'o', color = 'k', markersize = 13, markerfacecolor = color)\n",
    "        else:\n",
    "            flier.set(marker = 'o', color = 'k', alpha = use_alpha, markersize = 13, markerfacecolor = color)\n",
    "        \n",
    "    ax.grid(True, axis = 'y')\n",
    "    ax.set_title('Deep Layer Shear (Dropsonde & DAWN, 0.5km - 7.6km)', fontsize = 30, fontweight = 'bold')\n",
    "    ax.set_xlabel('Convective Type', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[kts]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([0,50])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    ax.text(0.165, 0.97, 'In Precip Profiles Included', horizontalalignment='center', verticalalignment='center', \n",
    "            transform=ax.transAxes, fontsize = 30, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 5})\n",
    "    \n",
    "    #plot the DAWN/dropsonde speed shear scatter plot\n",
    "    ax = group_fig.add_subplot(1,2,2)\n",
    "    \n",
    "    plotting_metric = df1[plotting_metric_name]   #both dropsonde and DAWN data\n",
    "    xlist = []\n",
    "    for i in range(len(df1)):\n",
    "        xstring = str(df1['Case'][i])\n",
    "        xlist.append(xstring)\n",
    "    \n",
    "    #custom legend\n",
    "    # legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 15, label='Isolated'),\n",
    "    #                     Line2D([], [], color='red', linewidth = 0, marker = '$P$', markersize = 15, label='Isolated (In Precip)'),\n",
    "    #                     Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 15, label='Organized'),\n",
    "    #                     Line2D([], [], color='blue', linewidth = 0, marker = '$P$', markersize = 15, label='Organized (In Precip)'),\n",
    "    #                     Line2D([], [], color='black', linewidth = 0, marker = 'o', markersize = 15, label='Scattered'),\n",
    "    #                     Line2D([], [], color='black', linewidth = 0, marker = '$P$', markersize = 15, label='Scattered (In Precip)')]\n",
    "    legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = 'o', markersize = 13, label='Isolated (Clear)'),\n",
    "                       Line2D([], [], color='red', linewidth = 0, marker = '$C$', markersize = 13, label='Isolated (In Cloud)'),\n",
    "                       Line2D([], [], color='red', linewidth = 0, marker = '$P$', markersize = 13, label='Isolated (In Precip)'),\n",
    "                       Line2D([], [], color='blue', linewidth = 0, marker = 'o', markersize = 13, label='Organized (Clear)'),\n",
    "                       Line2D([], [], color='blue', linewidth = 0, marker = '$C$', markersize = 13, label='Organized (In Cloud)'),\n",
    "                       Line2D([], [], color='blue', linewidth = 0, marker = '$P$', markersize = 13, label='Organized (In Precip)')]\n",
    "    \n",
    "    \n",
    "    #the lighter shades account for TS Cindy organized cases and, further, TS Cindy cases that were away from the main organized convection and instead near the cyclonic center\n",
    "    for j in range(len(plotting_metric)):\n",
    "        #if (df1['Convective Lifecycle'][j] != 'Weakening') and (df1['Environment Falling In'][j] == 'Clear Near' or df1['Environment Falling In'][j] == 'In Cloud' or df1['Environment Falling In'][j] == 'In Precip'):\n",
    "        if df1['Environment Falling In'][j] == 'Clear Near' or df1['Environment Falling In'][j] == 'In Cloud' or df1['Environment Falling In'][j] == 'In Precip':\n",
    "        #if (df1['Environment Falling In'][j] == 'Clear Near' or df1['Environment Falling In'][j] == 'In Cloud' or df1['Environment Falling In'][j] == 'In Precip') and (df1['Low-level Inflow Sonde'][j] != 'No' or df1['Mid-level Inflow Sonde'][j] != 'No'):\n",
    "            if df1['Primary Convective Type'][j] == 'Isolated':\n",
    "                color = 'red'\n",
    "                if df1['Date'][j] in TC_days:\n",
    "                    continue\n",
    "            elif df1['Primary Convective Type'][j] == 'Organized':\n",
    "                color = 'blue'\n",
    "                if df1['Date'][j] in TC_days:\n",
    "                    continue\n",
    "            # elif df1['Primary Convective Type'][j] == 'Scattered':\n",
    "            #     color = 'black'\n",
    "            #     if df1['Date'][j] in TC_days:\n",
    "            #         continue \n",
    "            else:\n",
    "                continue\n",
    "                  \n",
    "            if df1['Environment Falling In'][j] == 'In Precip':\n",
    "                if df1['Partially In Precip'][j] == 'Yes':\n",
    "                    mark = '$*P$'\n",
    "                    mark = '$P$'\n",
    "                else:\n",
    "                    mark = '$P$'\n",
    "                outline = None\n",
    "            elif df1['Environment Falling In'][j] == 'In Cloud':\n",
    "                if df1['Falling Through Weak Stratiform (Onion-ish Profile (Typically On Outer Edge of Storm))'][j] == 'Yes':\n",
    "                    mark = '$*C$'\n",
    "                    mark = '$C$'\n",
    "                else:\n",
    "                    mark = '$C$'\n",
    "                outline = None\n",
    "            else:\n",
    "                mark = 'o'\n",
    "                outline = 'black'\n",
    "\n",
    "            #if the metric is not NaN (need this filter, otherwise Python raises an error when showing/saving the figure)\n",
    "            if not np.isnan(plotting_metric[j]):  #could also use pd.isna(plotting_metric[j]) \n",
    "                if df1['Low-level Inflow Sonde'][j] != 'No' or df1['Mid-level Inflow Sonde'][j] != 'No':               \n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, s = 150, marker = mark, edgecolor = outline)\n",
    "                else:\n",
    "                    ax.scatter(xlist[j], plotting_metric[j], c = color, alpha = use_alpha, s = 150, marker = mark, edgecolor = outline)\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.set_title('Deep Layer Shear (Dropsonde & DAWN, 0.5km - 7.6km)', fontsize = 30, fontweight = 'bold')\n",
    "    ax.set_xlabel('Case', fontsize = 30, fontweight = 'bold', labelpad = 15.0)\n",
    "    ax.set_ylabel('[kts]', fontsize = 30, fontweight = 'bold', labelpad = 20.0)\n",
    "    #ax.set_ylim([0,50])\n",
    "    ax.tick_params(length = 15, width = 5, labelsize = 25)\n",
    "    ax.legend(handles = legend_elements, fontsize = 22, ncol = 1, loc = 'upper left')\n",
    "    ax.text(0.5, 0.5, 'IGNORE: JUST CPEX-CV CASES', horizontalalignment='center', verticalalignment='center', \n",
    "        transform=ax.transAxes, fontsize = 60, bbox={'facecolor': 'white', 'alpha': 0.5, 'pad': 5})\n",
    "\n",
    "box_plot_onlyIsoOrg_SSwDAWN('500m Bottom Cap Deep Layer Speed Shear [kts]')\n",
    "plt.subplots_adjust(wspace = 0.20)\n",
    "plt.savefig('/Users/ben/Desktop/SSwDAWN_box_scatter_allCPEX.png', bbox_inches = 'tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869228a-4499-467e-b691-3bbfd8b27ea5",
   "metadata": {},
   "source": [
    "### For CPEX-CV AGU Paper (Figure 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919a4fc-b8de-46c9-bc2f-f812cc4ef00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from CPEXCV_Metrics_Regression.ipynb\n",
    "#This script plots a joint distribution scatter plot and regression line for two desired CPEX(-AW,-CV) dropsonde environmental metrics.\n",
    "    #The points are color coded by any desired CPEX-CV dropsonde metric.\n",
    "#COLORED BY CONVECTIVE TYPE, LABELED BY FIELD CAMPAIGN, SHADED BY INFLOW (Clear and In Cloud Sondes Only, Inflow Sondes Saturated)\n",
    "\n",
    "#should only need to change the 9 variables in below\n",
    "metric_x = 'SHARPpy Direct Method Upper Layer Speed Shear [kts]'  #metric to be placed on the x-axis; should be the independent metric\n",
    "metric_x_label = 'Upper Layer Shear [kts]'\n",
    "metric_x_title_label = 'Upper Layer Shear'\n",
    "metric_y = 'Upper Layer RH [%]'  #metric to be placed on the y-axis; should be the dependent metric\n",
    "metric_y_label = 'Upper Layer RH [%]'\n",
    "metric_y_title_label = 'Upper Layer RH'\n",
    "metric_colorby = 'Primary Convective Type'  #metric for which to color the scatter plot points by\n",
    "metric_colorby_label = 'Convective Type'\n",
    "vlims = [0,40]  #if metric_colorby is a numerical metric, give the desired numerical range for the colorbar\n",
    "filter_out_precip = True  #recommend filtering out In Precip sondes if one of the desired metrics is moisture- or CAPE-related\n",
    "save_name = '/Users/ben/Desktop/UpperShear_vs_UpperRH_option2.png' #filepath of the location where the plot should be saved\n",
    "\n",
    "#options for metric_x, metric_y, and metric_colorby above:\n",
    "# rh_layers = ['Deep Layer RH [%]', 'PBL RH [%]', 'Mid Layer RH [%]', 'Upper Layer RH [%]']\n",
    "# speed_shear_layers = ['SHARPpy Direct Method Deep Layer Speed Shear [kts]', 'SHARPpy Direct Method PBL Speed Shear [kts]', \n",
    "#                       'SHARPpy Direct Method Mid Layer Speed Shear [kts]', 'SHARPpy Direct Method Upper Layer Speed Shear [kts]']\n",
    "# cape_layers = ['Deep Layer MUCAPE [J/kg]', 'Deep Layer MLCAPE [J/kg]', 'Above FZL MUCAPE [J/kg]', 'Above FZL MLCAPE [J/kg]']\n",
    "\n",
    "\n",
    "#set some baseline plot displays\n",
    "matplotlib.rcParams['axes.labelsize'] = 30\n",
    "matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "matplotlib.rcParams['axes.titlesize'] = 33\n",
    "matplotlib.rcParams['xtick.labelsize'] = 25\n",
    "matplotlib.rcParams['ytick.labelsize'] = 25\n",
    "matplotlib.rcParams['legend.fontsize'] = 25\n",
    "matplotlib.rcParams['legend.title_fontsize'] = 20\n",
    "matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "\n",
    "\n",
    "TC_days = [20170619, 20170620, 20210826, 20210828, 20210901, 20210904, 20220923]  #including TDs as TCs (from both CPEX and CPEX-AW and CPEX-CV)\n",
    "drop_filepath_cpexaw = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations.csv')  #filepath of the dropsonde metric CSV file\n",
    "drop_filepath_cpexcv = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')  #filepath of the dropsonde metric CSV file\n",
    "\n",
    "#read in the dropsonde metric CSV files and filter out dropsondes that don't have both desired metrics and/or are associated with a TD/TC\n",
    "    #also filter out In Precip sondes, if desired (recommended if one of the metrics is moisture- or CAPE-related)\n",
    "\n",
    "#NOTE: if a dropsonde's \"metric_colorby\" metric is blank, then the code will run fine BUT Python will skip plotting that dropsonde point\n",
    "df_cpexaw = pd.read_csv(drop_filepath_cpexaw)\n",
    "df_cpexcv = pd.read_csv(drop_filepath_cpexcv)\n",
    "df = pd.concat([df_cpexaw, df_cpexcv], ignore_index = True)  #concatenates fields with same heading\n",
    "df = df[~df['Date'].isin(TC_days)].copy()  #filter out cases associated with a TD/TC\n",
    "\n",
    "if filter_out_precip:\n",
    "    df = df[(~df[metric_x].isna()) & (~df[metric_y].isna()) & (df['Environment Falling In'] != 'In Precip')].copy()\n",
    "    #df = df[(~df[metric_x].isna()) & (~df[metric_y].isna()) & (df['Environment Falling In'] != 'In Precip') & (df['Environment Falling In'] != 'In Cloud')].copy()\n",
    "else:\n",
    "    df = df[(~df[metric_x].isna()) & (~df[metric_y].isna())].copy()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(16,16))\n",
    "\n",
    "plotting_metric_x = df[metric_x]\n",
    "plotting_metric_y = df[metric_y]\n",
    "plotting_color_metric = df[metric_colorby]\n",
    "\n",
    "# if 'SHARP' in metric_x:  #shorten the shear labels in the plot if needed\n",
    "#     metric_x_label = metric_x[22:]\n",
    "# else:\n",
    "#     metric_x_label = metric_x[:]\n",
    "    \n",
    "# if 'SHARP' in metric_y:  #shorten the shear labels in the plot if needed\n",
    "#     metric_y_label = metric_y[22:]\n",
    "# else:\n",
    "#     metric_y_label = metric_y[:]\n",
    "    \n",
    "# if 'SHARP' in metric_colorby:  #shorten the shear labels in the plot if needed\n",
    "#     metric_colorby_label = metric_colorby[22:]\n",
    "# else:\n",
    "#     metric_colorby_label = metric_colorby[:]\n",
    "    \n",
    "for j in range(len(df)):\n",
    "\n",
    "    if df['Primary Convective Type'].iloc[j] == 'Scattered':\n",
    "        continue\n",
    "\n",
    "    if str(df['Date'].iloc[j])[:4] == '2022':\n",
    "        mark = '^'\n",
    "        outline = 'black'\n",
    "    else:\n",
    "        mark = 's'\n",
    "        outline = 'black'\n",
    "\n",
    "    # if df['Environment Falling In'].iloc[j] == 'In Cloud':\n",
    "    #     mark = '$C$'\n",
    "    #     outline = None\n",
    "    # elif df['Environment Falling In'].iloc[j] == 'In Precip':\n",
    "    #     mark = '$P$'\n",
    "    #     outline = None\n",
    "    # else:\n",
    "    #     mark = 'o'\n",
    "    #     outline = 'black'\n",
    "\n",
    "    #use_alpha = 1\n",
    "    if (df['Low-level Inflow Sonde'].iloc[j] == 'Yes') or (df['Mid-level Inflow Sonde'].iloc[j] == 'Yes'):   #is it an inflow sonde\n",
    "    #if str(df['Date'].iloc[j])[:4] == '2022':   #is it a CPEX-CV sonde\n",
    "        use_alpha = 1\n",
    "    else:\n",
    "        use_alpha = 0.25   #a non-inflow sonde or a CPEX(-AW) sonde (same lower alpha value for the box plots and scatter plots)\n",
    "    \n",
    "    if metric_colorby == 'Primary Convective Type':\n",
    "        if df['Primary Convective Type'].iloc[j] == 'Isolated':\n",
    "            color = 'red'\n",
    "        elif df['Primary Convective Type'].iloc[j] == 'Organized':\n",
    "            color = 'blue'\n",
    "        # elif df['Primary Convective Type'].iloc[j] == 'Scattered':\n",
    "        #     color = 'black'\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        #NOTE: if a dropsonde's \"metric_colorby\" metric is blank, then the code will run fine BUT Python will skip plotting that dropsonde point\n",
    "        ax.scatter(plotting_metric_x.iloc[j], plotting_metric_y.iloc[j], c = color, s = 150, alpha = use_alpha, marker = mark, edgecolor = outline)\n",
    "        \n",
    "    else:\n",
    "        if 'Direction' in metric_x or 'Direction' in metric_y: #use a cyclical colormap for directional shear\n",
    "            #NOTE: if a dropsonde's \"metric_colorby\" metric is blank, then the code will run fine BUT Python will skip plotting that dropsonde point\n",
    "            pm = ax.scatter(plotting_metric_x.iloc[j], plotting_metric_y.iloc[j], c = plotting_color_metric.iloc[j], cmap = cm.hsv, vmin = 0, vmax = 360, s = 150, alpha = use_alpha, marker = mark, edgecolor = outline)\n",
    "        else:\n",
    "            #NOTE: if a dropsonde's \"metric_colorby\" metric is blank, then the code will run fine BUT Python will skip plotting that dropsonde point\n",
    "            pm = ax.scatter(plotting_metric_x.iloc[j], plotting_metric_y.iloc[j], c = plotting_color_metric.iloc[j], cmap = cm.Oranges, vmin = vlims[0], vmax = vlims[1], s = 150, alpha = use_alpha, marker = mark, edgecolor = outline)\n",
    "\n",
    "\n",
    "# # Fit linear regression via least squares with numpy.polyfit\n",
    "# # It returns a slope (a) and intercept (b)\n",
    "# # deg = 1 means linear fit (i.e. polynomial of degree 1)\n",
    "# #a, b = np.polyfit(plotting_metric_x, plotting_metric_y, deg = 1)\n",
    "\n",
    "# #plotting_metric_x is the independent variable, plotting_metric_y is the independent variable\n",
    "# slope, intercept, Rvalue, pvalue, stderr = scipy.stats.linregress(plotting_metric_x, plotting_metric_y)\n",
    "\n",
    "# #the below two lines do not produce the same slope, intercept, and stderr\n",
    "# #print (scipy.stats.linregress(plotting_metric_x, plotting_metric_y)) (e.g., slope = mb/%)\n",
    "# #print (scipy.stats.linregress(plotting_metric_y, plotting_metric_x)) (e.g., slope = %/mb)\n",
    "\n",
    "# #Create sequence of 1000 values of the plotting_metric_x to create the regression line\n",
    "#     #subtracting/adding a fraction of the max value to make sure the regression line runs through the whole plot range\n",
    "# xseq = np.linspace(plotting_metric_x.min() - 0.5 * plotting_metric_x.max(), plotting_metric_x.max() + 0.5 * plotting_metric_x.max(), num=1000)\n",
    "\n",
    "# print ('Linear Regression Coefficient:', slope)\n",
    "# print ('Pearson Correlation Coefficient:', Rvalue)\n",
    "# #print ('Pearson Correlation coefficient:', scipy.stats.pearsonr(plotting_metric_x, plotting_metric_y)) #same as above\n",
    "# #print ('Pearson Correlation coefficient:', scipy.stats.pearsonr(plotting_metric_y, plotting_metric_x)) #same as above\n",
    "\n",
    "# # Plot regression line\n",
    "# ax.plot(xseq, slope * xseq + intercept, color = 'k', linestyle = '--', linewidth = 3)         \n",
    "\n",
    "\n",
    "ax.grid(True)\n",
    "#ax.set_title(f'{metric_y_label} vs. {metric_x_label}\\n(Clear and In Cloud Sondes Only, Inflow Sondes Saturated)')\n",
    "ax.set_title(f'c) {metric_y_title_label} vs. {metric_x_title_label}')\n",
    "ax.set_xlabel(metric_x_label, labelpad = 20.0)\n",
    "ax.set_ylabel(metric_y_label, labelpad = 20.0)\n",
    "ax.tick_params(length = 15, width = 5)\n",
    "#ax.set_xlim([plotting_metric_x.min() - 0.05 * plotting_metric_x.min(), plotting_metric_x.max() + 0.05 * plotting_metric_x.max()])\n",
    "#ax.set_ylim([plotting_metric_y.min() - 0.05 * plotting_metric_y.min(), plotting_metric_y.max() + 0.05 * plotting_metric_y.max()])\n",
    "\n",
    "#ax.invert_yaxis()\n",
    "#ax.set_yticks(np.arange(1000,899,-10))\n",
    "\n",
    "if metric_colorby == 'Primary Convective Type':\n",
    "    legend_elements = [Line2D([], [], color='red', linewidth = 0, marker = '^', markersize = 13, label='Isolated (CPEX-CV)'),\n",
    "                       Line2D([], [], color='red', linewidth = 0, marker = 's', markersize = 13, label='Isolated (CPEX(-AW))'),\n",
    "                       Line2D([], [], color='blue', linewidth = 0, marker = '^', markersize = 13, label='Organized (CPEX-CV)'),\n",
    "                       Line2D([], [], color='blue', linewidth = 0, marker = 's', markersize = 13, label='Organized (CPEX(-AW))')]\n",
    "    \n",
    "    #legend = ax.legend(handles = legend_elements, title = r\"TOC Type and Sonde Env't\")\n",
    "    legend = ax.legend(handles = legend_elements, loc = 'lower right')\n",
    "    \n",
    "else: \n",
    "    legend_elements = [Line2D([], [], color='k', linewidth = 3, linestyle = '--', label=f'$y = {slope:.1f}x {intercept:+.1f}$'),\n",
    "                       Line2D([], [], color='k', linewidth = 3, linestyle = '', label=f'$R_{{pearson}} = {Rvalue:.3f}$'),\n",
    "                       Line2D([], [], color='k', linewidth = 0, marker = '^', markersize = 13, label='CPEX-CV'),\n",
    "                       Line2D([], [], color='k', linewidth = 0, marker = 's', markersize = 13, label='CPEX(-AW)')]\n",
    "    \n",
    "    #legend = ax.legend(handles = legend_elements, title = \"Sonde Env't\")\n",
    "    legend = ax.legend(handles = legend_elements, loc = 'lower right')\n",
    "    \n",
    "    cbar0 = plt.colorbar(pm, ax=ax, pad = 0.02)\n",
    "    cbar0.set_label(metric_colorby_label)\n",
    "\n",
    "#plt.setp(legend.get_title(), fontweight = 'bold')\n",
    "plt.savefig(save_name, bbox_inches = 'tight')\n",
    "\n",
    "#plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "#plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "plt.close()\n",
    "plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "im = Image.open(save_name)\n",
    "\n",
    "try:\n",
    "    im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "except:\n",
    "    #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "    #though this line will have isolated, extremely minor image effects due to \n",
    "    #only using 256 colors instead of the 3-element RGB scale\n",
    "    im2 = im.convert('P')\n",
    "\n",
    "im2.save(save_name)\n",
    "im.close()\n",
    "im2.close()\n",
    "\n",
    "print ('Done!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
