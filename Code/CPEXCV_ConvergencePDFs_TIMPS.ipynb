{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978cbe8a",
   "metadata": {},
   "source": [
    "## This script uses Mani's TIMPS data (https://docs.google.com/document/d/1aAjcnRubP0HV8hbdqYfNSSnzMediNNgUlP27cOhq2d8/edit) to plot PDFs of ERA5 convergence for each CPEX-CV convective case that is a (are) TIMPS-tracked MCS(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm  #to get python's normal library of colormaps\n",
    "import matplotlib.colors as mplc\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "#from cartopy.util import add_cyclic_point\n",
    "#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "\n",
    "from PIL import Image\n",
    "import icartt            #needed to read .ict files\n",
    "\n",
    "import time\n",
    "\n",
    "tstart = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "timps_test_path = os.path.join(os.getcwd(), 'TIMPS_data', 'TIMPS_0230041_202209060430_16_-24.nc')\n",
    "\n",
    "testds = xr.open_dataset(timps_test_path)\n",
    "testds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ERA5 data\n",
    "era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "era5_path = os.path.join(era5_folder, 'CPEXCV_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "ds_era5_test = xr.open_dataset(era5_path)\n",
    "\n",
    "ds_era5_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478dca0-cf71-4c09-a727-3ebf16635dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aew_path_old = os.path.join(aew_folder, 'AEW_data_postprocessed__0924_UPDATE_EXTENDED_2022_B1-6hr_OLD.nc')\n",
    "# xr.open_dataset(aew_path_old)\n",
    "# #^^^ the old file above is very similar to the new file below, but without AEW_strength, TC_gen_time, and TC_name variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AEW tracker data, 6-hourly (Quintonâ€™s AEW tracker: https://osf.io/jnv5u, https://github.com/qlawton/QTrack?tab=readme-ov-file)\n",
    "aew_folder = os.path.join(os.getcwd(), 'AEW_Tracker_Data')\n",
    "#aew_path_old = os.path.join(aew_folder, 'AEW_data_postprocessed__0924_UPDATE_EXTENDED_2022_B1-6hr.nc')\n",
    "aew_path = os.path.join(aew_folder, 'AEW_tracks_post_processed_year_2022.nc')\n",
    "ds_aew_test = xr.open_dataset(aew_path)\n",
    "\n",
    "ds_aew_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d045df",
   "metadata": {},
   "outputs": [],
   "source": [
    "###the only variables you need to change are the 2 in this cell\n",
    "pressures_to_plot_conv = [975, 700]    #desired pressure levels for convergence\n",
    "\n",
    "#dict of desired cases to calculate ERA5 convergence PDFs for and each case's CPEX-CV collection time range\n",
    "    #(rounded to the nearest hours (to match with hourly ERA5 data), with start/end times at XX:30 UTC rounded\n",
    "    #to the most inclusive hour) (see \"CPEX-CV Well Documented Convection.docx\" for collection hours)    \n",
    "    \n",
    "#CPEX-CV convective cases (average time for dropsondes for a given convective case, per CPEX-CV Well Documented Convection.docx)\n",
    "# case_dict_conv = {8: ['20220906', [11,12]],\n",
    "#                   9: ['20220906', [13,14,15,16]],\n",
    "#                   10: ['20220906', [16,17,18]],\n",
    "#                   11: ['20220907', [13,14]],\n",
    "#                   12: ['20220907', [14,15,16,17,18]],\n",
    "#                   13: ['20220907', [15,16]],\n",
    "#                   14: ['20220914', [10,11,12,14,15]],\n",
    "#                   15: ['20220914', [12,13,14,15,16,17]],\n",
    "#                   16: ['20220916', [14,15,16]],\n",
    "#                   17: ['20220916', [17,18,19]],\n",
    "#                   18: ['20220922', [6,7,8,9]],\n",
    "#                   19: ['20220923', [9,10,11,12,13,14,15]],\n",
    "#                   20: ['20220926', [7,8,9,10,11]],\n",
    "#                   21: ['20220929', [11,12,13,14]],\n",
    "#                   22: ['20220930', [14,15]]}\n",
    "\n",
    "#CPEX-CV convective cases associated with an AEW\n",
    "    #(average time for dropsondes for a given convective case, per CPEX-CV Well Documented Convection.docx)\n",
    "# case_dict_conv_aew = {8: ['20220906', [11,12]],\n",
    "#                   9: ['20220906', [13,14,15,16]],\n",
    "#                   10: ['20220906', [16,17,18]],\n",
    "#                   11: ['20220907', [13,14]],\n",
    "#                   12: ['20220907', [14,15,16,17,18]],\n",
    "#                   13: ['20220907', [15,16]],\n",
    "#                   16: ['20220916', [14,15,16]],\n",
    "#                   17: ['20220916', [17,18,19]],\n",
    "#                   18: ['20220922', [6,7,8,9]],\n",
    "#                   19: ['20220923', [9,10,11,12,13,14,15]],\n",
    "#                   20: ['20220926', [7,8,9,10,11]],\n",
    "#                   21: ['20220929', [11,12,13,14]],\n",
    "#                   22: ['20220930', [14,15]]}\n",
    "\n",
    "#CPEX-CV convective cases to compare against one another for AGU paper\n",
    "case_dict_conv_aew = {10: ['20220906', [16,17,18]],\n",
    "                      12: ['20220907', [14,15,16,17,18]]}\n",
    "\n",
    "#test\n",
    "#case_dict_conv = {19: ['20220923', [9,10,11,12,13,14,15]]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b51f10",
   "metadata": {},
   "source": [
    "### THE FOLLOWING CELL PLOTS A 3-PANEL PLOT OF CONVECTION-RELATIVE CONVERGENCE PDFs (1 PANEL PER LIFECYCLE STAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #THIS CELL PLOTS A 3-PANEL PLOT OF CONVECTION-RELATIVE CONVERGENCE PDFs (1 PANEL PER LIFECYCLE STAGE)\n",
    "\n",
    "# #For each CPEX-CV case, calculate and plot domain-PDFs of low- (975 hPa) and mid- (700 hPa) level convergence \n",
    "# #(display median, mean, standard deviation, skewness, and kurtosis as well), with the domain being an \n",
    "# #2-by-2 degree box around TIMPS MCS centroids (e.g., Galarneau et al., 2023)\n",
    "#     #partition the data by lifecycle of the convection (use TIMPS \"gmd\" variable)\n",
    "#     #for calculating domain-mean convergence, cosine-weight each grid box value (see AOS 573 material)\n",
    "#     #KEEP TRACK OF WHICH GRID CELLS YOU HAVE ALREADY ADDED CONVERGENCE FOR, SO THAT YOU DON'T\n",
    "#         #DOUBLE COUNT GRID CELLS WHEN YOU HAVE A CASE WITH MULTIPLE TIMPS IDS WHOSE BOXES MAY OVERLAP\n",
    "\n",
    "\n",
    "# #set some baseline plot displays\n",
    "\n",
    "# #matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "# matplotlib.rcParams['axes.labelsize'] = 16\n",
    "# matplotlib.rcParams['axes.titlesize'] = 16\n",
    "# matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "# matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "# matplotlib.rcParams['xtick.labelsize'] = 16\n",
    "# matplotlib.rcParams['ytick.labelsize'] = 16\n",
    "# matplotlib.rcParams['legend.fontsize'] = 16\n",
    "# #matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "# #matplotlib.rcParams['axes.facecolor'] = 'w'\n",
    "# matplotlib.rcParams['font.family'] = 'arial'\n",
    "# matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "        \n",
    "# #Dropsonde data\n",
    "# drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "# df_drop = pd.read_csv(drop_metric_filepath)\n",
    "\n",
    "# #ERA5 data\n",
    "# era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "# era5_path = os.path.join(era5_folder, 'CPEXCV_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "# ds_era5 = xr.open_dataset(era5_path)\n",
    "\n",
    "# #TIMPS data\n",
    "# timps_folder = os.path.join(os.getcwd(), 'TIMPS_data')\n",
    "\n",
    "# conv_bins = np.arange(-15,15.1,0.5)\n",
    "\n",
    "# for case_num in case_dict_conv.keys():\n",
    "\n",
    "#     print (f'Case {case_num} convergence PDF plots in progress...')\n",
    "    \n",
    "#     df_drop_case = df_drop[df_drop['Case'] == case_num].copy()\n",
    "    \n",
    "#     case_date = case_dict_conv[case_num][0]\n",
    "#     case_hours = case_dict_conv[case_num][1]\n",
    "#     case_timps_ids = df_drop_case['TIMPS ID'].unique()\n",
    "    \n",
    "#     for pres_lev in pressures_to_plot_conv:          #plot convergence PDFs at low- and mid-levels\n",
    "        \n",
    "#         group_fig = plt.figure(figsize = (36, 12))   #initialize the convergence PDF figure for the given case\n",
    "        \n",
    "#         for lifecycle in [1,2,3]:                    #partition convergence PDFs by TIMPS convective lifecycle stage\n",
    "\n",
    "#             conv_df = pd.DataFrame()\n",
    "#             conv_lats_df = pd.DataFrame()\n",
    "#             #conv_lons_df = pd.DataFrame()\n",
    "\n",
    "#             for hr in case_hours:\n",
    "#                 hr2 = str(hr).zfill(2)\n",
    "                \n",
    "#                 coords_check_list = []\n",
    "\n",
    "#                 for ii, unique_timps_id in enumerate(case_timps_ids):\n",
    "\n",
    "#                     timps_filepath = None\n",
    "#                     if pd.isnull(unique_timps_id):\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         unique_timps_id = str(int(unique_timps_id))\n",
    "#                         for filename in os.listdir(timps_folder):\n",
    "#                             if unique_timps_id in filename:\n",
    "#                                 timps_filepath = os.path.join(timps_folder, filename)\n",
    "#                                 break\n",
    "\n",
    "#                     if timps_filepath == None:\n",
    "#                         sys.exit(f'Could not find TIMPS file for TIMP ID {unique_timps_id}')\n",
    "#                     else:\n",
    "#                         timps_ds0 = xr.open_dataset(timps_filepath)\n",
    "#                         timps_ds0 = timps_ds0.sel(time = case_date)\n",
    "#                         timps_ds = timps_ds0.sel(time = timps_ds0.time.dt.hour.isin(hr))   #gives 2 times: minute = 0 and minute = 30\n",
    "#                         timps_ds = timps_ds.sel(time = timps_ds.time.dt.minute.isin(0))    #grab the time on the hour to match with ERA5\n",
    "                        \n",
    "#                         if len(timps_ds.gmd) == 0:\n",
    "#                             print (f'{hr2} UTC is out of range of the TIMPS ID range ({timps_ds0.time[0].values.astype(str)[:-10]} - {timps_ds0.time[-1].values.astype(str)[:-10]})')\n",
    "#                             timps_ds0.close()\n",
    "#                             continue\n",
    "                        \n",
    "#                         #TIMPS has an \"unidentifiable\" lifecycle phase for the collection range for Case 10, so \n",
    "#                             #we're using our manual characterization of \"mature\" instead for Case 10\n",
    "#                         if case_num == 10:\n",
    "#                             timps_gmd = 2\n",
    "#                         else:\n",
    "#                             timps_gmd = timps_ds.gmd.item()\n",
    "                            \n",
    "#                         #only add ERA5 convergence data for the given lifecycle stage\n",
    "#                         if timps_gmd != lifecycle:\n",
    "#                             timps_ds0.close()\n",
    "#                             continue\n",
    "                            \n",
    "#                         timps_weighted_lat = timps_ds.centlatwgt.item()\n",
    "#                         timps_weighted_lon = timps_ds.centlonwgt.item()\n",
    "\n",
    "#                         #create 2-by-2 degree box around weighted centroid for the given TIMPS ID at the given hour\n",
    "#                         timps_lat_range = slice(timps_weighted_lat + 1, timps_weighted_lat - 1)\n",
    "#                         timps_lon_range = slice(timps_weighted_lon - 1, timps_weighted_lon + 1)\n",
    "\n",
    "#                         #grab all the ERA5 low-/mid-level convergence values and corresponding lats/lons within the given convective box at the given hour\n",
    "#                         v700 = ds_era5.v.sel(time = case_date).sel(level = 700)\n",
    "#                         v700 = v700.sel(time = v700.time.dt.hour.isin(hr))\n",
    "#                         v700 = mpcalc.smooth_gaussian(v700, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                         v700 = v700.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                        \n",
    "#                         #manually calculating convergence from ERA5 u and v winds (recommended by Brandon Wolding via George Kiladis)\n",
    "#                         u = ds_era5.u.sel(time = case_date).sel(level = pres_lev)\n",
    "#                         u = u.sel(time = u.time.dt.hour.isin(hr))\n",
    "#                         u = mpcalc.smooth_gaussian(u, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                         #u = u.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                \n",
    "#                         v = ds_era5.v.sel(time = case_date).sel(level = pres_lev)\n",
    "#                         v = v.sel(time = v.time.dt.hour.isin(hr))\n",
    "#                         v = mpcalc.smooth_gaussian(v, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                         #v = v.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                        \n",
    "#                         delta_lons = 0.25   #ERA5 lat/lon resolution is 0.25 degrees\n",
    "#                         delta_lons_meters = (111.3195 * 1000 * delta_lons) * np.cos(u.latitude.values * np.pi/180)  #distance between longitude lines at equator is 111.3195 km and cosine weighting this distance by latitude\n",
    "#                         dudx = (u[:,:,1:].values - u[:,:,:-1].values).squeeze() / np.expand_dims(np.abs(delta_lons_meters), axis=1)  #squeeze() removes dimensions of size 1 from an array, and expand_dims() inserts a new axis that will appear at the axis position\n",
    "#                         dudx = np.column_stack((dudx, dudx[:,-1]))  #duplicate the last column of dudx to match original grid shape (and shape of dvdy)\n",
    "            \n",
    "#                         delta_lats = 0.25\n",
    "#                         delta_lats_meters = 110.5744 * 1000 * delta_lats  #distance between latitude lines everywhere\n",
    "#                         dvdy = (v[:,:-1,:].values - v[:,1:,:].values).squeeze() / delta_lats_meters  #squeeze() removes dimensions of size 1 from an array\n",
    "#                         dvdy = np.vstack((dvdy, dvdy[-1,:]))  #duplicate the last row of dvdy to match original grid shape (and shape of dudx)\n",
    "            \n",
    "#                         conv_old = (dudx + dvdy) * -1 * 10**5  #manually calculated convergence from ERA5 u and v winds (times 10**5 1/s)\n",
    "#                         ds_conv = xr.Dataset(data_vars = dict(convergence = ([\"latitude\", \"longitude\"], conv_old)),\n",
    "#                                              coords = dict(latitude = (\"latitude\", u.latitude.values), \n",
    "#                                                            longitude = (\"longitude\", u.longitude.values)),\n",
    "#                                              attrs = dict(description = \"Manually calculated ERA5 convergence data\"))\n",
    "                        \n",
    "#                         conv = ds_conv.convergence.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "\n",
    "#                         # #grab all the ERA5 low-level convergence values and corresponding lats/lons within the given TIMPS ID box at the given hour\n",
    "#                         # conv = ds_era5.d.sel(time = case_date).sel(level = pres_lev) * -1    #convergence of the wind (1/s)\n",
    "#                         # conv = conv.sel(time = conv.time.dt.hour.isin(hr)) * 10**5  #convergence of the wind (times 10**5 1/s)\n",
    "#                         # conv = conv.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "\n",
    "#                         lon, lat = np.meshgrid(conv.longitude, conv.latitude)\n",
    "#                         lons = lon.reshape(-1)\n",
    "#                         lats = lat.reshape(-1)\n",
    "#                         conv_values = conv.values.reshape(-1)\n",
    "\n",
    "#                         #add data from each hour as COLUMNS to corresponding df\n",
    "#                         #only add values to conv_df that haven't been already been added from a prior TIMPS ID for the given hour (so no duplicates!)\n",
    "#                         if ii != 0:  #if not the first TIMPS ID for the given hour\n",
    "#                             for xx, coord in enumerate(zip(lats,lons)):\n",
    "#                                 if coord in coords_check_list:\n",
    "#                                     print (f'Duplicate coordinate for Case {case_num}, replacing with NaN')\n",
    "#                                     lats[xx] = np.nan\n",
    "#                                     lons[xx] = np.nan\n",
    "#                                     conv_values[xx] = np.nan\n",
    "                        \n",
    "#                         coords_check_list += list(zip(lats, lons))  #append the lats/lons to the list as tuples (coordinate pairs)\n",
    "                        \n",
    "#                         conv_df = pd.concat((conv_df, pd.Series(conv_values)), axis = 1, ignore_index = True)\n",
    "#                         conv_lats_df = pd.concat((conv_lats_df, pd.Series(lats)), axis = 1, ignore_index = True)\n",
    "#                         #conv_lons_df = pd.concat((conv_lons_df, pd.Series(lons)), axis = 1, ignore_index = True)\n",
    "\n",
    "#                         timps_ds0.close()\n",
    "\n",
    "#             ax = group_fig.add_subplot(1, 3, lifecycle)        \n",
    "\n",
    "#             if lifecycle == 1:\n",
    "#                 clc = 'Growth'\n",
    "#                 color = 'limegreen'\n",
    "#             elif lifecycle == 2:\n",
    "#                 clc = 'Mature'\n",
    "#                 color = 'darkred'\n",
    "#             elif lifecycle == 3:\n",
    "#                 clc = 'Decay'\n",
    "#                 color = 'blue'\n",
    "            \n",
    "#             ax.set_title('Case %i (%s Stage) ERA5 %i hPa Convergence PDF (Convection-Relative)' % (case_num, clc, pres_lev))\n",
    "                \n",
    "#             if len(conv_df) == 0:  #no data for the given lifecycle for the given case for the given hours\n",
    "#                 ax.text(0.5, 0.5, f'{clc} stage was not sampled\\nfor Case {case_num} during CPEX-CV', \n",
    "#                         horizontalalignment = 'center', verticalalignment = 'center', \n",
    "#                         fontsize = 30, bbox = {'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "#                 ax.set_xticklabels([])\n",
    "#                 ax.set_yticklabels([])\n",
    "#                 continue\n",
    "            \n",
    "#             conv_df = conv_df.values  #convert Pandas DataFrame to NumPy array\n",
    "#             conv_lats_df = conv_lats_df.values  #convert Pandas DataFrame to NumPy array\n",
    "            \n",
    "#             #mask NaN values in the Dataframes so that the numpy stat calculations work below\n",
    "#             conv_df_masked = np.ma.masked_where(np.isnan(conv_df), conv_df)\n",
    "#             conv_lats_df_masked = np.ma.masked_where(np.isnan(conv_lats_df), conv_lats_df)\n",
    "                \n",
    "#             ax.hist(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None,\n",
    "#                     histtype = 'step', align = 'mid', orientation = 'vertical', color = color, linewidth = 2)\n",
    "#                 #density = True returns a probability density: each bin will display the bin's raw count \n",
    "#                     #divided by the total number of counts and the bin width\n",
    "#                     #(density = counts / (sum(counts) * np.diff(bins))), so that the area under the \n",
    "#                     #histogram integrates to 1 (np.sum(density * np.diff(bins)) == 1)\n",
    "    \n",
    "#             cos_weights = np.sqrt(np.cos(conv_lats_df_masked * np.pi/180))   #cosine weights to apply to conv_df\n",
    "        \n",
    "#             conv_count = np.count_nonzero(~np.isnan(conv_df))\n",
    "#             conv_median = np.round(np.nanmedian(conv_df, axis = None), 2)\n",
    "#             conv_mean = np.round(np.nanmean(conv_df, axis = None), 2)                                        #non-weighted mean (1st moment)\n",
    "#             conv_wgt_mean = np.round(np.average(conv_df_masked, axis = None, weights = cos_weights), 2)      #cosine-weighted mean (1st moment)\n",
    "#             conv_std = np.round(np.std(conv_df_masked, axis = None), 2)                                      #standard deviation (2nd moment)\n",
    "#             conv_skew = np.round(scipy.stats.skew(conv_df_masked, axis = None, nan_policy = 'omit'), 4)      #skewness (3rd moment)\n",
    "#             conv_kurt = np.round(scipy.stats.kurtosis(conv_df_masked, axis = None, nan_policy = 'omit'), 4)  #kurtosis (4th moment)\n",
    "        \n",
    "#             ax.axvline(x = 0, color = 'k', linestyle = '--', alpha = 0.5)\n",
    "#             ax.text(0.98, 0.875, \n",
    "#                     f'Count: {conv_count}\\nMedian: {conv_median}\\nMean: {conv_mean}\\nWeighted Mean: {conv_wgt_mean}\\nStandard Deviation: {conv_std}\\nSkewness: {conv_skew}\\nKurtosis: {conv_kurt}\\n', \n",
    "#                     transform = ax.transAxes, horizontalalignment = 'right', verticalalignment = 'center', \n",
    "#                     fontsize = 16, fontweight = 'bold', color = color)\n",
    "            \n",
    "#             ax.set_xlabel('Convergence [10$^{-5}$ s$^{-1}$]')\n",
    "#             ax.set_ylabel('Prob(Convergence)')\n",
    "#             ax.set_xlim([-15,15])\n",
    "#             ax.set_xticks(np.arange(-15, 15.1, 2))\n",
    "#             ax.set_yticks(np.arange(0, 0.401, 0.05))\n",
    "#             ax.grid(axis = 'y')\n",
    "                    \n",
    "#         #plt.tight_layout()\n",
    "#         #plt.subplots_adjust(wspace = 0.1)\n",
    "\n",
    "#         #save the figure\n",
    "#         plot_save_name = f'Case{case_num}_{pres_lev}hPa_convergence_PDFs_convection_relative_separate.png'\n",
    "#         plt.savefig(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/Convection_Relative/Smoothed_ERA5_winds', plot_save_name), bbox_inches = 'tight')\n",
    "#         #plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "#         #plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "#         plt.close()\n",
    "#         plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "#         ##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "#         ##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "#         im = Image.open(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/Convection_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "\n",
    "#         try:\n",
    "#             im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "#         except:\n",
    "#             #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "#             #though this line will have isolated, extremely minor image effects due to \n",
    "#             #only using 256 colors instead of the 3-element RGB scale\n",
    "#             im2 = im.convert('P')\n",
    "\n",
    "#         im2.save(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/Convection_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "#         im.close()\n",
    "#         im2.close()\n",
    "    \n",
    "#     print (f'Case {case_num} convergence PDF plots complete!\\n')\n",
    "\n",
    "# ds_era5.close()\n",
    "\n",
    "# tend = time.time()\n",
    "# print (f'This script took {np.round((tend - tstart) / 60, 1)} minutes to complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862dbda",
   "metadata": {},
   "source": [
    "### THE FOLLOWING CELL PLOTS A 3-PANEL PLOT OF CONVECTION-RELATIVE CONVERGENCE PDFs COMPARING 2 CASES (1 PANEL PER LIFECYCLE STAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #THIS CELL PLOTS A 3-PANEL PLOT OF CONVECTION-RELATIVE CONVERGENCE PDFs COMPARING 2 CASES (1 PANEL PER LIFECYCLE STAGE)\n",
    "\n",
    "# #For each CPEX-CV case, calculate and plot domain-PDFs of low- (975 hPa) and mid- (700 hPa) level convergence \n",
    "# #(display median, mean, standard deviation, skewness, and kurtosis as well), with the domain being an \n",
    "# #2-by-2 degree box around TIMPS MCS centroids (e.g., Galarneau et al., 2023)\n",
    "#     #partition the data by lifecycle of the convection (use TIMPS \"gmd\" variable)\n",
    "#     #for calculating domain-mean convergence, cosine-weight each grid box value (see AOS 573 material)\n",
    "#     #KEEP TRACK OF WHICH GRID CELLS YOU HAVE ALREADY ADDED CONVERGENCE FOR, SO THAT YOU DON'T\n",
    "#         #DOUBLE COUNT GRID CELLS WHEN YOU HAVE A CASE WITH MULTIPLE TIMPS IDS WHOSE BOXES MAY OVERLAP\n",
    "\n",
    "# #CPEX-CV convective cases (average time for dropsondes for a given convective case, per CPEX-CV Well Documented Convection.docx)\n",
    "# case_dict_comp = {10: ['20220906', [16,17,18]],\n",
    "#                   12: ['20220907', [14,15,16,17,18]]}\n",
    "\n",
    "# # case_dict_comp = {8: ['20220906', [11,12]],\n",
    "# #                   9: ['20220906', [13,14,15,16]],\n",
    "# #                   10: ['20220906', [16,17,18]],\n",
    "# #                   11: ['20220907', [13,14]],\n",
    "# #                   12: ['20220907', [14,15,16,17,18]],\n",
    "# #                   13: ['20220907', [15,16]],\n",
    "# #                   14: ['20220914', [10,11,12,14,15]],\n",
    "# #                   15: ['20220914', [12,13,14,15,16,17]],\n",
    "# #                   16: ['20220916', [14,15,16]],\n",
    "# #                   17: ['20220916', [17,18,19]],\n",
    "# #                   18: ['20220922', [6,7,8,9]],\n",
    "# #                   19: ['20220923', [9,10,11,12,13,14,15]],\n",
    "# #                   20: ['20220926', [7,8,9,10,11]],\n",
    "# #                   21: ['20220929', [11,12,13,14]],\n",
    "# #                   22: ['20220930', [14,15]]}        \n",
    "        \n",
    "# #set some baseline plot displays\n",
    "\n",
    "# #matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "# matplotlib.rcParams['axes.labelsize'] = 20\n",
    "# matplotlib.rcParams['axes.titlesize'] = 20\n",
    "# matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "# matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "# matplotlib.rcParams['xtick.labelsize'] = 20\n",
    "# matplotlib.rcParams['ytick.labelsize'] = 20\n",
    "# matplotlib.rcParams['legend.fontsize'] = 18\n",
    "# #matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "# #matplotlib.rcParams['axes.facecolor'] = 'w'\n",
    "# matplotlib.rcParams['font.family'] = 'arial'\n",
    "# matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "        \n",
    "# #Dropsonde data\n",
    "# drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "# df_drop = pd.read_csv(drop_metric_filepath)\n",
    "\n",
    "# #ERA5 data\n",
    "# era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "# era5_path = os.path.join(era5_folder, 'CPEXCV_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "# ds_era5 = xr.open_dataset(era5_path)\n",
    "\n",
    "# #TIMPS data\n",
    "# timps_folder = os.path.join(os.getcwd(), 'TIMPS_data')\n",
    "\n",
    "# conv_bins = np.arange(-15,15.1,0.5)\n",
    "\n",
    "# for pres_lev in pressures_to_plot_conv:          #plot convergence PDFs at low- and mid-levels\n",
    "\n",
    "#     group_fig = plt.figure(figsize = (36, 12))   #initialize the convergence PDF figure for the given case comparison\n",
    "\n",
    "#     first_case_no_lifecycle_data = [False, False, False]\n",
    "    \n",
    "#     for case_num in case_dict_comp.keys():\n",
    "\n",
    "#         #print (f'Case {case_num} convergence PDF plots in progress...')\n",
    "\n",
    "#         df_drop_case = df_drop[df_drop['Case'] == case_num].copy()\n",
    "\n",
    "#         case_date = case_dict_comp[case_num][0]\n",
    "#         case_hours = case_dict_comp[case_num][1]\n",
    "#         case_timps_ids = df_drop_case['TIMPS ID'].unique()\n",
    "\n",
    "#         for lifecycle in [1,2,3]:                    #partition convergence PDFs by TIMPS convective lifecycle stage\n",
    "\n",
    "#             conv_df = pd.DataFrame()\n",
    "#             conv_lats_df = pd.DataFrame()\n",
    "#             #conv_lons_df = pd.DataFrame()\n",
    "\n",
    "#             for hr in case_hours:\n",
    "#                 hr2 = str(hr).zfill(2)\n",
    "\n",
    "#                 coords_check_list = []\n",
    "\n",
    "#                 for ii, unique_timps_id in enumerate(case_timps_ids):\n",
    "\n",
    "#                     timps_filepath = None\n",
    "#                     if pd.isnull(unique_timps_id):\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         unique_timps_id = str(int(unique_timps_id))\n",
    "#                         for filename in os.listdir(timps_folder):\n",
    "#                             if unique_timps_id in filename:\n",
    "#                                 timps_filepath = os.path.join(timps_folder, filename)\n",
    "#                                 break\n",
    "\n",
    "#                     if timps_filepath == None:\n",
    "#                         sys.exit(f'Could not find TIMPS file for TIMP ID {unique_timps_id}')\n",
    "#                     else:\n",
    "#                         timps_ds0 = xr.open_dataset(timps_filepath)\n",
    "#                         timps_ds0 = timps_ds0.sel(time = case_date)\n",
    "#                         timps_ds = timps_ds0.sel(time = timps_ds0.time.dt.hour.isin(hr))   #gives 2 times: minute = 0 and minute = 30\n",
    "#                         timps_ds = timps_ds.sel(time = timps_ds.time.dt.minute.isin(0))    #grab the time on the hour to match with ERA5\n",
    "\n",
    "#                         if len(timps_ds.gmd) == 0:\n",
    "#                             print (f'{hr2} UTC is out of range of the TIMPS ID range ({timps_ds0.time[0].values.astype(str)[:-10]} - {timps_ds0.time[-1].values.astype(str)[:-10]})')\n",
    "#                             timps_ds0.close()\n",
    "#                             continue\n",
    "\n",
    "#                         #TIMPS has an \"unidentifiable\" lifecycle phase for the collection range for Case 10, so \n",
    "#                             #we're using our manual characterization of \"mature\" instead for Case 10\n",
    "#                         if case_num == 10:\n",
    "#                             timps_gmd = 2\n",
    "#                         else:\n",
    "#                             timps_gmd = timps_ds.gmd.item()\n",
    "\n",
    "#                         #only add ERA5 convergence data for the given lifecycle stage\n",
    "#                         if timps_gmd != lifecycle:\n",
    "#                             timps_ds0.close()\n",
    "#                             continue\n",
    "\n",
    "#                         timps_weighted_lat = timps_ds.centlatwgt.item()\n",
    "#                         timps_weighted_lon = timps_ds.centlonwgt.item()\n",
    "\n",
    "#                         #create 2-by-2 degree box around weighted centroid for the given TIMPS ID at the given hour\n",
    "#                         timps_lat_range = slice(timps_weighted_lat + 1, timps_weighted_lat - 1)\n",
    "#                         timps_lon_range = slice(timps_weighted_lon - 1, timps_weighted_lon + 1)\n",
    "\n",
    "#                         #grab all the ERA5 low-/mid-level convergence values and corresponding lats/lons within the given convective box at the given hour\n",
    "#                         v700 = ds_era5.v.sel(time = case_date).sel(level = 700)\n",
    "#                         v700 = v700.sel(time = v700.time.dt.hour.isin(hr))\n",
    "#                         v700 = mpcalc.smooth_gaussian(v700, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                         v700 = v700.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                        \n",
    "#                         #manually calculating convergence from ERA5 u and v winds (recommended by Brandon Wolding via George Kiladis)\n",
    "#                         u = ds_era5.u.sel(time = case_date).sel(level = pres_lev)\n",
    "#                         u = u.sel(time = u.time.dt.hour.isin(hr))\n",
    "#                         u = mpcalc.smooth_gaussian(u, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                         #u = u.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                \n",
    "#                         v = ds_era5.v.sel(time = case_date).sel(level = pres_lev)\n",
    "#                         v = v.sel(time = v.time.dt.hour.isin(hr))\n",
    "#                         v = mpcalc.smooth_gaussian(v, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                         #v = v.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                        \n",
    "#                         delta_lons = 0.25   #ERA5 lat/lon resolution is 0.25 degrees\n",
    "#                         delta_lons_meters = (111.3195 * 1000 * delta_lons) * np.cos(u.latitude.values * np.pi/180)  #distance between longitude lines at equator is 111.3195 km and cosine weighting this distance by latitude\n",
    "#                         dudx = (u[:,:,1:].values - u[:,:,:-1].values).squeeze() / np.expand_dims(np.abs(delta_lons_meters), axis=1)  #squeeze() removes dimensions of size 1 from an array, and expand_dims() inserts a new axis that will appear at the axis position\n",
    "#                         dudx = np.column_stack((dudx, dudx[:,-1]))  #duplicate the last column of dudx to match original grid shape (and shape of dvdy)\n",
    "            \n",
    "#                         delta_lats = 0.25\n",
    "#                         delta_lats_meters = 110.5744 * 1000 * delta_lats  #distance between latitude lines everywhere\n",
    "#                         dvdy = (v[:,:-1,:].values - v[:,1:,:].values).squeeze() / delta_lats_meters  #squeeze() removes dimensions of size 1 from an array\n",
    "#                         dvdy = np.vstack((dvdy, dvdy[-1,:]))  #duplicate the last row of dvdy to match original grid shape (and shape of dudx)\n",
    "            \n",
    "#                         conv_old = (dudx + dvdy) * -1 * 10**5  #manually calculated convergence from ERA5 u and v winds (times 10**5 1/s)\n",
    "#                         ds_conv = xr.Dataset(data_vars = dict(convergence = ([\"latitude\", \"longitude\"], conv_old)),\n",
    "#                                              coords = dict(latitude = (\"latitude\", u.latitude.values), \n",
    "#                                                            longitude = (\"longitude\", u.longitude.values)),\n",
    "#                                              attrs = dict(description = \"Manually calculated ERA5 convergence data\"))\n",
    "                        \n",
    "#                         conv = ds_conv.convergence.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "\n",
    "#                         # #grab all the ERA5 low-level convergence values and corresponding lats/lons within the given TIMPS ID box at the given hour\n",
    "#                         # conv = ds_era5.d.sel(time = case_date).sel(level = pres_lev) * -1    #convergence of the wind (1/s)\n",
    "#                         # conv = conv.sel(time = conv.time.dt.hour.isin(hr)) * 10**5  #convergence of the wind (times 10**5 1/s)\n",
    "#                         # conv = conv.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "\n",
    "#                         lon, lat = np.meshgrid(conv.longitude, conv.latitude)\n",
    "#                         lons = lon.reshape(-1)\n",
    "#                         lats = lat.reshape(-1)\n",
    "#                         conv_values = conv.values.reshape(-1)\n",
    "\n",
    "#                         #add data from each hour as COLUMNS to corresponding df\n",
    "#                         #only add values to conv_df that haven't been already been added from a prior TIMPS ID for the given hour (so no duplicates!)\n",
    "#                         if ii != 0:  #if not the first TIMPS ID for the given hour\n",
    "#                             for xx, coord in enumerate(zip(lats,lons)):\n",
    "#                                 if coord in coords_check_list:\n",
    "#                                     print (f'Duplicate coordinate for Case {case_num}, replacing with NaN')\n",
    "#                                     lats[xx] = np.nan\n",
    "#                                     lons[xx] = np.nan\n",
    "#                                     conv_values[xx] = np.nan\n",
    "\n",
    "#                         coords_check_list += list(zip(lats, lons))  #append the lats/lons to the list as tuples (coordinate pairs)\n",
    "\n",
    "#                         conv_df = pd.concat((conv_df, pd.Series(conv_values)), axis = 1, ignore_index = True)\n",
    "#                         conv_lats_df = pd.concat((conv_lats_df, pd.Series(lats)), axis = 1, ignore_index = True)\n",
    "#                         #conv_lons_df = pd.concat((conv_lons_df, pd.Series(lons)), axis = 1, ignore_index = True)\n",
    "\n",
    "#                         timps_ds0.close()\n",
    "            \n",
    "#             #create or grab the axis of interest\n",
    "#             if case_num == list(case_dict_comp.keys())[0]:\n",
    "#                 ax = group_fig.add_subplot(1, 3, lifecycle)\n",
    "#                 text_denom = 0\n",
    "                \n",
    "#                 if lifecycle == 1:\n",
    "#                     clc = 'Growth'\n",
    "#                     color = 'limegreen'\n",
    "#                 elif lifecycle == 2:\n",
    "#                     clc = 'Mature'\n",
    "#                     color = 'darkred'\n",
    "#                 elif lifecycle == 3:\n",
    "#                     clc = 'Decay'\n",
    "#                     color = 'cornflowerblue'\n",
    "#             else:  #case_num == list(case_dict_comp.keys())[-1]\n",
    "#                 ax = group_fig.get_axes()[lifecycle - 1]\n",
    "#                 text_denom = 0.2\n",
    "\n",
    "#                 if lifecycle == 1:\n",
    "#                     clc = 'Growth'\n",
    "#                     color = 'darkgreen'\n",
    "#                 elif lifecycle == 2:\n",
    "#                     clc = 'Mature'\n",
    "#                     color = 'navy'\n",
    "#                 elif lifecycle == 3:\n",
    "#                     clc = 'Decay'\n",
    "#                     color = 'darkblue'\n",
    "            \n",
    "#                 ax.set_title('ERA5 %i hPa Convergence PDFs\\n(Convection-Relative, %s Stage, 2x2 Degree Box)' % (pres_lev, clc))\n",
    "            \n",
    "#             if (len(conv_df) == 0) and (case_num == list(case_dict_comp.keys())[0]):     #no data for the given lifecycle for the given case for the given hours\n",
    "#                 first_case_no_lifecycle_data[lifecycle - 1] = True\n",
    "#                 continue\n",
    "#             elif (len(conv_df) == 0) and (case_num == list(case_dict_comp.keys())[-1]):  #no data for the given lifecycle for the given case for the given hours\n",
    "#                 if first_case_no_lifecycle_data[lifecycle - 1]:  #no data for the given lifecycle for either case\n",
    "#                     ax.text(0.5, 0.5, f'{clc} stage was not sampled for\\nCase {list(case_dict_comp.keys())[0]} nor Case {case_num} during CPEX-CV', \n",
    "#                             horizontalalignment = 'center', verticalalignment = 'center', \n",
    "#                             fontsize = 30, bbox = {'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "#                     ax.set_xticklabels([])\n",
    "#                     ax.set_yticklabels([])\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     continue\n",
    "#             else:\n",
    "#                 pass\n",
    "\n",
    "#             conv_df = conv_df.values  #convert Pandas DataFrame to NumPy array\n",
    "#             conv_lats_df = conv_lats_df.values  #convert Pandas DataFrame to NumPy array\n",
    "\n",
    "#             #mask NaN values in the Dataframes so that the numpy stat calculations work below\n",
    "#             conv_df_masked = np.ma.masked_where(np.isnan(conv_df), conv_df)\n",
    "#             conv_lats_df_masked = np.ma.masked_where(np.isnan(conv_lats_df), conv_lats_df)\n",
    "\n",
    "#             #line plot histogram (clearer to interpret than \"step\" histogram below)\n",
    "#             hist, bins = np.histogram(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None)\n",
    "#             bin_centers = (bins[:-1] + bins[1:]) / 2  # Midpoints of the bins\n",
    "#             ax.plot(bin_centers, hist, linewidth = 2, linestyle = '-', color = color, label = f'Case {case_num}')\n",
    "\n",
    "#             # #normal \"step\" histogram\n",
    "#             # ax.hist(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None,\n",
    "#             #         histtype = 'step', align = 'mid', orientation = 'vertical', color = color, \n",
    "#             #         linewidth = 2, label = f'Case {case_num}')\n",
    "#             #     #density = True returns a probability density: each bin will display the bin's raw count \n",
    "#             #         #divided by the total number of counts and the bin width\n",
    "#             #         #(density = counts / (sum(counts) * np.diff(bins))), so that the area under the \n",
    "#             #         #histogram integrates to 1 (np.sum(density * np.diff(bins)) == 1)\n",
    "\n",
    "#             cos_weights = np.sqrt(np.cos(conv_lats_df_masked * np.pi/180))   #cosine weights to apply to conv_df\n",
    "\n",
    "#             conv_count = np.count_nonzero(~np.isnan(conv_df))\n",
    "#             conv_median = np.round(np.nanmedian(conv_df, axis = None), 2)\n",
    "#             conv_mean = np.round(np.nanmean(conv_df, axis = None), 2)                                        #non-weighted mean (1st moment)\n",
    "#             conv_wgt_mean = np.round(np.average(conv_df_masked, axis = None, weights = cos_weights), 2)      #cosine-weighted mean (1st moment)\n",
    "#             conv_std = np.round(np.std(conv_df_masked, axis = None), 2)                                      #standard deviation (2nd moment)\n",
    "#             conv_skew = np.round(scipy.stats.skew(conv_df_masked, axis = None, nan_policy = 'omit'), 4)      #skewness (3rd moment)\n",
    "#             conv_kurt = np.round(scipy.stats.kurtosis(conv_df_masked, axis = None, nan_policy = 'omit'), 4)  #kurtosis (4th moment)\n",
    "\n",
    "#             ax.axvline(x = 0, color = 'k', linestyle = '--', alpha = 0.5)\n",
    "            \n",
    "#             ax.text(0.98, 0.89 - text_denom, \n",
    "#                     f'Count: {conv_count}\\nMedian: {conv_median}\\nMean: {conv_mean}\\nWeighted Mean: {conv_wgt_mean}\\nStandard Deviation: {conv_std}\\nSkewness: {conv_skew}\\nKurtosis: {conv_kurt}\\n', \n",
    "#                     transform = ax.transAxes, horizontalalignment = 'right', verticalalignment = 'center', \n",
    "#                     fontsize = 16, fontweight = 'bold', color = color)\n",
    "                    \n",
    "#         #print (f'Case {case_num} convergence PDF plots complete!\\n')\n",
    "                    \n",
    "#     for ax in group_fig.get_axes():\n",
    "#         ax.set_xlabel('Convergence [10$^{-5}$ s$^{-1}$]')\n",
    "#         ax.set_ylabel('Prob(Convergence)')\n",
    "#         ax.set_xlim([-13,13])\n",
    "#         ax.set_xticks(np.arange(-13, 13.1, 2))\n",
    "#         ax.set_ylim(bottom = 0)\n",
    "#         #ax.set_yticks(np.arange(0, 0.401, 0.05))\n",
    "#         ax.grid(axis = 'y')\n",
    "#         ax.legend(loc = 'upper left')\n",
    "\n",
    "#     #plt.tight_layout()\n",
    "#     #plt.subplots_adjust(wspace = 0.1)\n",
    "    \n",
    "#     #save the figure\n",
    "#     case_nums = '-'.join(map(str, case_dict_comp.keys()))\n",
    "#     plot_save_name = f'Case{case_nums}_{pres_lev}hPa_convergence_PDFs_convection_relative_separate.png'\n",
    "#     plt.savefig(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/Convection_Relative/Smoothed_ERA5_winds', plot_save_name), bbox_inches = 'tight')\n",
    "#     #plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "#     #plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "#     plt.close()\n",
    "#     plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "#     ##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "#     ##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "#     im = Image.open(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/Convection_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "\n",
    "#     try:\n",
    "#         im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "#     except:\n",
    "#         #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "#         #though this line will have isolated, extremely minor image effects due to \n",
    "#         #only using 256 colors instead of the 3-element RGB scale\n",
    "#         im2 = im.convert('P')\n",
    "\n",
    "#     im2.save(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/Convection_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "#     im.close()\n",
    "#     im2.close()\n",
    "\n",
    "# ds_era5.close()\n",
    "\n",
    "# #tend = time.time()\n",
    "# #print (f'This script took {np.round((tend - tstart) / 60, 1)} minutes to complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ed40c",
   "metadata": {},
   "source": [
    "### THE FOLLOWING CELL PLOTS A 1-PANEL PLOT OF CONVECTION-RELATIVE CONVERGENCE PDFs (LIFECYCLE STAGE PDFs OVERLAID ON ONE ANOTHER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #THIS CELL PLOTS A 1-PANEL PLOT OF CONVECTION-RELATIVE CONVERGENCE PDFs (LIFECYCLE STAGE PDFs OVERLAID ON ONE ANOTHER)\n",
    "\n",
    "# #For each CPEX-CV case, calculate and plot domain-PDFs of low- (975 hPa) and mid- (700 hPa) level convergence \n",
    "# #(display median, mean, standard deviation, skewness, and kurtosis as well), with the domain being an \n",
    "# #2-by-2 degree box around TIMPS MCS centroids (e.g., Galarneau et al., 2023)\n",
    "#     #partition the data by lifecycle of the convection (use TIMPS \"gmd\" variable)\n",
    "#     #for calculating domain-mean convergence, cosine-weight each grid box value (see AOS 573 material)\n",
    "#     #KEEP TRACK OF WHICH GRID CELLS YOU HAVE ALREADY ADDED CONVERGENCE FOR, SO THAT YOU DON'T\n",
    "#         #DOUBLE COUNT GRID CELLS WHEN YOU HAVE A CASE WITH MULTIPLE TIMPS IDS WHOSE BOXES MAY OVERLAP\n",
    "\n",
    "# #set some baseline plot displays\n",
    "\n",
    "# #matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "# matplotlib.rcParams['axes.labelsize'] = 18\n",
    "# matplotlib.rcParams['axes.titlesize'] = 18\n",
    "# matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "# matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "# matplotlib.rcParams['xtick.labelsize'] = 18\n",
    "# matplotlib.rcParams['ytick.labelsize'] = 18\n",
    "# matplotlib.rcParams['legend.fontsize'] = 17\n",
    "# #matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "# #matplotlib.rcParams['axes.facecolor'] = 'w'\n",
    "# matplotlib.rcParams['font.family'] = 'arial'\n",
    "# matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "        \n",
    "# #Dropsonde data\n",
    "# drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "# df_drop = pd.read_csv(drop_metric_filepath)\n",
    "\n",
    "# #ERA5 data\n",
    "# era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "# era5_path = os.path.join(era5_folder, 'CPEXCV_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "# ds_era5 = xr.open_dataset(era5_path)\n",
    "\n",
    "# #TIMPS data\n",
    "# timps_folder = os.path.join(os.getcwd(), 'TIMPS_data')\n",
    "\n",
    "# conv_bins = np.arange(-15,15.1,0.5)\n",
    "\n",
    "# for case_num in case_dict_conv.keys():\n",
    "\n",
    "#     print (f'Case {case_num} convergence PDF plots in progress...')\n",
    "    \n",
    "#     df_drop_case = df_drop[df_drop['Case'] == case_num].copy()\n",
    "    \n",
    "#     case_date = case_dict_conv[case_num][0]\n",
    "#     case_hours = case_dict_conv[case_num][1]\n",
    "#     case_timps_ids = df_drop_case['TIMPS ID'].unique()\n",
    "    \n",
    "#     for pres_lev in pressures_to_plot_conv:          #plot convergence PDFs at low- and mid-levels\n",
    "        \n",
    "#         no_lifecycle_counter = 0\n",
    "        \n",
    "#         group_fig = plt.figure(figsize = (12, 12))   #initialize the convergence PDF figure for the given case\n",
    "#         ax = group_fig.add_subplot(1, 1, 1)\n",
    "        \n",
    "#         for lifecycle in [1,2,3]:                    #partition convergence PDFs by TIMPS convective lifecycle stage\n",
    "\n",
    "#             conv_df = pd.DataFrame()\n",
    "#             conv_lats_df = pd.DataFrame()\n",
    "#             #conv_lons_df = pd.DataFrame()\n",
    "\n",
    "#             for hr in case_hours:\n",
    "#                 hr2 = str(hr).zfill(2)\n",
    "                \n",
    "#                 coords_check_list = []\n",
    "\n",
    "#                 for ii, unique_timps_id in enumerate(case_timps_ids):\n",
    "\n",
    "#                     timps_filepath = None\n",
    "#                     if pd.isnull(unique_timps_id):\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         unique_timps_id = str(int(unique_timps_id))\n",
    "#                         for filename in os.listdir(timps_folder):\n",
    "#                             if unique_timps_id in filename:\n",
    "#                                 timps_filepath = os.path.join(timps_folder, filename)\n",
    "#                                 break\n",
    "\n",
    "#                     if timps_filepath == None:\n",
    "#                         sys.exit(f'Could not find TIMPS file for TIMP ID {unique_timps_id}')\n",
    "#                     else:\n",
    "#                         timps_ds0 = xr.open_dataset(timps_filepath)\n",
    "#                         timps_ds0 = timps_ds0.sel(time = case_date)\n",
    "#                         timps_ds = timps_ds0.sel(time = timps_ds0.time.dt.hour.isin(hr))   #gives 2 times: minute = 0 and minute = 30\n",
    "#                         timps_ds = timps_ds.sel(time = timps_ds.time.dt.minute.isin(0))    #grab the time on the hour to match with ERA5\n",
    "                        \n",
    "#                         if len(timps_ds.gmd) == 0:\n",
    "#                             print (f'{hr2} UTC is out of range of the TIMPS ID range ({timps_ds0.time[0].values.astype(str)[:-10]} - {timps_ds0.time[-1].values.astype(str)[:-10]})')\n",
    "#                             timps_ds0.close()\n",
    "#                             continue\n",
    "                            \n",
    "#                         #TIMPS has an \"unidentifiable\" lifecycle phase for the collection range for Case 10, so \n",
    "#                             #we're using our manual characterization of \"mature\" instead for Case 10\n",
    "#                         if case_num == 10:\n",
    "#                             timps_gmd = 2\n",
    "#                         else:\n",
    "#                             timps_gmd = timps_ds.gmd.item()\n",
    "                            \n",
    "#                         #only add ERA5 convergence data for the given lifecycle stage\n",
    "#                         if timps_gmd != lifecycle:\n",
    "#                             timps_ds0.close()\n",
    "#                             continue\n",
    "                            \n",
    "#                         timps_weighted_lat = timps_ds.centlatwgt.item()\n",
    "#                         timps_weighted_lon = timps_ds.centlonwgt.item()\n",
    "\n",
    "#                         #create 2-by-2 degree box around weighted centroid for the given TIMPS ID at the given hour\n",
    "#                         timps_lat_range = slice(timps_weighted_lat + 1, timps_weighted_lat - 1)\n",
    "#                         timps_lon_range = slice(timps_weighted_lon - 1, timps_weighted_lon + 1)\n",
    "\n",
    "#                         #grab all the ERA5 low-/mid-level convergence values and corresponding lats/lons within the given convective box at the given hour\n",
    "#                         v700 = ds_era5.v.sel(time = case_date).sel(level = 700)\n",
    "#                         v700 = v700.sel(time = v700.time.dt.hour.isin(hr))\n",
    "#                         v700 = mpcalc.smooth_gaussian(v700, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                         v700 = v700.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                        \n",
    "#                         #manually calculating convergence from ERA5 u and v winds (recommended by Brandon Wolding via George Kiladis)\n",
    "#                         u = ds_era5.u.sel(time = case_date).sel(level = pres_lev)\n",
    "#                         u = u.sel(time = u.time.dt.hour.isin(hr))\n",
    "#                         u = mpcalc.smooth_gaussian(u, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                         #u = u.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                \n",
    "#                         v = ds_era5.v.sel(time = case_date).sel(level = pres_lev)\n",
    "#                         v = v.sel(time = v.time.dt.hour.isin(hr))\n",
    "#                         v = mpcalc.smooth_gaussian(v, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                         #v = v.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "                        \n",
    "#                         delta_lons = 0.25   #ERA5 lat/lon resolution is 0.25 degrees\n",
    "#                         delta_lons_meters = (111.3195 * 1000 * delta_lons) * np.cos(u.latitude.values * np.pi/180)  #distance between longitude lines at equator is 111.3195 km and cosine weighting this distance by latitude\n",
    "#                         dudx = (u[:,:,1:].values - u[:,:,:-1].values).squeeze() / np.expand_dims(np.abs(delta_lons_meters), axis=1)  #squeeze() removes dimensions of size 1 from an array, and expand_dims() inserts a new axis that will appear at the axis position\n",
    "#                         dudx = np.column_stack((dudx, dudx[:,-1]))  #duplicate the last column of dudx to match original grid shape (and shape of dvdy)\n",
    "            \n",
    "#                         delta_lats = 0.25\n",
    "#                         delta_lats_meters = 110.5744 * 1000 * delta_lats  #distance between latitude lines everywhere\n",
    "#                         dvdy = (v[:,:-1,:].values - v[:,1:,:].values).squeeze() / delta_lats_meters  #squeeze() removes dimensions of size 1 from an array\n",
    "#                         dvdy = np.vstack((dvdy, dvdy[-1,:]))  #duplicate the last row of dvdy to match original grid shape (and shape of dudx)\n",
    "            \n",
    "#                         conv_old = (dudx + dvdy) * -1 * 10**5  #manually calculated convergence from ERA5 u and v winds (times 10**5 1/s)\n",
    "#                         ds_conv = xr.Dataset(data_vars = dict(convergence = ([\"latitude\", \"longitude\"], conv_old)),\n",
    "#                                              coords = dict(latitude = (\"latitude\", u.latitude.values), \n",
    "#                                                            longitude = (\"longitude\", u.longitude.values)),\n",
    "#                                              attrs = dict(description = \"Manually calculated ERA5 convergence data\"))\n",
    "                        \n",
    "#                         conv = ds_conv.convergence.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "\n",
    "#                         # #grab all the ERA5 low-level convergence values and corresponding lats/lons within the given TIMPS ID box at the given hour\n",
    "#                         # conv = ds_era5.d.sel(time = case_date).sel(level = pres_lev) * -1    #convergence of the wind (1/s)\n",
    "#                         # conv = conv.sel(time = conv.time.dt.hour.isin(hr)) * 10**5  #convergence of the wind (times 10**5 1/s)\n",
    "#                         # conv = conv.sel(longitude = timps_lon_range, latitude = timps_lat_range)\n",
    "\n",
    "#                         lon, lat = np.meshgrid(conv.longitude, conv.latitude)\n",
    "#                         lons = lon.reshape(-1)\n",
    "#                         lats = lat.reshape(-1)\n",
    "#                         conv_values = conv.values.reshape(-1)\n",
    "\n",
    "#                         #add data from each hour as COLUMNS to corresponding df\n",
    "#                         #only add values to conv_df that haven't been already been added from a prior TIMPS ID for the given hour (so no duplicates!)\n",
    "#                         if ii != 0:  #if not the first TIMPS ID for the given hour\n",
    "#                             for xx, coord in enumerate(zip(lats,lons)):\n",
    "#                                 if coord in coords_check_list:\n",
    "#                                     print (f'Duplicate coordinate for Case {case_num}, replacing with NaN')\n",
    "#                                     lats[xx] = np.nan\n",
    "#                                     lons[xx] = np.nan\n",
    "#                                     conv_values[xx] = np.nan\n",
    "                        \n",
    "#                         coords_check_list += list(zip(lats, lons))  #append the lats/lons to the list as tuples (coordinate pairs)\n",
    "                        \n",
    "#                         conv_df = pd.concat((conv_df, pd.Series(conv_values)), axis = 1, ignore_index = True)\n",
    "#                         conv_lats_df = pd.concat((conv_lats_df, pd.Series(lats)), axis = 1, ignore_index = True)\n",
    "#                         #conv_lons_df = pd.concat((conv_lons_df, pd.Series(lons)), axis = 1, ignore_index = True)\n",
    "\n",
    "#                         timps_ds0.close()        \n",
    "\n",
    "#             if lifecycle == 1:\n",
    "#                 clc = 'Growth'\n",
    "#                 color = 'limegreen'\n",
    "#                 text_denom = 1\n",
    "#             elif lifecycle == 2:\n",
    "#                 clc = 'Mature'\n",
    "#                 color = 'darkred'\n",
    "#                 text_denom = 1.325\n",
    "#             elif lifecycle == 3:\n",
    "#                 clc = 'Decay'\n",
    "#                 color = 'blue'\n",
    "#                 text_denom = 1.96\n",
    "                \n",
    "#             if len(conv_df) == 0:  #no lifecycle data for the given case for the given hours\n",
    "#                 no_lifecycle_counter += 1\n",
    "#                 continue\n",
    "            \n",
    "#             conv_df = conv_df.values  #convert Pandas DataFrame to NumPy array\n",
    "#             conv_lats_df = conv_lats_df.values  #convert Pandas DataFrame to NumPy array\n",
    "            \n",
    "#             #mask NaN values in the Dataframes so that the numpy stat calculations work below\n",
    "#             conv_df_masked = np.ma.masked_where(np.isnan(conv_df), conv_df)\n",
    "#             conv_lats_df_masked = np.ma.masked_where(np.isnan(conv_lats_df), conv_lats_df)\n",
    "                \n",
    "#             ax.hist(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None,\n",
    "#                     histtype = 'step', align = 'mid', orientation = 'vertical', color = color,\n",
    "#                     linewidth = 2, label = clc)\n",
    "#                 #density = True returns a probability density: each bin will display the bin's raw count \n",
    "#                     #divided by the total number of counts and the bin width\n",
    "#                     #(density = counts / (sum(counts) * np.diff(bins))), so that the area under the \n",
    "#                     #histogram integrates to 1 (np.sum(density * np.diff(bins)) == 1)\n",
    "    \n",
    "#             cos_weights = np.sqrt(np.cos(conv_lats_df_masked * np.pi/180))   #cosine weights to apply to conv_df\n",
    "        \n",
    "#             conv_count = np.count_nonzero(~np.isnan(conv_df))\n",
    "#             conv_median = np.round(np.nanmedian(conv_df, axis = None), 2)\n",
    "#             conv_mean = np.round(np.nanmean(conv_df, axis = None), 2)                                        #non-weighted mean (1st moment)\n",
    "#             conv_wgt_mean = np.round(np.average(conv_df_masked, axis = None, weights = cos_weights), 2)      #cosine-weighted mean (1st moment)\n",
    "#             conv_std = np.round(np.std(conv_df_masked, axis = None), 2)                                      #standard deviation (2nd moment)\n",
    "#             conv_skew = np.round(scipy.stats.skew(conv_df_masked, axis = None, nan_policy = 'omit'), 4)      #skewness (3rd moment)\n",
    "#             conv_kurt = np.round(scipy.stats.kurtosis(conv_df_masked, axis = None, nan_policy = 'omit'), 4)  #kurtosis (4th moment)\n",
    "        \n",
    "#             ax.text(0.98, 0.875 / text_denom, \n",
    "#                     f'Count: {conv_count}\\nMedian: {conv_median}\\nMean: {conv_mean}\\nWeighted Mean: {conv_wgt_mean}\\nStandard Deviation: {conv_std}\\nSkewness: {conv_skew}\\nKurtosis: {conv_kurt}\\n', \n",
    "#                     transform = ax.transAxes, horizontalalignment = 'right', verticalalignment = 'center', \n",
    "#                     fontsize = 16, fontweight = 'bold', color = color)\n",
    "        \n",
    "#         ax.set_title('Case %i (%s-%s-%s) ERA5 %i hPa Convergence PDFs (Convection-Relative)' % (case_num, case_date[:4], case_date[4:6], case_date[6:], pres_lev))\n",
    "        \n",
    "#         if no_lifecycle_counter == 3:  #no lifecycle data for the given case for the given hours\n",
    "#             ax.text(0.5, 0.5, f'Unidentifiable lifecycle stage throughout\\nthe CPEX-CV sampling time for Case {case_num}', \n",
    "#                     horizontalalignment = 'center', verticalalignment = 'center', \n",
    "#                     fontsize = 30, bbox = {'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "#             ax.set_xticklabels([])\n",
    "#             ax.set_yticklabels([])\n",
    "#         else:\n",
    "#             ax.axvline(x = 0, color = 'k', linestyle = '--', alpha = 0.5)\n",
    "#             ax.set_xlabel('Convergence [10$^{-5}$ s$^{-1}$]')\n",
    "#             ax.set_ylabel('Prob(Convergence)')\n",
    "#             ax.set_xlim([-15,15])\n",
    "#             ax.set_xticks(np.arange(-15, 15.1, 2))\n",
    "#             ax.set_yticks(np.arange(0, 0.401, 0.05))\n",
    "#             ax.grid(axis = 'y')\n",
    "#             ax.legend(title = 'Lifecycle Stage', title_fontproperties = {'weight': 'bold', 'size': 18}, loc = 'upper left')\n",
    "                    \n",
    "#         #plt.tight_layout()\n",
    "#         #plt.subplots_adjust(wspace = 0.1)\n",
    "\n",
    "#         #save the figure\n",
    "#         plot_save_name = f'Case{case_num}_{pres_lev}hPa_convergence_PDFs_convection_relative.png'\n",
    "#         plt.savefig(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/Convection_Relative/Smoothed_ERA5_winds', plot_save_name), bbox_inches = 'tight')\n",
    "#         #plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "#         #plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "#         plt.close()\n",
    "#         plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "#         ##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "#         ##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "#         im = Image.open(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/Convection_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "\n",
    "#         try:\n",
    "#             im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "#         except:\n",
    "#             #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "#             #though this line will have isolated, extremely minor image effects due to \n",
    "#             #only using 256 colors instead of the 3-element RGB scale\n",
    "#             im2 = im.convert('P')\n",
    "\n",
    "#         im2.save(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/Convection_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "#         im.close()\n",
    "#         im2.close()\n",
    "    \n",
    "#     print (f'Case {case_num} convergence PDF plots complete!\\n')\n",
    "\n",
    "# ds_era5.close()\n",
    "\n",
    "# tend = time.time()\n",
    "# print (f'This script took {np.round((tend - tstart) / 60, 1)} minutes to complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14b771",
   "metadata": {},
   "source": [
    "### THE FOLLOWING CELL PLOTS A 1-PANEL PLOT OF AEW-RELATIVE CONVERGENCE PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #THIS CELL PLOTS A 1-PANEL PLOT OF AEW-RELATIVE CONVERGENCE PDFs\n",
    "\n",
    "# #For each CPEX-CV case, calculate and plot domain-PDFs of low- (975 hPa) and mid- (700 hPa) level convergence \n",
    "# #(display median, mean, standard deviation, skewness, and kurtosis as well), with the domain being a \n",
    "# #10-by-10 degree box around the AEW center for the given case (get from Quintonâ€™s AEW tracker: https://osf.io/jnv5u, https://zenodo.org/records/13350860)\n",
    "#     #partition the data into the 2 sectors of the AEW for the given case (ahead/behind the AEW)\n",
    "#     #for calculating domain-mean convergence, cosine-weight each grid box value (see AOS 573 material)\n",
    "    \n",
    "#     #WON'T NEED TO KEEP TRACK OF WHICH GRID CELLS YOU HAVE ALREADY ADDED CONVERGENCE FOR, SINCE YOU'RE NOT\n",
    "#         #WORKING WITH MULTIPLE AEWs AT A GIVEN HOUR (LIKE YOU WERE WITH MULTIPLE TIMPS IDs PER HOUR)\n",
    "        \n",
    "#     #Also don't need to split up convergence PDFs by convective lifecycle (just ahead/behind the AEW), because\n",
    "#         #it would be difficult to relate convergence of an AEW region to convective lifecycle, since one\n",
    "#         #AEW region could (and likely often does) have convective systems that are in different lifecycle stages\n",
    "\n",
    "# #set some baseline plot displays\n",
    "\n",
    "# #matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "# matplotlib.rcParams['axes.labelsize'] = 18\n",
    "# matplotlib.rcParams['axes.titlesize'] = 18\n",
    "# matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "# matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "# matplotlib.rcParams['xtick.labelsize'] = 18\n",
    "# matplotlib.rcParams['ytick.labelsize'] = 18\n",
    "# matplotlib.rcParams['legend.fontsize'] = 16\n",
    "# #matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "# #matplotlib.rcParams['axes.facecolor'] = 'w'\n",
    "# matplotlib.rcParams['font.family'] = 'arial'\n",
    "# matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "        \n",
    "# #Dropsonde data\n",
    "# drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "# df_drop = pd.read_csv(drop_metric_filepath)\n",
    "\n",
    "# #ERA5 data\n",
    "# era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "# era5_path = os.path.join(era5_folder, 'CPEXCV_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "# ds_era5 = xr.open_dataset(era5_path)\n",
    "\n",
    "# #AEW tracker data, 6-hourly (Quintonâ€™s AEW tracker: https://osf.io/jnv5u, https://zenodo.org/records/13350860)\n",
    "# aew_folder = os.path.join(os.getcwd(), 'AEW_Tracker_Data')\n",
    "# aew_path = os.path.join(aew_folder, 'AEW_tracks_post_processed_year_2022.nc')\n",
    "# ds_aew = xr.open_dataset(aew_path)\n",
    "\n",
    "# #TIMPS data\n",
    "# timps_folder = os.path.join(os.getcwd(), 'TIMPS_data')\n",
    "\n",
    "# conv_bins = np.arange(-15,15.1,0.5)\n",
    "\n",
    "# for case_num in case_dict_conv_aew.keys():\n",
    "\n",
    "#     print (f'Case {case_num} convergence PDF plots in progress...')\n",
    "    \n",
    "#     aew_system_index_use = None\n",
    "    \n",
    "#     df_drop_case = df_drop[df_drop['Case'] == case_num].copy()\n",
    "    \n",
    "#     case_date = case_dict_conv_aew[case_num][0]\n",
    "#     case_hours = case_dict_conv_aew[case_num][1]\n",
    "#     case_timps_ids = df_drop_case['TIMPS ID'].unique()\n",
    "    \n",
    "#     if len(case_timps_ids) == 1 and pd.isnull(case_timps_ids[0]):  #no TIMPS IDs for the given case\n",
    "#         continue\n",
    "        \n",
    "#     aews_at_given_date = ds_aew.sel(time = case_date)\n",
    "#     day_after = datetime.strptime(case_date, '%Y%m%d') + timedelta(days = 1)  #day after case_date\n",
    "#     aews_at_given_date_plus1 = ds_aew.sel(time = datetime.strftime(day_after, '%Y%m%d'))\n",
    "    \n",
    "#     #match the convective case to the nearest AEW (longitudinally) from Quinton's tracker\n",
    "#         #and confirm that each TIMPS ID for the given case matches to the same AEW (sanity check)\n",
    "#     case_hour_nearest_multiple6 = case_hours[np.nanargmin(np.abs(np.array(case_hours) % 6))]  #find the hour closest to a multiple of 6 (AEW tracker is only in 6-hourly intervals)\n",
    "#     nearest_6_to_case_hour = 6 * round(case_hour_nearest_multiple6 / 6)  #find the multiple of 6 closest to case_hour_nearest_multiple6\n",
    "    \n",
    "#     for ii, unique_timps_id in enumerate(case_timps_ids):\n",
    "        \n",
    "#         unique_timps_id = str(int(unique_timps_id))\n",
    "        \n",
    "#         timps_filepath = None\n",
    "#         for filename in os.listdir(timps_folder):\n",
    "#             if unique_timps_id in filename:\n",
    "#                 timps_filepath = os.path.join(timps_folder, filename)\n",
    "#                 break\n",
    "\n",
    "#         if timps_filepath == None:\n",
    "#             sys.exit(f'Could not find TIMPS file for TIMP ID {unique_timps_id}')\n",
    "#         else:\n",
    "#             timps_ds0 = xr.open_dataset(timps_filepath)\n",
    "#             timps_ds0 = timps_ds0.sel(time = case_date)\n",
    "#             timps_ds = timps_ds0.sel(time = timps_ds0.time.dt.hour.isin(case_hour_nearest_multiple6))   #gives 2 times: minute = 0 and minute = 30\n",
    "#             timps_ds = timps_ds.sel(time = timps_ds.time.dt.minute.isin(0))    #grab the time on the hour to match with ERA5 and AEW tracker\n",
    "\n",
    "#             if len(timps_ds.gmd) == 0:\n",
    "#                 print (f'{str(case_hour_nearest_multiple6).zfill(2)} UTC is out of range of the TIMPS ID range ({timps_ds0.time[0].values.astype(str)[:-10]} - {timps_ds0.time[-1].values.astype(str)[:-10]})')\n",
    "#                 timps_ds0.close()\n",
    "#                 continue\n",
    "\n",
    "#             #timps_weighted_lat = timps_ds.centlatwgt.item()\n",
    "#             timps_weighted_lon = timps_ds.centlonwgt.item()\n",
    "            \n",
    "#             aews_at_given_hour = aews_at_given_date.sel(time = aews_at_given_date.time.dt.hour.isin(nearest_6_to_case_hour))\n",
    "            \n",
    "#             lon_difs = aews_at_given_hour['AEW_lon_smooth'][:,0] - timps_weighted_lon\n",
    "#             aew_system_index = np.nanargmin(np.abs(lon_difs).values)   #np.argmin() would grab a NaN value!\n",
    "            \n",
    "#             if aew_system_index_use != None:\n",
    "#                 if case_num != 22:\n",
    "#                     assert aew_system_index == aew_system_index_use, 'TIMPS IDs for the given case are not matching to the same AEW'\n",
    "#                 else:  #Case 22 has 2 TIMPS IDs which incorrectly match to different AEWs;\n",
    "#                        #the 2nd TIMPS ID (304352) matches to the correct AEW, so we're forcing the code to choose that AEW here (not ideal, but just one case we need to do this for)\n",
    "#                     aew_system_index_use = aew_system_index * 1   #the number of the AEW in the AEW tracker file (using * 1 so that aew_system_index_use variable doesn't point (i.e., isn't tied to) to the same reference as aew_system_index)\n",
    "#                     print (f\"Actual AEW system: {aew_system_index_use + 1}, AEW central longitude and strength at {case_date} {nearest_6_to_case_hour} UTC: {np.round(aews_at_given_hour['AEW_lon_smooth'][aew_system_index_use,0].item(), 2)}, {aews_at_given_hour['AEW_strength'][aew_system_index_use,0].item()} s-1\")\n",
    "#             else:\n",
    "#                 aew_system_index_use = aew_system_index * 1   #the number of the AEW in the AEW tracker file (using * 1 so that aew_system_index_use variable doesn't point (i.e., isn't tied to) to the same reference as aew_system_index)\n",
    "#                 print (f\"AEW system: {aew_system_index_use + 1}, AEW central longitude and strength at {case_date} {nearest_6_to_case_hour} UTC: {np.round(aews_at_given_hour['AEW_lon_smooth'][aew_system_index_use,0].item(), 2)}, {aews_at_given_hour['AEW_strength'][aew_system_index_use,0].item()} s-1\")\n",
    "                \n",
    "#             timps_ds0.close()\n",
    "            \n",
    "#     matched_aew_ds = aews_at_given_date.isel(system = aew_system_index_use)\n",
    "#     matched_aew_ds_dayafter = aews_at_given_date_plus1.isel(system = aew_system_index_use)\n",
    "\n",
    "    \n",
    "#     #plot time evolution of AEW strength for the AEW associated with the given case (put dotted line at the AEW time closest to the sampling of the case)\n",
    "#     group_fig0 = plt.figure(figsize = (24, 12))   #initialize the time series figure for the given case\n",
    "#     ax0 = group_fig0.add_subplot(1, 1, 1)\n",
    "#     #aew_strength = ds_aew['AEW_strength'][aew_system_index_use, :] * 10**5\n",
    "#     #aew_strength_nonans = aew_strength[~np.isnan(aew_strength)]\n",
    "#     ax0.plot(ds_aew.time, ds_aew['AEW_strength'][aew_system_index_use, :] * 10**5, linewidth = 2, linestyle = '-', color = 'k')\n",
    "#     ax0.axvline(x = aews_at_given_hour.time.values[0], color = 'k', linestyle = '--', alpha = 0.5)\n",
    "    \n",
    "#     ax0.set_title(f'Case {case_num} AEW Strength Evolution')\n",
    "#     ax0.set_xlabel('Time [UTC]')\n",
    "#     ax0.set_ylabel('Non-divergent Curvature Vorticity [10$^{-5}$ s$^{-1}$]')\n",
    "#     #ax0.set_xlim([aew_strength_nonans.time.values[0], aew_strength_nonans.time.values[-1]])\n",
    "#     #ax0.set_xticks(ds_aew.time[::60])\n",
    "#     ax0.set_ylim([0,2])\n",
    "#     ax0.set_yticks(np.arange(0, 2.01, 0.2))\n",
    "\n",
    "#     #save the figure\n",
    "#     plot_save_name = f'Case{case_num}_AEW_strength_time_series.png'\n",
    "#     plt.savefig(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/AEW_Relative/AEW_strength_evolution', plot_save_name), bbox_inches = 'tight')\n",
    "#     #plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "#     #plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "#     plt.close()\n",
    "#     plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "#     ##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "#     ##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "#     im = Image.open(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/AEW_Relative/AEW_strength_evolution', plot_save_name))\n",
    "\n",
    "#     try:\n",
    "#         im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "#     except:\n",
    "#         #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "#         #though this line will have isolated, extremely minor image effects due to \n",
    "#         #only using 256 colors instead of the 3-element RGB scale\n",
    "#         im2 = im.convert('P')\n",
    "\n",
    "#     im2.save(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/AEW_Relative/AEW_strength_evolution', plot_save_name))\n",
    "#     im.close()\n",
    "#     im2.close()\n",
    "    \n",
    "\n",
    "#     #plot convergence PDFs at low- and mid-levels\n",
    "#     for pres_lev in pressures_to_plot_conv:\n",
    "        \n",
    "#         group_fig = plt.figure(figsize = (12, 12))   #initialize the convergence PDF figure for the given case\n",
    "#         ax = group_fig.add_subplot(1, 1, 1)\n",
    "        \n",
    "#         for sector in ['full', 'south_ahead', 'ahead', 'north_ahead', 'south_behind', 'behind', 'north_behind']:\n",
    "\n",
    "#             conv_df = pd.DataFrame()\n",
    "#             conv_lats_df = pd.DataFrame()\n",
    "#             #conv_lons_df = pd.DataFrame()\n",
    "\n",
    "#             for hr in case_hours:\n",
    "\n",
    "#                 #grab/calculate the AEW centroid coordinates for the given hr\n",
    "#                 if hr % 6 == 0:  #if hr is a multiple of 6, then don't need to interpolate the AEW centroid at all\n",
    "#                     matched_aew_smoothed_lat = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(hr)).AEW_lat_smooth.item()\n",
    "#                     matched_aew_smoothed_lon = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(hr)).AEW_lon_smooth.item()\n",
    "\n",
    "#                 else:  #interpolate the centroid of the matched AEW to the given hr\n",
    "\n",
    "#                     #find the multiple of 6 directly below/equal to hr; this will be your starting hour to interpolate the AEW centroid to hr\n",
    "#                     nearest_6_below_hr = 6 * (hr // 6)\n",
    "#                     nearest_6_above_hr = nearest_6_below_hr + 6  #this will be your ending hour to interpolate the AEW centroid to hr\n",
    "\n",
    "#                     matched_aew_start_hr_ds = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(nearest_6_below_hr))\n",
    "\n",
    "#                     if nearest_6_above_hr == 24:  #grab 00 UTC from the next day\n",
    "#                         matched_aew_end_hr_ds = matched_aew_ds_dayafter.sel(time = matched_aew_ds_dayafter.time.dt.hour.isin(0))\n",
    "#                     else:\n",
    "#                         matched_aew_end_hr_ds = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(nearest_6_above_hr))\n",
    "\n",
    "#                     matched_aew_start_lat = matched_aew_start_hr_ds.AEW_lat_smooth.item()\n",
    "#                     matched_aew_start_lon = matched_aew_start_hr_ds.AEW_lon_smooth.item()\n",
    "#                     matched_aew_end_lat = matched_aew_end_hr_ds.AEW_lat_smooth.item()\n",
    "#                     matched_aew_end_lon = matched_aew_end_hr_ds.AEW_lon_smooth.item()\n",
    "\n",
    "#                     #AEW centroid moves XX degrees per hour\n",
    "#                     lat_per_hour = (matched_aew_end_lat - matched_aew_start_lat) / 6\n",
    "#                     lon_per_hour = (matched_aew_end_lon - matched_aew_start_lon) / 6\n",
    "\n",
    "#                     #interpolated AEW centroid at the given hr\n",
    "#                     matched_aew_smoothed_lat = matched_aew_start_lat + (lat_per_hour * (hr % 6))\n",
    "#                     matched_aew_smoothed_lon = matched_aew_start_lon + (lon_per_hour * (hr % 6))\n",
    "\n",
    "#                 #create 10-by-10 degree box around the (interpolated) AEW centroid at the given hr\n",
    "#                 aew_lat_range = slice(matched_aew_smoothed_lat + 5, matched_aew_smoothed_lat - 5)\n",
    "#                 aew_lon_range = slice(matched_aew_smoothed_lon - 5, matched_aew_smoothed_lon + 5)\n",
    "\n",
    "#                 #grab all the ERA5 low-/mid-level convergence values and corresponding lats/lons within the given AEW box at the given hour\n",
    "#                     #and separate the data into the 2 sectors (ahead/behind) of the AEW\n",
    "#                 v700 = ds_era5.v.sel(time = case_date).sel(level = 700)\n",
    "#                 v700 = v700.sel(time = v700.time.dt.hour.isin(hr))\n",
    "#                 v700 = mpcalc.smooth_gaussian(v700, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                 v700 = v700.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "#                 #manually calculating convergence from ERA5 u and v winds (recommended by Brandon Wolding via George Kiladis)\n",
    "#                 u = ds_era5.u.sel(time = case_date).sel(level = pres_lev)\n",
    "#                 u = u.sel(time = u.time.dt.hour.isin(hr))\n",
    "#                 u = mpcalc.smooth_gaussian(u, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                 #u = u.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "#                 v = ds_era5.v.sel(time = case_date).sel(level = pres_lev)\n",
    "#                 v = v.sel(time = v.time.dt.hour.isin(hr))\n",
    "#                 v = mpcalc.smooth_gaussian(v, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "#                 #v = v.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "#                 delta_lons = 0.25   #ERA5 lat/lon resolution is 0.25 degrees\n",
    "#                 delta_lons_meters = (111.3195 * 1000 * delta_lons) * np.cos(u.latitude.values * np.pi/180)  #distance between longitude lines at equator is 111.3195 km and cosine weighting this distance by latitude\n",
    "#                 dudx = (u[:,:,1:].values - u[:,:,:-1].values).squeeze() / np.expand_dims(np.abs(delta_lons_meters), axis=1)  #squeeze() removes dimensions of size 1 from an array, and expand_dims() inserts a new axis that will appear at the axis position\n",
    "#                 dudx = np.column_stack((dudx, dudx[:,-1]))  #duplicate the last column of dudx to match original grid shape (and shape of dvdy)\n",
    "\n",
    "#                 delta_lats = 0.25\n",
    "#                 delta_lats_meters = 110.5744 * 1000 * delta_lats  #distance between latitude lines everywhere\n",
    "#                 dvdy = (v[:,:-1,:].values - v[:,1:,:].values).squeeze() / delta_lats_meters  #squeeze() removes dimensions of size 1 from an array\n",
    "#                 dvdy = np.vstack((dvdy, dvdy[-1,:]))  #duplicate the last row of dvdy to match original grid shape (and shape of dudx)\n",
    "\n",
    "#                 conv_old = (dudx + dvdy) * -1 * 10**5  #manually calculated convergence from ERA5 u and v winds (times 10**5 1/s)\n",
    "#                 ds_conv = xr.Dataset(data_vars = dict(convergence = ([\"latitude\", \"longitude\"], conv_old)),\n",
    "#                                      coords = dict(latitude = (\"latitude\", u.latitude.values), \n",
    "#                                                    longitude = (\"longitude\", u.longitude.values)),\n",
    "#                                      attrs = dict(description = \"Manually calculated ERA5 convergence data\"))\n",
    "                \n",
    "#                 conv = ds_conv.convergence.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "#                 # #using convergence variable from ERA5\n",
    "#                 # conv = ds_era5.d.sel(time = case_date).sel(level = pres_lev) * -1    #convergence of the wind (1/s)\n",
    "#                 # conv = conv.sel(time = conv.time.dt.hour.isin(hr)) * 10**5  #convergence of the wind (times 10**5 1/s)\n",
    "#                 # conv = conv.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "#                 #filter the convergence data by the 700-hPa v-component of the wind  \n",
    "#                     #v <= 0: the grid point is ahead of the AEW center \n",
    "#                     #v > 0: the grid point is behind the AEW center\n",
    "#                         #This dynamically defines ahead/behind AEW centers, which is especially practical for asymmetric AEWs!\n",
    "#                 if sector == 'full':\n",
    "#                     conv = conv * 1\n",
    "#                     clc = 'Full'\n",
    "#                     color = 'k'\n",
    "#                     text_denom = 0\n",
    "#                 elif sector == 'south_ahead':\n",
    "#                     conv = conv.where(conv.latitude < matched_aew_smoothed_lat).where(v700 <= 0)  #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "#                     clc = 'Ahead (South)'\n",
    "#                     color = 'skyblue'\n",
    "#                     text_denom = 0.14\n",
    "#                 elif sector == 'ahead':\n",
    "#                     conv = conv.where(v700 <= 0)  #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "#                     clc = 'Ahead'\n",
    "#                     color = 'dodgerblue'\n",
    "#                     text_denom = 0.28\n",
    "#                 elif sector == 'north_ahead':\n",
    "#                     conv = conv.where(conv.latitude >= matched_aew_smoothed_lat).where(v700 <= 0)  #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "#                     clc = 'Ahead (North)'\n",
    "#                     color = 'navy'\n",
    "#                     text_denom = 0.42\n",
    "#                 elif sector == 'south_behind':\n",
    "#                     conv = conv.where(conv.latitude < matched_aew_smoothed_lat).where(v700 > 0)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "#                     clc = 'Behind (South)'\n",
    "#                     color = 'lightsalmon'\n",
    "#                     text_denom = 0.56\n",
    "#                 elif sector == 'behind':\n",
    "#                     conv = conv.where(v700 > 0)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "#                     clc = 'Behind'\n",
    "#                     color = 'red'\n",
    "#                     text_denom = 0.7\n",
    "#                 elif sector == 'north_behind':\n",
    "#                     conv = conv.where(conv.latitude >= matched_aew_smoothed_lat).where(v700 > 0)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "#                     clc = 'Behind (North)'\n",
    "#                     color = 'darkred'\n",
    "#                     text_denom = 0.84\n",
    "\n",
    "#                 lon, lat = np.meshgrid(conv.longitude, conv.latitude)\n",
    "#                 lats = lat.reshape(-1)\n",
    "#                 conv_values = conv.values.reshape(-1)\n",
    "\n",
    "#                 #add data from each hour as COLUMNS to corresponding df\n",
    "#                 conv_df = pd.concat((conv_df, pd.Series(conv_values)), axis = 1, ignore_index = True)\n",
    "#                 conv_lats_df = pd.concat((conv_lats_df, pd.Series(lats)), axis = 1, ignore_index = True)\n",
    "#                 #conv_lons_df = pd.concat((conv_lons_df, pd.Series(lons)), axis = 1, ignore_index = True)         \n",
    "                    \n",
    "#             # if sector == 'ahead':\n",
    "#             #     clc = 'Ahead'\n",
    "#             #     color = 'darkred'\n",
    "#             #     text_denom = 1\n",
    "#             # elif sector == 'behind':\n",
    "#             #     clc = 'Behind'\n",
    "#             #     color = 'blue'\n",
    "#             #     text_denom = 1.325\n",
    "\n",
    "#             conv_df = conv_df.values  #convert Pandas DataFrame to NumPy array\n",
    "#             conv_lats_df = conv_lats_df.values  #convert Pandas DataFrame to NumPy array\n",
    "\n",
    "#             #mask NaN values in the Dataframes so that the numpy stat calculations work below\n",
    "#             conv_df_masked = np.ma.masked_where(np.isnan(conv_df), conv_df)\n",
    "#             conv_lats_df_masked = np.ma.masked_where(np.isnan(conv_lats_df), conv_lats_df)\n",
    "\n",
    "#             #line plot histogram (clearer to interpret than \"step\" histogram below)\n",
    "#             hist, bins = np.histogram(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None)\n",
    "#             bin_centers = (bins[:-1] + bins[1:]) / 2  # Midpoints of the bins\n",
    "#             ax.plot(bin_centers, hist, linewidth = 2, linestyle = '-', color = color, label = clc)\n",
    "\n",
    "#             # #normal \"step\" histogram\n",
    "#             # ax.hist(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None,\n",
    "#             #         histtype = 'step', align = 'mid', orientation = 'vertical', color = color,\n",
    "#             #         linewidth = 2, label = clc)\n",
    "#             #     #density = True returns a probability density: each bin will display the bin's raw count \n",
    "#             #         #divided by the total number of counts times the bin width\n",
    "#             #         #(density = counts / (sum(counts) * np.diff(bins))), so that the area under the \n",
    "#             #         #histogram integrates to 1 (np.sum(density * np.diff(bins)) == 1)\n",
    "\n",
    "#             cos_weights = np.sqrt(np.cos(conv_lats_df_masked * np.pi/180))   #cosine weights to apply to conv_df_masked\n",
    "\n",
    "#             conv_count = np.count_nonzero(~np.isnan(conv_df))\n",
    "#             conv_median = np.round(np.nanmedian(conv_df, axis = None), 2)\n",
    "#             conv_mean = np.round(np.nanmean(conv_df, axis = None), 2)                                        #non-weighted mean (1st moment)\n",
    "#             conv_wgt_mean = np.round(np.average(conv_df_masked, axis = None, weights = cos_weights), 2)      #cosine-weighted mean (1st moment)\n",
    "#             conv_std = np.round(np.std(conv_df_masked, axis = None), 2)                                      #standard deviation (2nd moment)\n",
    "#             conv_skew = np.round(scipy.stats.skew(conv_df_masked, axis = None, nan_policy = 'omit'), 4)      #skewness (3rd moment)\n",
    "#             conv_kurt = np.round(scipy.stats.kurtosis(conv_df_masked, axis = None, nan_policy = 'omit'), 4)  #kurtosis (4th moment)\n",
    "\n",
    "#             # ax.text(0.98, 0.875 / text_denom, \n",
    "#             #         f'Count: {conv_count}\\nMedian: {conv_median}\\nMean: {conv_mean}\\nWeighted Mean: {conv_wgt_mean}\\nStandard Deviation: {conv_std}\\nSkewness: {conv_skew}\\nKurtosis: {conv_kurt}\\n', \n",
    "#             #         transform = ax.transAxes, horizontalalignment = 'right', verticalalignment = 'center', \n",
    "#             #         fontsize = 16, fontweight = 'bold', color = color)\n",
    "#             ax.text(0.98, 0.92 - text_denom, \n",
    "#                     f'Count: {conv_count}\\nMedian: {conv_median}\\nMean: {conv_mean}\\nWeighted Mean: {conv_wgt_mean}\\nStandard Deviation: {conv_std}\\nSkewness: {conv_skew}\\nKurtosis: {conv_kurt}\\n', \n",
    "#                     transform = ax.transAxes, horizontalalignment = 'right', verticalalignment = 'center', \n",
    "#                     fontsize = 12, fontweight = 'bold', color = color)\n",
    "        \n",
    "#         ax.set_title('Case %i (%s-%s-%s) ERA5 %i hPa Convergence PDFs (AEW-Relative)' % (case_num, case_date[:4], case_date[4:6], case_date[6:], pres_lev))\n",
    "        \n",
    "#         if case_num == 20:\n",
    "#             ax.text(0.5, 0.5, f'Case {case_num} not being correctly\\nmatched to the appropriate AEW', \n",
    "#             transform = ax.transAxes, horizontalalignment = 'center', verticalalignment = 'center', \n",
    "#             fontsize = 30, bbox = {'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "        \n",
    "#         ax.axvline(x = 0, color = 'k', linestyle = '--', alpha = 0.5)\n",
    "#         ax.set_xlabel('Convergence [10$^{-5}$ s$^{-1}$]')\n",
    "#         ax.set_ylabel('Prob(Convergence)')\n",
    "#         ax.set_xlim([-11,11])\n",
    "#         ax.set_xticks(np.arange(-11, 11.1, 2))\n",
    "#         ax.set_ylim(bottom = 0)\n",
    "#         #ax.set_yticks(np.arange(0, 0.601, 0.05))\n",
    "#         ax.grid(axis = 'y')\n",
    "#         ax.legend(title = 'Sector of AEW', title_fontproperties = {'weight': 'bold', 'size': 18}, loc = 'upper left')\n",
    "                    \n",
    "#         #plt.tight_layout()\n",
    "#         #plt.subplots_adjust(wspace = 0.1)\n",
    "\n",
    "#         #save the figure\n",
    "#         plot_save_name = f'0_Case{case_num}_{pres_lev}hPa_convergence_PDFs_AEW_relative_all_sectors.png'\n",
    "#         plt.savefig(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/AEW_Relative/Smoothed_ERA5_winds', plot_save_name), bbox_inches = 'tight')\n",
    "#         #plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "#         #plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "#         plt.close()\n",
    "#         plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "#         ##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "#         ##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "#         im = Image.open(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/AEW_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "\n",
    "#         try:\n",
    "#             im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "#         except:\n",
    "#             #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "#             #though this line will have isolated, extremely minor image effects due to \n",
    "#             #only using 256 colors instead of the 3-element RGB scale\n",
    "#             im2 = im.convert('P')\n",
    "\n",
    "#         im2.save(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/AEW_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "#         im.close()\n",
    "#         im2.close()\n",
    "    \n",
    "#     print (f'Case {case_num} convergence PDF plots complete!\\n')\n",
    "\n",
    "# ds_era5.close()\n",
    "# ds_aew.close()\n",
    "\n",
    "# tend = time.time()\n",
    "# print (f'This script took {np.round((tend - tstart) / 60, 1)} minutes to complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf94825-a64f-4e42-87c7-6b844d25cd32",
   "metadata": {},
   "source": [
    "### THE FOLLOWING CELL CREATES PLOTS OF AEW-RELATIVE CONVERGENCE PDFs COMPARING 2 CASES (1 PANEL PER AEW SECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6df552-78a1-40ed-a851-29874dfe5315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CELL PLOTS A 1-PANEL PLOT OF AEW-RELATIVE CONVERGENCE PDFs\n",
    "\n",
    "#For each CPEX-CV case, calculate and plot domain-PDFs of low- (975 hPa) and mid- (700 hPa) level convergence \n",
    "#(display median, mean, standard deviation, skewness, and kurtosis as well), with the domain being a \n",
    "#10-by-10 degree box around the AEW center for the given case (get from Quintonâ€™s AEW tracker: https://osf.io/jnv5u, https://zenodo.org/records/13350860)\n",
    "    #partition the data into the 2 sectors of the AEW for the given case (ahead/behind the AEW)\n",
    "    #for calculating domain-mean convergence, cosine-weight each grid box value (see AOS 573 material)\n",
    "    \n",
    "    #WON'T NEED TO KEEP TRACK OF WHICH GRID CELLS YOU HAVE ALREADY ADDED CONVERGENCE FOR, SINCE YOU'RE NOT\n",
    "        #WORKING WITH MULTIPLE AEWs AT A GIVEN HOUR (LIKE YOU WERE WITH MULTIPLE TIMPS IDs PER HOUR)\n",
    "        \n",
    "    #Also don't need to split up convergence PDFs by convective lifecycle (just ahead/behind the AEW), because\n",
    "        #it would be difficult to relate convergence of an AEW region to convective lifecycle, since one\n",
    "        #AEW region could (and likely often does) have convective systems that are in different lifecycle stages\n",
    "\n",
    "#set some baseline plot displays\n",
    "\n",
    "#matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 18\n",
    "matplotlib.rcParams['axes.titlesize'] = 18\n",
    "matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "matplotlib.rcParams['axes.titleweight'] = 'bold'\n",
    "matplotlib.rcParams['xtick.labelsize'] = 18\n",
    "matplotlib.rcParams['ytick.labelsize'] = 18\n",
    "matplotlib.rcParams['legend.fontsize'] = 16\n",
    "#matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "#matplotlib.rcParams['axes.facecolor'] = 'w'\n",
    "matplotlib.rcParams['font.family'] = 'arial'\n",
    "matplotlib.rcParams['hatch.linewidth'] = 0.3\n",
    "        \n",
    "#Dropsonde data\n",
    "drop_metric_filepath = os.path.join(os.getcwd(), 'Dropsonde_Metric_Calculations_CPEXCV.csv')\n",
    "df_drop = pd.read_csv(drop_metric_filepath)\n",
    "\n",
    "#ERA5 data\n",
    "era5_folder = os.path.join(os.getcwd(), 'ERA5_Reanalysis_Data')\n",
    "era5_path = os.path.join(era5_folder, 'CPEXCV_ERA5_Reanalysis_Hourly_Pressure.nc')\n",
    "ds_era5 = xr.open_dataset(era5_path)\n",
    "\n",
    "#AEW tracker data, 6-hourly (Quintonâ€™s AEW tracker: https://osf.io/jnv5u, https://zenodo.org/records/13350860)\n",
    "aew_folder = os.path.join(os.getcwd(), 'AEW_Tracker_Data')\n",
    "aew_path = os.path.join(aew_folder, 'AEW_tracks_post_processed_year_2022.nc')\n",
    "ds_aew = xr.open_dataset(aew_path)\n",
    "\n",
    "#TIMPS data\n",
    "timps_folder = os.path.join(os.getcwd(), 'TIMPS_data')\n",
    "\n",
    "conv_bins = np.arange(-15,15.1,0.5)\n",
    "\n",
    "#plot convergence PDFs at low- and mid-levels for each AEW sector for each case\n",
    "for pres_lev in pressures_to_plot_conv:\n",
    "    for sector in ['full', 'south_ahead', 'ahead', 'north_ahead', 'south_behind', 'behind', 'north_behind', 'north']:\n",
    "    \n",
    "        group_fig = plt.figure(figsize = (12, 12))   #initialize the convergence PDF figure for the given case\n",
    "        ax = group_fig.add_subplot(1, 1, 1)\n",
    "        \n",
    "        for case_num in case_dict_conv_aew.keys():\n",
    "        \n",
    "            #print (f'Case {case_num} convergence PDF plots in progress...')\n",
    "            \n",
    "            aew_system_index_use = None\n",
    "            \n",
    "            df_drop_case = df_drop[df_drop['Case'] == case_num].copy()\n",
    "            \n",
    "            case_date = case_dict_conv_aew[case_num][0]\n",
    "            case_hours = case_dict_conv_aew[case_num][1]\n",
    "            case_timps_ids = df_drop_case['TIMPS ID'].unique()\n",
    "            \n",
    "            if len(case_timps_ids) == 1 and pd.isnull(case_timps_ids[0]):  #no TIMPS IDs for the given case\n",
    "                continue\n",
    "            \n",
    "            aews_at_given_date = ds_aew.sel(time = case_date)\n",
    "            day_after = datetime.strptime(case_date, '%Y%m%d') + timedelta(days = 1)  #day after case_date\n",
    "            aews_at_given_date_plus1 = ds_aew.sel(time = datetime.strftime(day_after, '%Y%m%d'))\n",
    "            \n",
    "            #match the convective case to the nearest AEW (longitudinally) from Quinton's tracker\n",
    "                #and confirm that each TIMPS ID for the given case matches to the same AEW (sanity check)\n",
    "            case_hour_nearest_multiple6 = case_hours[np.nanargmin(np.abs(np.array(case_hours) % 6))]  #find the hour closest to a multiple of 6 (AEW tracker is only in 6-hourly intervals)\n",
    "            nearest_6_to_case_hour = 6 * round(case_hour_nearest_multiple6 / 6)  #find the multiple of 6 closest to case_hour_nearest_multiple6\n",
    "        \n",
    "            for ii, unique_timps_id in enumerate(case_timps_ids):\n",
    "                \n",
    "                unique_timps_id = str(int(unique_timps_id))\n",
    "                \n",
    "                timps_filepath = None\n",
    "                for filename in os.listdir(timps_folder):\n",
    "                    if unique_timps_id in filename:\n",
    "                        timps_filepath = os.path.join(timps_folder, filename)\n",
    "                        break\n",
    "        \n",
    "                if timps_filepath == None:\n",
    "                    sys.exit(f'Could not find TIMPS file for TIMP ID {unique_timps_id}')\n",
    "                else:\n",
    "                    timps_ds0 = xr.open_dataset(timps_filepath)\n",
    "                    timps_ds0 = timps_ds0.sel(time = case_date)\n",
    "                    timps_ds = timps_ds0.sel(time = timps_ds0.time.dt.hour.isin(case_hour_nearest_multiple6))   #gives 2 times: minute = 0 and minute = 30\n",
    "                    timps_ds = timps_ds.sel(time = timps_ds.time.dt.minute.isin(0))    #grab the time on the hour to match with ERA5 and AEW tracker\n",
    "        \n",
    "                    if len(timps_ds.gmd) == 0:\n",
    "                        print (f'{str(case_hour_nearest_multiple6).zfill(2)} UTC is out of range of the TIMPS ID range ({timps_ds0.time[0].values.astype(str)[:-10]} - {timps_ds0.time[-1].values.astype(str)[:-10]})')\n",
    "                        timps_ds0.close()\n",
    "                        continue\n",
    "        \n",
    "                    #timps_weighted_lat = timps_ds.centlatwgt.item()\n",
    "                    timps_weighted_lon = timps_ds.centlonwgt.item()\n",
    "                    \n",
    "                    aews_at_given_hour = aews_at_given_date.sel(time = aews_at_given_date.time.dt.hour.isin(nearest_6_to_case_hour))\n",
    "                \n",
    "                    lon_difs = aews_at_given_hour['AEW_lon_smooth'][:,0] - timps_weighted_lon\n",
    "                    aew_system_index = np.nanargmin(np.abs(lon_difs).values)   #np.argmin() would grab a NaN value!\n",
    "                    \n",
    "                    if aew_system_index_use != None:\n",
    "                        if case_num != 22:\n",
    "                            assert aew_system_index == aew_system_index_use, 'TIMPS IDs for the given case are not matching to the same AEW'\n",
    "                        else:  #Case 22 has 2 TIMPS IDs which incorrectly match to different AEWs;\n",
    "                               #the 2nd TIMPS ID (304352) matches to the correct AEW, so we're forcing the code to choose that AEW here (not ideal, but just one case we need to do this for)\n",
    "                            aew_system_index_use = aew_system_index * 1   #the number of the AEW in the AEW tracker file (using * 1 so that aew_system_index_use variable doesn't point (i.e., isn't tied to) to the same reference as aew_system_index)\n",
    "                            print (f\"Actual AEW system: {aew_system_index_use + 1}, AEW central longitude and strength at {case_date} {nearest_6_to_case_hour} UTC: {np.round(aews_at_given_hour['AEW_lon_smooth'][aew_system_index_use,0].item(), 2)}, {aews_at_given_hour['AEW_strength'][aew_system_index_use,0].item()} s-1\")\n",
    "                    else:\n",
    "                        aew_system_index_use = aew_system_index * 1   #the number of the AEW in the AEW tracker file (using * 1 so that aew_system_index_use variable doesn't point (i.e., isn't tied to) to the same reference as aew_system_index)\n",
    "                        print (f\"AEW system: {aew_system_index_use + 1}, AEW central longitude and strength at {case_date} {nearest_6_to_case_hour} UTC: {np.round(aews_at_given_hour['AEW_lon_smooth'][aew_system_index_use,0].item(), 2)}, {aews_at_given_hour['AEW_strength'][aew_system_index_use,0].item()} s-1\")\n",
    "                        \n",
    "                    timps_ds0.close()\n",
    "                \n",
    "            matched_aew_ds = aews_at_given_date.isel(system = aew_system_index_use)\n",
    "            matched_aew_ds_dayafter = aews_at_given_date_plus1.isel(system = aew_system_index_use)\n",
    "    \n",
    "            conv_df = pd.DataFrame()\n",
    "            conv_lats_df = pd.DataFrame()\n",
    "            #conv_lons_df = pd.DataFrame()\n",
    "    \n",
    "            for hr in case_hours:\n",
    "    \n",
    "                #grab/calculate the AEW centroid coordinates for the given hr\n",
    "                if hr % 6 == 0:  #if hr is a multiple of 6, then don't need to interpolate the AEW centroid at all\n",
    "                    matched_aew_smoothed_lat = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(hr)).AEW_lat_smooth.item()\n",
    "                    matched_aew_smoothed_lon = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(hr)).AEW_lon_smooth.item()\n",
    "    \n",
    "                else:  #interpolate the centroid of the matched AEW to the given hr\n",
    "    \n",
    "                    #find the multiple of 6 directly below/equal to hr; this will be your starting hour to interpolate the AEW centroid to hr\n",
    "                    nearest_6_below_hr = 6 * (hr // 6)\n",
    "                    nearest_6_above_hr = nearest_6_below_hr + 6  #this will be your ending hour to interpolate the AEW centroid to hr\n",
    "    \n",
    "                    matched_aew_start_hr_ds = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(nearest_6_below_hr))\n",
    "    \n",
    "                    if nearest_6_above_hr == 24:  #grab 00 UTC from the next day\n",
    "                        matched_aew_end_hr_ds = matched_aew_ds_dayafter.sel(time = matched_aew_ds_dayafter.time.dt.hour.isin(0))\n",
    "                    else:\n",
    "                        matched_aew_end_hr_ds = matched_aew_ds.sel(time = matched_aew_ds.time.dt.hour.isin(nearest_6_above_hr))\n",
    "    \n",
    "                    matched_aew_start_lat = matched_aew_start_hr_ds.AEW_lat_smooth.item()\n",
    "                    matched_aew_start_lon = matched_aew_start_hr_ds.AEW_lon_smooth.item()\n",
    "                    matched_aew_end_lat = matched_aew_end_hr_ds.AEW_lat_smooth.item()\n",
    "                    matched_aew_end_lon = matched_aew_end_hr_ds.AEW_lon_smooth.item()\n",
    "    \n",
    "                    #AEW centroid moves XX degrees per hour\n",
    "                    lat_per_hour = (matched_aew_end_lat - matched_aew_start_lat) / 6\n",
    "                    lon_per_hour = (matched_aew_end_lon - matched_aew_start_lon) / 6\n",
    "    \n",
    "                    #interpolated AEW centroid at the given hr\n",
    "                    matched_aew_smoothed_lat = matched_aew_start_lat + (lat_per_hour * (hr % 6))\n",
    "                    matched_aew_smoothed_lon = matched_aew_start_lon + (lon_per_hour * (hr % 6))\n",
    "    \n",
    "                #create 10-by-10 degree box around the (interpolated) AEW centroid at the given hr\n",
    "                aew_lat_range = slice(matched_aew_smoothed_lat + 5, matched_aew_smoothed_lat - 5)\n",
    "                aew_lon_range = slice(matched_aew_smoothed_lon - 5, matched_aew_smoothed_lon + 5)\n",
    "    \n",
    "                #grab all the ERA5 low-/mid-level convergence values and corresponding lats/lons within the given AEW box at the given hour\n",
    "                    #and separate the data into the 2 sectors (ahead/behind) of the AEW\n",
    "                v700 = ds_era5.v.sel(time = case_date).sel(level = 700)\n",
    "                v700 = v700.sel(time = v700.time.dt.hour.isin(hr))\n",
    "                v700 = mpcalc.smooth_gaussian(v700, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "                v700 = v700.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "            \n",
    "                #manually calculating convergence from ERA5 u and v winds (recommended by Brandon Wolding via George Kiladis)\n",
    "                u = ds_era5.u.sel(time = case_date).sel(level = pres_lev)\n",
    "                u = u.sel(time = u.time.dt.hour.isin(hr))\n",
    "                u = mpcalc.smooth_gaussian(u, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "                #u = u.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "                v = ds_era5.v.sel(time = case_date).sel(level = pres_lev)\n",
    "                v = v.sel(time = v.time.dt.hour.isin(hr))\n",
    "                v = mpcalc.smooth_gaussian(v, 5)   #smooth ERA5 winds using a 5-point filter (Quinton)\n",
    "                #v = v.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "                delta_lons = 0.25   #ERA5 lat/lon resolution is 0.25 degrees\n",
    "                delta_lons_meters = (111.3195 * 1000 * delta_lons) * np.cos(u.latitude.values * np.pi/180)  #distance between longitude lines at equator is 111.3195 km and cosine weighting this distance by latitude\n",
    "                dudx = (u[:,:,1:].values - u[:,:,:-1].values).squeeze() / np.expand_dims(np.abs(delta_lons_meters), axis=1)  #squeeze() removes dimensions of size 1 from an array, and expand_dims() inserts a new axis that will appear at the axis position\n",
    "                dudx = np.column_stack((dudx, dudx[:,-1]))  #duplicate the last column of dudx to match original grid shape (and shape of dvdy)\n",
    "    \n",
    "                delta_lats = 0.25\n",
    "                delta_lats_meters = 110.5744 * 1000 * delta_lats  #distance between latitude lines everywhere\n",
    "                dvdy = (v[:,:-1,:].values - v[:,1:,:].values).squeeze() / delta_lats_meters  #squeeze() removes dimensions of size 1 from an array\n",
    "                dvdy = np.vstack((dvdy, dvdy[-1,:]))  #duplicate the last row of dvdy to match original grid shape (and shape of dudx)\n",
    "    \n",
    "                conv_old = (dudx + dvdy) * -1 * 10**5  #manually calculated convergence from ERA5 u and v winds (times 10**5 1/s)\n",
    "                ds_conv = xr.Dataset(data_vars = dict(convergence = ([\"latitude\", \"longitude\"], conv_old)),\n",
    "                                     coords = dict(latitude = (\"latitude\", u.latitude.values), \n",
    "                                                   longitude = (\"longitude\", u.longitude.values)),\n",
    "                                     attrs = dict(description = \"Manually calculated ERA5 convergence data\"))\n",
    "                \n",
    "                conv = ds_conv.convergence.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "            \n",
    "                # #using convergence variable from ERA5\n",
    "                # conv = ds_era5.d.sel(time = case_date).sel(level = pres_lev) * -1    #convergence of the wind (1/s)\n",
    "                # conv = conv.sel(time = conv.time.dt.hour.isin(hr)) * 10**5  #convergence of the wind (times 10**5 1/s)\n",
    "                # conv = conv.sel(longitude = aew_lon_range, latitude = aew_lat_range)\n",
    "                \n",
    "                #filter the convergence data by the 700-hPa v-component of the wind  \n",
    "                    #v <= 0: the grid point is ahead of the AEW center \n",
    "                    #v > 0: the grid point is behind the AEW center\n",
    "                        #This dynamically defines ahead/behind AEW centers, which is especially practical for asymmetric AEWs!\n",
    "                if sector == 'full':\n",
    "                    conv = conv * 1\n",
    "                    clc = 'Full'\n",
    "                elif sector == 'south_ahead':\n",
    "                    conv = conv.where(conv.latitude < matched_aew_smoothed_lat).where(v700 <= 0)  #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Ahead (South)'\n",
    "                elif sector == 'ahead':\n",
    "                    conv = conv.where(v700 <= 0)  #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Ahead'\n",
    "                elif sector == 'north_ahead':\n",
    "                    conv = conv.where(conv.latitude >= matched_aew_smoothed_lat).where(v700 <= 0)  #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Ahead (North)'\n",
    "                elif sector == 'south_behind':\n",
    "                    conv = conv.where(conv.latitude < matched_aew_smoothed_lat).where(v700 > 0)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Behind (South)'\n",
    "                elif sector == 'behind':\n",
    "                    conv = conv.where(v700 > 0)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Behind'\n",
    "                elif sector == 'north_behind':\n",
    "                    conv = conv.where(conv.latitude >= matched_aew_smoothed_lat).where(v700 > 0)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'Behind (North)'\n",
    "                elif sector == 'north':\n",
    "                    conv = conv.where(conv.latitude >= matched_aew_smoothed_lat)   #returns elements from 'conv' where condition is True, otherwise fill in NaNs by default\n",
    "                    clc = 'North'\n",
    "    \n",
    "                lon, lat = np.meshgrid(conv.longitude, conv.latitude)\n",
    "                lats = lat.reshape(-1)\n",
    "                conv_values = conv.values.reshape(-1)\n",
    "    \n",
    "                #add data from each hour as COLUMNS to corresponding df\n",
    "                conv_df = pd.concat((conv_df, pd.Series(conv_values)), axis = 1, ignore_index = True)\n",
    "                conv_lats_df = pd.concat((conv_lats_df, pd.Series(lats)), axis = 1, ignore_index = True)\n",
    "                #conv_lons_df = pd.concat((conv_lons_df, pd.Series(lons)), axis = 1, ignore_index = True)         \n",
    "                    \n",
    "            if case_num == list(case_dict_conv_aew.keys())[0]:\n",
    "                color = 'darkred'\n",
    "                text_denom = 0\n",
    "            elif case_num == list(case_dict_conv_aew.keys())[1]:\n",
    "                color = 'navy'\n",
    "                text_denom = 0.2\n",
    "    \n",
    "            conv_df = conv_df.values  #convert Pandas DataFrame to NumPy array\n",
    "            conv_lats_df = conv_lats_df.values  #convert Pandas DataFrame to NumPy array\n",
    "    \n",
    "            #mask NaN values in the Dataframes so that the numpy stat calculations work below\n",
    "            conv_df_masked = np.ma.masked_where(np.isnan(conv_df), conv_df)\n",
    "            conv_lats_df_masked = np.ma.masked_where(np.isnan(conv_lats_df), conv_lats_df)\n",
    "    \n",
    "            #line plot histogram (clearer to interpret than \"step\" histogram below)\n",
    "            hist, bins = np.histogram(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None)\n",
    "            bin_centers = (bins[:-1] + bins[1:]) / 2  # Midpoints of the bins\n",
    "            ax.plot(bin_centers, hist, linewidth = 2, linestyle = '-', color = color, label = f'Case {case_num}')\n",
    "    \n",
    "            # #normal \"step\" histogram\n",
    "            # ax.hist(conv_df_masked.reshape(-1), bins = conv_bins, density = True, weights = None,\n",
    "            #         histtype = 'step', align = 'mid', orientation = 'vertical', color = color,\n",
    "            #         linewidth = 2, label = f'Case {case_num}')\n",
    "            #     #density = True returns a probability density: each bin will display the bin's raw count \n",
    "            #         #divided by the total number of counts times the bin width\n",
    "            #         #(density = counts / (sum(counts) * np.diff(bins))), so that the area under the \n",
    "            #         #histogram integrates to 1 (np.sum(density * np.diff(bins)) == 1)\n",
    "    \n",
    "            cos_weights = np.sqrt(np.cos(conv_lats_df_masked * np.pi/180))   #cosine weights to apply to conv_df_masked\n",
    "    \n",
    "            conv_count = np.count_nonzero(~np.isnan(conv_df))\n",
    "            conv_median = np.round(np.nanmedian(conv_df, axis = None), 2)\n",
    "            conv_mean = np.round(np.nanmean(conv_df, axis = None), 2)                                        #non-weighted mean (1st moment)\n",
    "            conv_wgt_mean = np.round(np.average(conv_df_masked, axis = None, weights = cos_weights), 2)      #cosine-weighted mean (1st moment)\n",
    "            conv_std = np.round(np.std(conv_df_masked, axis = None), 2)                                      #standard deviation (2nd moment)\n",
    "            conv_skew = np.round(scipy.stats.skew(conv_df_masked, axis = None, nan_policy = 'omit'), 4)      #skewness (3rd moment)\n",
    "            conv_kurt = np.round(scipy.stats.kurtosis(conv_df_masked, axis = None, nan_policy = 'omit'), 4)  #kurtosis (4th moment)\n",
    "    \n",
    "            # ax.text(0.98, 0.875 / text_denom, \n",
    "            #         f'Count: {conv_count}\\nMedian: {conv_median}\\nMean: {conv_mean}\\nWeighted Mean: {conv_wgt_mean}\\nStandard Deviation: {conv_std}\\nSkewness: {conv_skew}\\nKurtosis: {conv_kurt}\\n', \n",
    "            #         transform = ax.transAxes, horizontalalignment = 'right', verticalalignment = 'center', \n",
    "            #         fontsize = 16, fontweight = 'bold', color = color)\n",
    "            ax.text(0.98, 0.89 - text_denom, \n",
    "                    f'Count: {conv_count}\\nMedian: {conv_median}\\nMean: {conv_mean}\\nWeighted Mean: {conv_wgt_mean}\\nStandard Deviation: {conv_std}\\nSkewness: {conv_skew}\\nKurtosis: {conv_kurt}\\n', \n",
    "                    transform = ax.transAxes, horizontalalignment = 'right', verticalalignment = 'center', \n",
    "                    fontsize = 16, fontweight = 'bold', color = color)\n",
    "        \n",
    "            if case_num == 20:\n",
    "                ax.text(0.5, 0.5, f'Case {case_num} not being correctly\\nmatched to the appropriate AEW', \n",
    "                transform = ax.transAxes, horizontalalignment = 'center', verticalalignment = 'center', \n",
    "                fontsize = 30, bbox = {'facecolor': 'white', 'alpha': 0.5, 'pad': 10})\n",
    "\n",
    "        ax.set_title('ERA5 %i hPa Convergence PDFs (AEW-Relative, %s)' % (pres_lev, clc))\n",
    "        ax.axvline(x = 0, color = 'k', linestyle = '--', alpha = 0.5)\n",
    "        ax.set_xlabel('Convergence [10$^{-5}$ s$^{-1}$]')\n",
    "        ax.set_ylabel('Prob(Convergence)')\n",
    "        ax.set_xlim([-11,11])\n",
    "        ax.set_xticks(np.arange(-11, 11.1, 2))\n",
    "        ax.set_ylim(bottom = 0)\n",
    "        #ax.set_yticks(np.arange(0, 0.601, 0.05))\n",
    "        ax.grid(axis = 'y')\n",
    "        ax.legend(loc = 'upper left')\n",
    "                        \n",
    "        #plt.tight_layout()\n",
    "        #plt.subplots_adjust(wspace = 0.1)\n",
    "\n",
    "        #save the figure\n",
    "        plot_save_name = f'Case{list(case_dict_conv_aew.keys())[0]}-{list(case_dict_conv_aew.keys())[1]}_{pres_lev}hPa_convergence_PDFs_AEW_relative_{sector}.png'\n",
    "        plt.savefig(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/AEW_Relative/Smoothed_ERA5_winds', plot_save_name), bbox_inches = 'tight')\n",
    "        #plt.show()  #plt.show() must come after plt.savefig() in order for the image to save properly\n",
    "        #plt.clf()   #supposedly speeds things up? According to: https://www.youtube.com/watch?v=jGVIZbi9uMY\n",
    "        plt.close()\n",
    "        plt.clf()    #if placing this after plt.close(), may release memory related to the figure (https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)\n",
    "\n",
    "        ##decrease file size of the image by 66% without noticeable image effects (if using Matplotlib)\n",
    "        ##(good to use if you're producing a lot of images, see https://www.youtube.com/watch?v=fzhAseXp5B4)\n",
    "        im = Image.open(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/AEW_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "\n",
    "        try:\n",
    "            im2 = im.convert('P', palette = Image.Palette.ADAPTIVE)\n",
    "        except:\n",
    "            #use this for older version of PIL/Pillow if the above line doesn't work, \n",
    "            #though this line will have isolated, extremely minor image effects due to \n",
    "            #only using 256 colors instead of the 3-element RGB scale\n",
    "            im2 = im.convert('P')\n",
    "    \n",
    "        im2.save(os.path.join('/Users/ben/Desktop/CPEX/CPEX-CV_Convergence_PDFs/Quinton_new_AEW_tracker/Using_Calculated_ERA5_convergence/AEW_Relative/Smoothed_ERA5_winds', plot_save_name))\n",
    "        im.close()\n",
    "        im2.close()\n",
    "        \n",
    "        #print (f'Case {case_num} convergence PDF plots complete!\\n')\n",
    "    \n",
    "ds_era5.close()\n",
    "ds_aew.close()\n",
    "\n",
    "tend = time.time()\n",
    "print (f'This script took {np.round((tend - tstart) / 60, 1)} minutes to complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94291a8-c94e-4049-830e-4704041768c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
